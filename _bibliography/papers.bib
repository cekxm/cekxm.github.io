---
---

@string{aps = {American Physical Society,}}

@ARTICLE{Lu2024quality,
  bibtex_show={true},
  abbr={IEEE Access},
  author={Lu, Huaizheng and Wang, Shiwei and Zhang, Dedong and Huang, Bin and Chen, Erkang and Sui, Yunfeng},
  journal={IEEE Access}, 
  title={Toward Accurate Quality Assessment of Machine-Generated Infrared Video Using Fréchet Video Distance}, 
  year={2024},
  volume={12},
  number={},
  pages={168837-168852},
  keywords={Measurement;Feature extraction;Quality assessment;Computational modeling;Video recording;Cameras;Three-dimensional displays;Infrared imaging;Videos;Correlation;Visual analytics;Machine-generated infrared video;Fréchet video distance;I3D model;correlation analysis;GAN},
  doi={10.1109/ACCESS.2024.3453406}}


@article{YIN2024MUPT,
bibtex_show={true},
abbr={Displays},
selected={true},
title = {MUPT-Net: Multi-scale U-shape pyramid transformer network for Infrared Small Target Detection},
journal = {Displays},
volume = {83},
pages = {102681},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102681},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224000453},
author = {Junjie Yin and Jingxia Jiang and Weijia Li and Erkang Chen and Liyuan Chen and Lihan Tong and Bin Huang},
keywords = {Infrared small target detection, U-shape interaction module, Axial compression attention module, Transformer, Information interaction},
abstract = {Infrared Small Target Detection (IRSTD) aims to detect small and dim targets in complex backgrounds. However, the low signal-to-noise ratio and reduced contrast in the infrared domain make it challenging to extract these targets, as the cluttered background can easily overpower them. Existing Convolutional Neural Networks (CNN)-based methods for IRSTD often suffer from information loss due to inadequate utilization of acquired information after downsampling operations. This limits their ability to accurately extract shape information related to infrared small targets. To address this challenge, we propose a Multi-scale U-shape Pyramid Transformer Network (MUPT-Net). Our network incorporates the U-shape Interaction Module (UIM) and the Multi-scale ViT Module (MSVM) to perform feature extraction. By fully leveraging and integrating the information obtained after each downsampling operation, our approach enables precise extraction of shape information for infrared small targets. Additionally, we introduce the Axial Compression Attention module (ACA), which focuses on capturing the interplay of positional information within the feature map to facilitate accurate detection of small targets. Through iteratively fusing and augmenting multi-scale features, our MUPT-Net effectively assimilates and harnesses contextual information of small targets. Experimental results on the SIRST v1, SIRST v2 and NUDT-SIRST datasets demonstrate the superiority of our approach compared to representative state-of-the-art (SOTA) IRSTD methods.}
}


@inproceedings{Li2024NanoTrack,
bibtex_show={true},
author = {Li, Weijia and Chen, Liyuan and Cai, Junzhi and Tong, Lihan and Chen, Erkang and Huang, Bin},
title = {NanoTrack: An Enhanced MOT Method by Recycling Low-score Detections from Light-weight Object Detector},
year = {2024},
isbn = {9798400716607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663976.3664008},
doi = {10.1145/3663976.3664008},
abstract = {In this paper, we introduced NanoTrack, a novel multi-object tracking (MOT) method that leverages light-weight object detectors to enhance tracking performance in real-time applications where computational resources are scarce. While light-weight detectors are efficient, they often produce an imbalance in detection quality, generating a significant number of low-scoring detections that pose challenges for tracking algorithms. Our approach innovatively utilizes these low-scoring detections for track initialization and maintenance, addressing the shortcomings observed in existing tracking by two-stage tracking methods like ByteTrack, which struggle with the abundance of low-scoring detections. By integrating two new light-weight modules, Refind High Detection (RHD) and Duplicate Track Checking (DTC), NanoTrack effectively incorporates low-scoring detections into the tracking process. Additionally, we enhance the pseudo-depth estimation technique for improved handling in dense target environments, mitigating issues like ID Switching. Our comprehensive experiments demonstrate that NanoTrack surpasses state-of-the-art two-stage TBD methods, including ByteTrack and SparseTrack, on benchmark datasets such as MOT16, MOT17, and MOT20, thereby establishing a new standard for MOT performance using light-weight detectors. The code is open source in https://github.com/VjiaLi/NanoTrack},
booktitle = {Proceedings of the 2024 2nd Asia Conference on Computer Vision, Image Processing and Pattern Recognition},
articleno = {24},
numpages = {9},
keywords = {computer vision, light-weight object detector, multi-object tracking},
location = {Xiamen, China},
series = {CVIPPR '24}
}

@INPROCEEDINGS{chen2024MUIR,
  author={Chen, Liyuan and Li, Weijia and Yang, Qingxia and Tong, Lihan and Chen, Erkang and Huang, Bin and Li, RuiWen},
  booktitle={2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)}, 
  title={MUIR: Mamba for Underwater Image Rendering}, 
  year={2024},
  volume={},
  number={},
  pages={172-177},
  keywords={Measurement;Image quality;Image synthesis;Computational modeling;Computer architecture;Machine learning;Rendering (computer graphics);underwater image;Mamba;rendering;lightweight},
  doi={10.1109/MLISE62164.2024.10674249}}

@article{CHEN2024Dual,
bibtex_show={true},
abbr={Digi. Sig. Proc.},
title = {Dual-former: Hybrid self-attention transformer for efficient image restoration},
journal = {Digital Signal Processing},
volume = {149},
pages = {104485},
year = {2024},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2024.104485},
url = {https://www.sciencedirect.com/science/article/pii/S1051200424001106},
author = {Sixiang Chen and Tian Ye and Yun Liu and Erkang Chen},
keywords = {Image restoration, Local feature extraction, Hybrid self-attention, Adaptive control module, Multi-branch feed-forward network},
abstract = {Recently, image restoration transformers have achieved comparable performance with previous state-of-the-art CNNs. In this work, we present Dual-former whose critical insight is to combine the powerful global modeling ability of self-attention modules and the local modeling ability of convolutions in an overall architecture. With convolution-based Local Feature Extraction modules equipped in the encoder and the decoder, we only adopt a novel Hybrid Transformer Block in the latent layer to model the long-distance dependence in spatial dimensions and handle the uneven distribution between channels. Such a design eliminates the substantial computational complexity in previous image restoration transformers and achieves superior performance on multiple image restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91 dB gain over the state-of-the-art MAXIM method on the Indoor dataset for single image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image deraining, it exceeds the SOTA method by 0.1 dB PSNR on the average results of five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses the latest methods on various tasks, with fewer parameters.}
}


@inproceedings{Tong2024Parallel,
author = {Tong, Lihan and Liu, Yun and Ye, Tian and Li, Weijia and Chen, Liyuan and Chen, Erkang},
title = {Parallel Cross Strip Attention Network for Single Image Dehazing},
year = {2024},
isbn = {9798400716751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670105.3670130},
doi = {10.1145/3670105.3670130},
abstract = {The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals. Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size. While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal dehazing performance. In this study, we introduce a novel dehazing network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy. PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain. To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features. We perform extensive testing on synthetic and real-world datasets and our PCSA-Net has established a new performance benchmark. This work not only advances the field of image dehazing but also serves as a reference for developing more diverse and efficient attention mechanisms. CCS Concepts: · Theory of computation → Theory and algorithms for application domains → Machine learning theory → Models of learning keywords: Parallel Cross Strip Attention, Single Image Dehazing, Horizontal Strip Attention, Vertical Strip Attention},
booktitle = {Proceedings of the 2024 5th International Conference on Computing, Networks and Internet of Things},
pages = {148–154},
numpages = {7},
location = {Tokyo, Japan},
series = {CNIOT '24}
}


@article{Chen2024Degradation,
bibtex_show={true},
abbr={Front. Comput. Sci.},
selected={true},
author = {Chen, Erkang and Chen, Sixiang and Ye, Tian and Liu, Yun},
title = {Degradation-adaptive neural network for jointly single image dehazing and desnowing},
year = {2024},
issue_date = {Apr 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-023-2764-y},
doi = {10.1007/s11704-023-2764-y},
journal = {Front. Comput. Sci.},
month = jan,
numpages = {3},
preview={snow.gif}
}


@article{Chen_2022_DISP ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2022 Elsevier Inc.},
copyright = {Compendex},
title = {Robust back-scattered light estimation for underwater image enhancement with polarization},
journal = {Displays},
author = {Chen, Sixiang and Chen, Erkang and Ye, Tian and Xue, Chenghao},
volume = {75},
year = {2022},
issn = {01419382},
URL = {http://dx.doi.org/10.1016/j.displa.2022.102296},
} 


@InProceedings{Ye2022Perce,
author="Ye, Tian
and Zhang, Yunchen
and Jiang, Mingchao
and Chen, Liang
and Liu, Yun
and Chen, Sixiang
and Chen, Erkang",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Perceiving and Modeling Density for Image Dehazing",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="130--145",
abstract="In the real world, the degradation of images taken under haze can be quite complex, where the spatial distribution of haze varies from image to image. Recent methods adopt deep neural networks to recover clean scenes from hazy images directly. However, due to the generic design of network architectures and the failure in estimating an accurate haze degradation model, the generalization ability of recent dehazing methods on real-world hazy images is not ideal. To address the problem of modeling real-world haze degradation, we propose a novel Separable Hybrid Attention (SHA) module to perceive haze density by capturing positional-sensitive features in the orthogonal directions to achieve this goal. Moreover, a density encoding matrix is proposed to model the uneven distribution of the haze explicitly. The density encoding matrix generates positional encoding in a semi-supervised way -- such a haze density perceiving and modeling strategy captures the unevenly distributed degeneration at the feature-level effectively. Through a suitable combination of SHA and density encoding matrix, we design a novel dehazing network architecture, which achieves a good complexity-performance trade-off. Comprehensive evaluation on both synthetic datasets and real-world datasets demonstrates that the proposed method surpasses all the state-of-the-art approaches with a large margin both quantitatively and qualitatively. The code is released in https://github.com/Owen718/ECCV22-Perceiving-and-Modeling-Density-for-Image-Dehazing.",
isbn="978-3-031-19800-7"
}


@InProceedings{Ye_2022_CVPRW,
    author    = {Ye, Tian and Chen, Sixiang and Liu, Yun and Ye, Yi and Chen, Erkang and Li, Yuche},
    title     = {Underwater Light Field Retention: Neural Rendering for Underwater Imaging},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {488-497}
}

@InProceedings{Ye_2022_ACCV,
    author    = {Ye, Tian and Chen, Sixiang and Liu, Yun and Ye, Yi and Bai, Jinbin and Chen, Erkang},
    title     = {Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {December},
    year      = {2022},
    pages     = {366-381}
}

@article{Zhang_2022_Opt,
author = {Dehuan Zhang and Jiaqi Shen and Jingchun Zhou and Erkang Chen and Weishi Zhang},
journal = {Opt. Express},
keywords = {Illumination design; Image processing; Image quality; Image quality assessment; Light scattering; Underwater imaging},
number = {18},
pages = {33412--33432},
publisher = {Optica Publishing Group},
title = {Dual-path joint correction network for underwater image enhancement},
volume = {30},
month = {Aug},
year = {2022},
url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-30-18-33412},
doi = {10.1364/OE.468633}
}
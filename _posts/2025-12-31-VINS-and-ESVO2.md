---
layout: post
title: "Vins And Esvo2"
date: 2025-12-31 08:19:46 +0800
categories: []
description: VINS, ESVO, SFM
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

## VINS

### 1. 系统架构：通用因子框架

该方法的核心思想是将每种传感器都视为框架中的一个**通用因子（Factor）** 。

- **多传感器支持**：框架支持多种传感器组合，如单目相机+IMU、双目相机、以及双目相机+IMU。
- **因子图构建**：共享公共状态变量的因子会被累加在一起，构建一个位姿图（Pose Graph）优化问题 。
- **鲁棒性**：由于不依赖特定传感器，系统可以轻松处理传感器失效的情况，通过移除失效因子的方式快速切换传感器组合。

该方法不是学习类的方法。

![image-20251226153814897](/images/2025-12-31-VINS-and-ESVO2/image-20251226153814897.png){: .img-fluid}

系统通过一个滑动窗口（Sliding Window）来维护待优化的状态变量。以下是该方法中**状态向量（State Vector）**的详细构成：

#### 完整的状态向量

在滑动窗口优化中，总的状态向量 $$\mathcal{X}$$ 被定义为窗口内所有帧的状态、外参以及特征点深度的集合：

$$\mathcal{X} = [\mathbf{x}_m, \mathbf{x}_{m+1}, \dots, \mathbf{x}_n, \mathbf{x}_c^b, \lambda_1, \lambda_2, \dots, \lambda_l]$$

其中：

- $$m, n$$ 是滑动窗口的起始帧和结束帧。
- $$\mathbf{x}_i$$ 是第 $$i$$ 帧对应的 **IMU 核心状态**。
- $$\mathbf{x}_c^b$$ 是相机与IMU之间的 **外参（Extrinsic Parameters）**。
- $$\lambda_l$$ 是第 $$l$$ 个视觉特征点的 **逆深度（Inverse Depth）**。

**和论文表示不太一样，但实际是一样的。这边body frame 的位姿状态放在IMU里面了。**

------

##### 具体的 IMU 状态 ($$\mathbf{x}_i$$)

对于窗口中的每一帧 $$i$$，其核心状态变量包含 15 个维度：

$$\mathbf{x}_i = [\mathbf{p}_{b_i}^w, \mathbf{v}_{b_i}^w, \mathbf{q}_{b_i}^w, \mathbf{b}_a, \mathbf{b}_g]$$

- **$$\mathbf{p}_{b_i}^w$$ (3D Position)**：IMU 坐标系在世界坐标系下的位置。
- **$$\mathbf{v}_{b_i}^w$$ (3D Velocity)**：IMU 在世界坐标系下的速度。
- **$$\mathbf{q}_{b_i}^w$$ (Quaternion/Rotation)**：从 IMU 坐标系到世界坐标系的旋转（通常用四元数表示 Hamilton 形式，或旋转矩阵）。
- **$$\mathbf{b}_a$$ (Accelerometer Bias)**：加速度计的零偏。
- **$$\mathbf{b}_g$$ (Gyroscope Bias)**：陀螺仪的零偏。

------

##### 辅助状态变量

为了保证系统的通用性和精度，论文还包括了以下状态：

- 传感器外参 ($$\mathbf{x}_c^b$$)：

  包含从相机到 IMU 坐标系的变换 $$[\mathbf{p}_c^b, \mathbf{q}_c^b]$$。在系统在线运行时，这些外参也可以被放入优化器进行实时修正。

- 视觉特征点状态 ($$\lambda_l$$)：

  VINS 采用**逆深度（Inverse Depth）**作为特征点的参数化方式。相比于直接使用 3D 坐标 $$(x, y, z)$$，逆深度可以更好地处理距离相机非常远的特征点，且其分布更接近高斯分布，有助于数值优化的收敛。

#### 因子

每一个方框是一个 factor，也就是一次测量。更细致的话

- 相机：一个测量对应一个特征点在该时刻的位置。特征点使用KLT进行跟踪，从之前 $$l$$ 时刻到当前 $$t$$  时刻有对应关系，这两个时刻在图像上的位置有约束。
- 如果双目相机，则特征点在左边和右边的位置上，也有约束。
- IMU: 基于预积分结果的相邻关键帧间的运动约束残差。

### 2. 代价函数 (Cost Function)

该系统将状态估计建模为一个**最大似然估计（MLE）**问题。在假设测量噪声符合高斯分布的条件下，MLE被转化为一个非线性最小二乘问题，即最小化所有传感器测量残差的加权平方和。

在滑动窗口优化中，总代价函数（MAP 形式）定义如下：

$$\mathcal{X}_{m:n}^* = \arg \min_{\mathcal{X}_{m:n}} \left\{ \sum_{t=m}^{n} \sum_{k \in S} \\mid  z_t^k - h_t^k(\mathcal{X}_{m:n}) \\mid _{\Omega_t^k}^2 + \\mid  H_p \delta \mathcal{X}_{m:n} - b_p \\mid ^2 \right\}$$

该代价函数主要由以下三部分组成：

- **视觉因子残差（Camera Factor）**：特征点在不同帧之间的重投影误差 。
- **IMU 因子残差（IMU Factor）**：基于预积分结果的相邻关键帧间的运动约束残差。
- **边缘化先验项（Prior Term）**：由滑动窗口中被移除（边缘化）的旧状态所转换而来的约束信息，用于保留历史观测对当前状态的影响。

### 3. 光束法平差 (Bundle Adjustment, BA)

论文将状态估计过程称为**基于滑动窗口的光束法平差**。

- **基本原理**：BA通过同时优化相机位姿、IMU状态（速度、偏置）以及特征点的逆深度，使得所有观测残差最小化。
- **求解方式**：
  - **线性化**：在当前估计值附近对代价函数进行一阶泰勒展开，将其转化为线性最小二乘问题。
  - **迭代优化**：采用 **Gauss-Newton** 或 **Levenberg-Marquardt** 方法进行多次迭代直至收敛。系统具体使用了 **Ceres Solver** 进行高效求解。
- **计算效率控制**：
  - **滑动窗口（Sliding Window）**：为了维持实时性，优化只在固定大小的窗口（通常是10帧左右）内进行，而不是处理整个轨迹。
  - **边缘化（Marginalization）**：当新帧加入导致窗口满时，利用**舒尔补（Schur Complement）**将最老的帧移除，并将其携带的信息转化为当前窗口内状态的先验分布。

这种基于优化的方法相比于传统的滤波器（如EKF）具有更高的精度，因为它可以在非线性空间内进行多次迭代更新，并能更好地处理传感器之间的时钟同步偏差。

## ESVO2

![image-20251227073033444](/images/2025-12-31-VINS-and-ESVO2/image-20251227073033444.png){: .img-fluid}

**ESVO2** 是由湖南大学（Yi Zhou 教授团队）与香港科技大学（Shaojie Shen 教授团队）等合作开发的一个**实时、紧耦合的双目事件相机视觉惯性里程计系统** 。

### 概述

相比前代系统，ESVO2 引入了以下四大创新 ：

- **自适应累积 (Adaptive Accumulation, AA)**：提出了一种新型的类图像事件表示方法，能够根据事件的局部动态自动调整累积时间。这使得系统能高效提取边缘轮廓点，而不受运动速度变化的影响。
- **引入 IMU 预积分**：通过将 IMU 测量值作为运动先验，解决了纯视觉追踪在特定旋转维度（如 Pitch 和 Yaw）上的简并（Degeneracy）问题 。
- **紧耦合后端优化**：设计了一个精简的后端，专门优化线性速度和 IMU 偏置（Bias），从而抑制轨迹漂移，确保全局一致性。
- **增强的建图模块**：结合了“时间同步双目（Temporal Stereo）”和“静态双目（Static Stereo）”配置，并引入快速块匹配方案，显著提升了深度图的完整性和局部平滑度。

#### 系统架构

ESVO2 采用并行设计，由四个独立线程组成：

1. **预处理线程**：负责事件的自适应累积及时间表面（Time Surface）的更新。
2. **追踪线程（Localization）**：执行 3D-2D 时空配准，利用无偏移平滑时间表面（OS-TS）进行位姿估计。
3. **建图线程（Mapping）**：实时恢复半稠密深度图并维护局部 3D 地图。
4. **后端线程（Back-end）**：进行滑动窗口优化，不断更新速度和 IMU 参数。

### Mapping

#### **Temporal Stereo（时间双目）** 

是一种核心的深度估计技术。它通过结合**时间维度**（单相机的运动）和**空间维度**（双相机的基线）来提高建图的鲁棒性。

##### 1. 定义与核心思想

**Temporal Stereo** 指的是利用**单只相机在不同时刻（即随时间位移）**观测到的视觉信息来估计物体深度的技术 。

* **对比 Static Stereo（静态双目）**：静态双目是利用左右两个相机在**同一时刻**的视图差异（视差）来计算深度。


* **结合的意义**：在 ESVO2 中，系统并不只依赖左右相机的瞬时匹配，而是将相机的**运动位移**也作为一种“虚拟基线”。

##### 2. 工作原理

在 ESVO2 的建图模块中，Temporal Stereo 的具体运作方式如下：

* 
**时空观测一致性**：当机器人移动时，同一个空间点会在不同时间点被相机捕获。系统会寻找当前事件流与该相机在不久前的历史观测之间的相关性 。


* 
**代价合并（Cost Fusion）**：ESVO2 将 Temporal Stereo 和 Static Stereo 的匹配代价（Matching Cost）进行融合 。


* 如果运动方向有利于时间双目（例如向前运动，提供了纵向观测变化），Temporal Stereo 能提供很好的约束。
* 如果左右相机之间的视差更明显，Static Stereo 则占据主导。

##### 3. 在 ESVO2 中的作用

引入 Temporal Stereo 对事件相机 SLAM 具有至关重要的作用：

* 
**解决观测退化**：事件相机对垂直于其边缘运动的方向敏感。如果机器人仅进行某种特定方向的运动，静态双目可能会失效。Temporal Stereo 增加了额外的观测维度，保证了在 6-DoF（六自由度）复杂运动下的建图完整性 。


* 
**提高深度图质量**：通过在时间轴上累积观测，系统能够过滤掉瞬时的噪声事件，生成的深度图更加平滑且空洞更少 。


* **快速块匹配**：论文提到在执行 Temporal Stereo 匹配时采用了快速块匹配方案，这使得系统能够在普通 CPU 上实时处理 VGA 分辨率的高频数据 。

在 ESVO2 中，**Temporal Stereo 实际上是将 VIO 系统变成了一个“多视图立体视觉（MVS）”系统**。它不再局限于双目相机那段固定的物理基线，而是将相机的整个运动轨迹都变成了获取几何信息的源泉，从而在各种运动条件下都能输出高精度的半稠密深度图 。

#### **Fast Static Stereo（快速静态双目）** 

**Fast Static Stereo（快速静态双目）** 的目的确实是为了获得三维点的坐标，但它的实现方式和传统的“特征点提取 + 三角化”有所不同。

以下是针对您问题的详细解答：

##### 1. 是为了获得稀疏三维点吗？

**是的，但更准确地说是“半稠密（Semi-dense）”的三维点。**

- **非稀疏**：传统的 VO（如 ORB-SLAM）只提取少量的特征点（如角点），产生的点云非常稀疏。
- **非全稠密**：由于事件相机只在亮度变化的地方产生数据（通常是物体的边缘），所以它无法像结构光或 LiDAR 那样获得物体的完整表面。
- **半稠密**：ESVO2 的 Fast Static Stereo 针对事件累积产生的**边缘轮廓**进行匹配，因此得到的是物体轮廓处的 3D 点云，这比传统的稀疏点云要密得多，足以辅助避障和局部建图。

##### 2. 只需要做三角化吗？

**不完全是。** 虽然双目外参（变换矩阵）已经离线标定好了，但“三角化”只是最后一步。最难、最核心的步骤是**数据关联（Data Association / Correspondence）**，即：**左目里的这个事件点，对应右目里的哪个像素？**

在 ESVO2 的 Fast Static Stereo 中，这个过程包含以下关键步骤：

- **极线搜索 (Epipolar Search)**：利用已标定的外参，在右目的极线上寻找匹配点。
- **块匹配 (Block Matching)**：由于事件数据不是灰度图，ESVO2 使用了 **Time Surface（时间表面）** 或像素块来进行相似度比较。
- **时空一致性检查**：系统会检查左右目事件在时间上的同步性（是否在极短的时间差内发生）以及空间上的几何一致性。
- **三角化 (Triangulation)**：只有当匹配的代价（Cost）足够低且置信度高时，系统才会根据已知的双目基线进行几何三角化，计算出深度 $$Z$$。

##### 3. ESVO2 “Fast” 的体现

之所以称为 **Fast**，是因为它做了一些专门针对 CPU 实时性的优化：

1. **查找表 (Look-up Table)**：预先计算极线方向，减少运行时的几何计算。
2. **跳过非边缘区域**：只对有事件发生的像素点进行匹配，避开大量空白区域。
3. **并行化**：利用多线程并行处理左右目的事件流块。

你理解的“通过三角化获得坐标”是最终目的，但 **Fast Static Stereo 的核心在于如何利用标定好的几何关系，在极高频率、高分辨率的事件流中快速找到左右目的匹配对。**

值得注意的是，ESVO2 还会将 **Static Stereo**（左右目匹配）的结果与 **Temporal Stereo**（单目随时间运动产生的匹配）进行融合，这样即使在双目基线方向运动导致视差较小时，依然能获得准确的深度。

#### 轮廓

在 ESVO2 论文中，**Fast Static Stereo（快速静态双目）不使用传统的稀疏特征点（如角点），而是直接使用物体的轮廓（Contour Points）进行匹配** 。

这种设计是由事件相机的物理特性决定的。以下是具体的实现逻辑：

##### 1. 为什么选择轮廓而不是特征点？

- **物理机制**：事件相机只在亮度变化时触发信号，因此其天然的输出就是物体的**边缘和轮廓**。
- **鲁棒性**：在高速运动中，传统的特征点（如 Harris 角点）可能因为观察不全而难以追踪 3。直接使用轮廓点能保留更完整的环境几何结构。

##### 2. ESVO2 的具体处理方式

ESVO2 并没有处理所有的事件点，而是通过以下步骤提取**精准的轮廓点**进行匹配：

- **自适应累积 (Adaptive Accumulation, AA)**：系统会根据事件的动态变化（速度快慢），自动决定累积多长时间的事件来生成一张类似图像的“AA图”。
- **轮廓点采样 (Contour-point Sampling)**：在 AA 图上，系统会采样那些代表瞬时边缘的像素点（即轮廓点） 。
- **排除冗余**：相比于直接使用原始事件流，这种方法筛选掉了大量噪声和冗余点，使得输入点更“精简且准确”。

##### 3. Fast Static Stereo 的“快”体现在哪？

由于处理的是轮廓点，ESVO2 采用了以下策略实现高效匹配：

- **块匹配 (Block Matching)**：在左右目相机的 AA 图或时间表面（Time Surface）上，沿着极线对这些**采样后的轮廓像素块**进行相似度搜索。
- **取消非线性精化**：论文提到，由于采样后的轮廓点已经非常精确，系统在 ESVO2 中**取消了耗时的子像素级非线性精化步骤**，从而显著提升了速度（在 VGA 分辨率下达到 20Hz 实时性），且精度几乎没有下降。

在 ESVO2 的 **Fast Static Stereo** 模块中，获得 3D 坐标的过程是一个从“事件像素”到“空间点”的几何推导过程。虽然不使用复杂的特征描述子，但它利用了极其严格的**时空约束**。

以下是具体的实现步骤：

##### 1. 极线约束搜索 (Epipolar Search)

假设左目相机在 $$u_L$$ 像素处采样了一个轮廓点，由于双目相机的外参（基线 $$b$$、焦距 $$f$$）已预先标定且图像已做过极线校正（Rectification），那么这个点在右目图像中对应的匹配点一定位于**水平的极线**上。

- **搜索范围**：系统会在右目的极线上，根据设定的最小和最大深度范围，确定一个搜索区间。

##### 2. 基于“时间表面”的匹配 (Time Surface Matching)

这是最关键的一步。因为事件相机没有灰度值，它使用 **Time Surface (TS)** 或 **AA 图**来计算匹配相似度：

- **提取 Patch**：在左目 $$u_L$$ 周围取一个小的像素块（例如 $$5 \times 5$$）。
- **计算相似度**：在右目极线的搜索区间内滑动，通过 **零均值归一化互相关 (ZNCC)** 或类似的度量函数，寻找与左目 Patch 最相似的区域。
- **原理**：虽然没有颜色，但物体轮廓在左右目形成的时间表面形状是非常相似的。

##### 3. 概率深度估计 (Probabilistic Depth Estimation)

ESVO2 并不只是简单地计算一个视差，而是为每个点维护一个概率分布：

- **逆深度建模**：系统将深度表示为逆深度（Inverse Depth），并假设其服从高斯分布。

- 代价聚合：当匹配代价最低的点被找到后，通过视差公式计算初始深度：

  $$Z = \frac{f \cdot b}{d}$$

  （其中 $$f$$ 是焦距，$$b$$ 是基线，$$d$$ 是左右目像素的位移差/视差）。

##### 4. 空间三角化 (Triangulation)

一旦确定了左右目的匹配对 $$(u_L, v_L)$$ 和 $$(u_R, v_R)$$，就可以通过标准的**三角化公式**求出该轮廓点在左目相机坐标系下的 3D 坐标 $$(X, Y, Z)$$：

- $$Z = \frac{f \cdot b}{u_L - u_R}$$
- $$X = \frac{(u_L - c_x) \cdot Z}{f}$$
- $$Y = \frac{(v_L - c_y) \cdot Z}{f}$$

##### 5. ESVO2 的特殊优化：取消非线性精化

在早期的 ESVO 版本中，三角化后通常还需要一个耗时的“非线性最小二乘精化”过程来微调 3D 位置。

ESVO2 的改进：由于采用了自适应累积 (AA) 提取的轮廓点非常精准，论文提到他们直接跳过了非线性精化步骤。只要匹配代价足够低，就直接输出三角化的结果。这种“直接法”极大地提升了处理 VGA 分辨率数据的速度。

##### 总结

简单来说，获得 3D 坐标的流程是：

左目采样轮廓点 $$\rightarrow$$ 右目极线搜索匹配块 $$\rightarrow$$ 计算视差 $$\rightarrow$$ 几何三角化 $$\rightarrow$$ 得到 3D 坐标。

这一过程生成的点云是**半稠密**的，因为它只存在于有光强变化的边缘位置，但对于机器人感知环境（如避障、路径规划）已经足够丰富。

### Localization 

#### 1. 空间上的 3D-2D 配准（当前时刻 vs 历史时刻）

追踪线程的核心逻辑是：**“用过去建立的地图，来对齐现在的观测。”**

- **3D 信息（来自过去/不同时刻）：** 追踪线程使用的 3D 点云是由“建图线程（Mapping）”提供的。这些 3D 点是根据**之前的一系列时刻**（滑动窗口内的历史帧）计算并累积出来的局部地图。
- **2D 信息（来自当前时刻）：** 追踪线程使用的是**当前最新时刻**产生的事件流（并转化成了 OS-TS 时间表面）。

**结论：** 它是将**历史时刻积累的 3D 结构**投影到**当前时刻的 2D 平面**上。如果投影的位置和当前看到的位置重合，就说明位姿估计是准确的。

------

#### 2. 时间上的“时空一致性”（时空配准）

ESVO2 之所以强调“时空（Spatio-temporal）”，是因为它不仅仅看空间位置，还考虑了**事件发生的时间戳**。

- **Time Surface (TS) 的本质：** 时间表面本身就存储了时间信息（像素值代表该位置最近一次事件发生的时间戳）。
- 配准逻辑： 1.  假设当前时间是 $$t_{now}$$。
  2. 如果 3D 点投影到 $$u$$ 位置，而 OS-TS 在 $$u$$ 位置记录的事件时间非常接近 $$t_{now}$$，说明这个 3D 点在当前时刻是“活跃”的，配准残差就小。
  3. 反之，如果该位置没有新事件，或者事件发生的时间很久远，残差就会很大。

------

#### 3. IMU 的作用：连接不同时刻的纽带

IMU 预积分在这里扮演了极其重要的角色：

- 它利用**两个时刻之间**的高频惯性数据，预测出从上一时刻到当前时刻的相对位姿变化。
- 这个预测值作为初值，告诉追踪线程：“根据 IMU 估计，那些 3D 点现在应该出现在这个位置。” 然后视觉配准再在这个基础上进行微调。

### “跨时刻”的配准

“跨时刻”的配准（通常指 SLAM 或视觉里程计中的位姿估计）确实是通过**雅可比矩阵（Jacobian）**来建立误差与位姿增量之间的线性关系，进而通过迭代优化的方式更新位姿的。

由于位姿（旋转 + 平移）所在的空间（如 $$SE(3)$$）并不是一个欧几里得空间（你不能简单地把两个旋转矩阵相加），数学上通常会引入**李群（Lie Group）**和**李代数（Lie Algebra）**来处理求导问题。

以下是这一过程的数学推导核心步骤：

------

#### 1. 定义误差函数（Residual）

假设在 $$t$$ 时刻，我们观察到一个空间点 $$P$$。在 $$t+1$$ 时刻，相机的位姿变为 $$T$$（包含旋转 $$R$$ 和平移 $$t$$）。该点在当前相机坐标系下的投影预测值为：

$$\hat{z} = h(T, P)$$

其中 $$h$$ 是投影函数。如果实际观测到的特征点坐标是 $$z$$，那么**误差（残差）**定义为：

$$e(T) = z - h(T, P)$$

#### 2. 引入扰动模型（Perturbation Model）

因为直接对旋转矩阵 $$R$$ 求导非常复杂且必须保持正交性约束，我们通常给当前的位姿 $$T$$ 左乘一个微小的扰动 $$\Delta T$$。

在李代数上，这个扰动可以用一个 6 维向量 $$\xi = [\rho, \phi]^T$$ 表示（前三维为平移扰动，后三维为旋转扰动）：

$$T_{new} = \exp(\xi^{\wedge}) \cdot T_{old}$$

#### 3. 利用雅可比矩阵进行线性化

我们要计算的是：当位姿发生微小变化 $$\xi$$ 时，误差 $$e$$ 发生了多少变化？

利用泰勒展开：

$$e(T_{new}) = e(\exp(\xi^{\wedge})T_{old}) \approx e(T_{old}) + \frac{\partial e}{\partial \xi} \xi$$

这里的 $$J = \frac{\partial e}{\partial \xi}$$ 就是你所说的雅可比矩阵。

#### 4. 雅可比矩阵的具体分解

根据链式法则，这个雅可比矩阵通常可以拆解为两部分：



$$J = \frac{\partial e}{\partial P'} \cdot \frac{\partial P'}{\partial \xi}$$

- **第一部分 $$\frac{\partial e}{\partial P'}$$**：像素误差对空间点坐标（在相机坐标系下）的导数。这取决于相机的内参模型（如针孔模型）。

- 第二部分 $$\frac{\partial P'}{\partial \xi}$$：变换后的空间点坐标对位姿扰动的导数。在 $$SE(3)$$ 下，对于点 $$P' = [X, Y, Z]^T$$，其推导结果通常是一个 $$3 \times 6$$ 的矩阵：

  

  $$\frac{\partial P'}{\partial \xi} = \begin{bmatrix} I & -P'^{\wedge} \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 & Z & -Y \\ 0 & 1 & 0 & -Z & 0 & X \\ 0 & 0 & 1 & Y & -X & 0 \end{bmatrix}$$

#### 5. 求解与位姿更新

为了使误差最小化（最小二乘问题），我们构建高斯-牛顿（Gauss-Newton）方程：



$$(J^T J) \Delta \xi = -J^T e$$

- **$$H = J^T J$$**：近似海森矩阵（Hessian）。
- **$$\Delta \xi$$**：计算出的最优位姿增量。

更新步：

得到 $$\Delta \xi$$ 后，我们将其映射回李群，更新当前的位姿估计：

$$T \leftarrow \exp(\Delta \xi^{\wedge}) \cdot T$$

通过不断重复上述过程（线性化 -> 求解 -> 更新），位姿会逐渐收敛到能够使跨时刻特征点对齐的最优值。

### Localization 与 Mapping并行且循环的工作流

在 ESVO2 的追踪线程（Localization）中，**它同时利用了“跨时刻”的 3D 信息和“当前时刻”的 2D 信息。**

#### 1. 追踪线程（Localization）：利用“过去”引导“现在”

- **正确性确认**：是的。追踪线程是第一步。
- **逻辑**：
  - **输入**：当前的 2D 信息（OS-TS 时间表面）+ 之前的 Local 3D Map。
  - **过程**：它执行 3D-2D 的时空配准。简单说，就是把已经建好的 3D 地图点投影到当前的 2D 画面上，看对不对得上。
  - **结果**：计算出当前最准确的 **Camera Pose**（相机位姿）。IMU 在这里提供了一个非常关键的初始预测位姿，使得追踪在剧烈运动时不会丢。

#### 2. 建图线程（Mapping）：利用“现在”更新“未来”

- **正确性确认**：是的。获得当前位姿后，紧接着（或并行）执行 Mapping。
- **逻辑**：
  - **输入**：当前的 Camera Pose + 当前的 2D 信息（左右目事件流）。
  - **Temporal Stereo 的作用**：正如你所说，Temporal Stereo 需要知道相机的运动轨迹。它利用刚算出来的 **Current Pose** 结合历史位姿，计算出相机在移动过程中产生的“时间视差”。
  - **结果**：结合 **Static Stereo**（左右目即时视差）和 **Temporal Stereo**，生成新的 3D 点，并更新 **Local 3D Map**。

#### 3. 为什么这个顺序很重要？

这个循环构成了一个典型的自洽系统：

1. **没有 Pose，无法建图**：尤其是 Temporal Stereo，必须精确知道相机从 A 点挪到了 B 点，才能根据像素的移动反推深度。
2. **没有 Map，无法追踪**：追踪线程必须有一个“参照物”（3D Map），才能知道当前看到的 2D 图像代表自己在空间中的什么位置。


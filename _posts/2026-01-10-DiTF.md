---
layout: post
title: "Diffusion Transformer"
date: 2026-01-10 18:45:11 +0800
categories: []
description: DiT, SD3, Flux, PixArt, DiTF
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---
## DiT
**Diffusion Transformer (DiT)** 是一种将 **Transformer 架构**与**扩散模型（Diffusion Models）**相结合的新型生成模型架构。它在图像生成领域逐渐取代了传统的以 UNet 为核心的架构（如早期的 Stable Diffusion），并成为当前最先进模型（如 SD3、Flux、Sora）的核心底座 。
以下是根据您提供的论文资料对 DiT 的详细介绍：
### 1. 核心定义与架构
DiT 摒弃了扩散模型中常用的带有下采样和上采样操作的 UNet 架构，转而使用完全基于 **Transformer** 的块（Blocks）来处理图像数据 。
- **图像 Patch 化**：输入图像或潜空间表示（Latent）首先被切分为一个个小方块（Patches），并展平为一串序列（Tokens），这与 Vision Transformer (ViT) 的处理方式一致 。
- **AdaLN-zero 调制**：这是 DiT 的关键技术之一。它根据扩散的时间步（Timestep）和文本条件，回归出缩放（Scale）和偏移（Shift）参数，通过 **自适应层归一化（AdaLN）** 来调制 Transformer 块内的特征 。
- **多模态交互（MM-DiT）**：在 SD3 等模型中，DiT 演进为多模态架构，拥有独立的流（Streams）来分别处理视觉特征和文本 Token，并通过注意力机制实现两者的双向信息流转 。
### 2. DiT 的主要优势
相比传统的 UNet 架构，DiT 展现出了以下优越性：
- **可扩展性（Scalability）**：DiT 非常适合通过增加参数量和训练数据来提升性能（Scaling law） 。
- **生成质量与鲁棒性**：在细节生成和图像质量方面表现更好，具有更强的空间感知和语义理解能力 。
- **模型效率**：Transformer 架构在现代硬件上具有极高的计算效率 。
## DiT 代表
在当前的人工智能领域，**DiT (Diffusion Transformer)** 架构已经成为生成式模型的主流选择。以下是 DiT 架构最具有代表性的几个模型及其应用场景：
### 1. 基础鼻祖：DiT (Peebles & Xie, 2023)
- **地位**：这是该架构的开山之作。
- **特点**：论文《Scalable Diffusion Models with Transformers》首次证明了使用 Transformer 替代传统的 UNet 作为扩散模型的主干网络是可行的，并且具有极强的**可扩展性（Scaling Law）**。
- **代表性设计**：引入了 **AdaLN-Zero**（自适应层归一化）来将时间步和类别信息注入模型。
### 2. 图像生成领域的标杆
- **Stable Diffusion 3 (SD3)**：
  - **地位**：Stability AI 推出的最强开源图像生成模型之一。
  - **创新**：使用了 **MM-DiT（Multimodal DiT）**，即文本和图像拥有各自独立的权重流，但在注意力层中进行交互。
- **Flux.1**：
  - **地位**：目前开源界公认的画质与文字生成最强的模型（由原 SD 核心团队开发的 Black Forest Labs 推出）。
  - **特点**：基于大规模参数的 DiT 架构，解决了之前模型在复杂指令遵循和人类手部生成上的难题。
- **PixArt-α / PixArt-Σ**：
  - **特点**：主打高效训练的 DiT 模型，通过解耦训练策略，在较小的计算资源下达到了接近 Midjourney 的生成质量。
### 3. 视频生成领域的统治者
- **Sora (OpenAI)**：
  - **地位**：视频生成领域的里程碑。
  - **技术核心**：OpenAI 明确指出 Sora 是一种 **Diffusion Transformer**。它将视频切分为时空补丁（Spacetime Patches），在 Transformer 架构中处理，实现了极长且一致的视频生成。
- **可灵 (Kling) / Vidu**：
  - **地位**：国产大模型中性能顶尖的视频生成模型，其底层架构同样深度参考了 DiT 的时空建模方案。
## PixArt-α
![image-20260110184036225](/images/2026-01-10-DiTF/image-20260110184036225.png){: .img-fluid data-zoomable=""}
## DiTF
DiFT（Diffusion Transformer Feature）是一项发表于 **NeurIPS 2025** 的研究，其核心结论围绕着如何“释放”预训练 Diffusion Transformer (DiT) 模型在视觉感知（如视觉对应任务）中的潜力。
以下是该论文的主要结论：
### 1. 发现 DiT 特征中的“巨量激活”现象 (Massive Activations)
- **现象描述**：与传统的 Stable Diffusion (SD) 模型不同，DiT 模型（如 SD3、Flux）在提取特征时，极少数维度的激活值会比其他维度高出 **100 倍以上** 。
- **空间特性**：这些巨量激活并不是随机分布的，而是**固定集中在极少数特定的通道维度**上（例如 SD3-5 在第 676 维，Flux 在第 154 和 1446 维），且出现在图像的所有 patch token 中 。
- **负面影响**：由于这些极值维度不包含局部语义信息，会导致特征向量在余弦相似度计算时表现出极高的相似性，使得特征变得**无区分度**，从而导致在视觉任务中表现糟糕 。
### 2. 揭示巨量激活与 AdaLN 层的关联
- **核心成因**：研究发现这些巨量激活的维度与 DiT 内部的 **自适应层归一化 (AdaLN)** 产生的残差缩放因子 $$\alpha_k$$ 高度一致 。
- **AdaLN 的双重作用**：虽然 AdaLN 参与了巨量激活的形成，但它同时也具备**定位和抑制**这些激活的能力 。研究证明，通过 AdaLN 进行通道调制，可以有效归一化这些异常值 。
### 3. 提出了免训练的 DiTF 提取框架
- **AdaLN 调制**：DiTF 并不是直接提取原始特征，而是利用模型内置的 AdaLN 对特征进行**通道维度的缩放和偏移调制**。这种操作显著增强了特征的空间语义一致性和边界清晰度 。
- **通道舍弃策略 (Channel Discard)**：对于调制后仍残留的微弱巨量激活维度，DiTF 采用直接**置零**的策略，进一步消除噪声 。
- **无需训练**：该框架是一个 **Training-free**（免训练）的方案，可以直接应用于现有的预训练 DiT 模型 。
### 4. 性能达到 SOTA 并与 DINOv2 互补
- **性能优越**：DiTF 在多个视觉对应任务（如 SPair-71k, AP-10K）上超越了基于 DINOv2 和基于 SD 的模型 。例如，DiTF(Flux) 在 SPair-71k 上的表现比之前的 DIFT 高出 **9.4%** 。
- **特征互补性**：实验显示，DiTF 与 DINOv2 的特征具有互补性。将两者结合使用时，性能会进一步大幅提升（例如在 SPair-71k 上从 64.6% 提升至 **72.2%**） 。
## DiTF vs DINO2
DiT 特征（DiTF）并非简单的直接提取，而是一种**免训练的、基于 AdaLN（自适应层归一化）调制的增强特征提取框架**。
以下是其与 DINOv2 的区别以及优于后者的原因：
### 1. DiT 特征与 DINOv2 的本质区别
- **提取机制不同**：DINOv2 是通过自监督学习直接训练的视觉骨干网络，其特征可直接提取使用 。而 DiT（如 SD3、Flux）在直接提取原始特征时，会由于“**巨量激活**”（Massive Activations）现象导致表现极差 。DiTF 框架通过 DiT 内部自带的 **AdaLN 层**对这些异常激活进行通道维度的调制（缩放和偏移），从而提取出具有语义区分度的特征 。
- **解决的核心问题不同**：DINOv2 侧重于通用视觉表征；而 DiTF 侧重于消除 DiT 特征中极少数固定维度上的极端激活值（这些值比中值大 100 倍以上，且不包含局部信息），使模型能够像 Stable Diffusion (SD) 一样有效地用于视觉感知任务 。
### 2. 为什么 DiT 特征优于 DINOv2？
研究表明，DiTF 在视觉对应（Visual Correspondence）任务中建立了一系列新的性能标杆，超越了 DINOv2 和基于 SD 的模型 ：
- **更强的语义边界和一致性**：经过 AdaLN 调制后的 DiT 特征（Post-AdaLN）在空间语义相干性上显著优于 DINOv2 。可视化显示，它能更清晰地界定物体部位的语义边界，而原始特征或某些自监督特征在区分物体与背景时相对较弱 。
- **强大的生成先验**：DiT 模型（如 SD3、Flux）在大规模文本到图像生成任务中预训练，具备极强的空间感知和语义理解能力 。通过 DiTF 框架“释放”这些潜能后，其特征在处理复杂语义对应（如跨物种对应）时表现出更强的鲁棒性 。
- **特征互补性**：虽然 DiTF 本身性能强劲，但它与 DINOv2 特征具有互补性 。实验显示，将 DiTF 与 DINOv2 特征集成后，性能会进一步大幅提升（例如在 Spair-71k 数据集上从 64.6% 提升至 72.2%） 。
### 3. DiTF 的核心改进手段
1. 
   **AdaLN 通道调制**：利用 DiT 内部的 AdaLN 层，自适应地定位并归一化巨量激活，增强特征的区分度 。
2. 
   **通道舍弃策略（Channel Discard）**：在调制后，主动丢弃（置零）那些仍然存在的、包含局部信息极少的异常维度，以进一步消除对表征学习的负面影响 。

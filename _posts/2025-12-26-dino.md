---
layout: post
title: "DINO 如何用于密集预测"
date: 2025-12-26 13:54:16 +0800
categories: []
description: 如何使用DINO, DPT, RoPE
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

## 资料

- [(3 封私信 / 80 条消息) 万字长文超详解读之DINO全系列—视觉表征对比学习的高峰 - 知乎](https://zhuanlan.zhihu.com/p/1933583851923439816)
- [(3 封私信 / 80 条消息) 万字长文超详解之DINO-V3（DINO全系列之补充篇） - 知乎](https://zhuanlan.zhihu.com/p/1940400858836742367)
- [Inside DINOv2: Architecture Analysis + CIFAR-10 Experiment - YouTube](https://www.youtube.com/watch?v=j2_42Yx_1_w)
- [Self-Supervised Learning Review: from SimCLR to DINOv2 \| Mashaan blog](https://mashaan14.github.io/YouTube-channel/self_supervised_learning/2025_05_19_SSL)

[^1]: [(3 封私信 / 80 条消息) DINOv3 is All You Need? 为什么 DINOv3 发布后，CV 圈感觉“天塌了”？ - 知乎](https://zhuanlan.zhihu.com/p/1986387271688139358)

> DINOv3 的发布，标志着计算机视觉进入了类似 NLP 的“GPT-3 时刻”[^1]。
>
> **对于学术界：**
>
> - **Bad News**：传统的“设计一个新 Backbone”、“魔改 Transformer 模块”刷点数的路子越来越窄了。在 7B 模型面前，微小的架构创新几乎没有意义。
> - **Good News**：新的研究方向被打开了。
>   - **Post-training**：如何更高效地利用这些冻结特征？
>   - **多模态对齐**：DINOv3 展示了初步的文本对齐能力，但这方面远未饱和。
>   - **视频理解**：利用 DINOv3 强大的时序一致性做原生的视频大模型。
>
> 
>
> **对于工业界/工程师：**
>
> - **这是巨大的利好**。你不再需要收集几十万张标注数据去训练一个分割模型。直接下载 DINOv3 的权重，冻结它，用几百张图训练一个轻量级 Head，你就能得到工业级可用的效果。
> - **Deployment**：Meta 提供的蒸馏版小模型（特别是 ViT-Small 和 Base）将是边缘端部署的神器。
>
> **DINOv3 并没有让天塌下来，它只是铺平了地基。** 它把提取“好特征”这件最脏最累最费算力的事做完了。现在的我们，可以站在 70 亿参数的肩膀上，去探索视觉智能更上层的逻辑——这何尝不是一种幸运？

## 如何使用 DINO

对于 dense prediction，一般要使用多个层的特征，比如 VGGT，以及 dinov3 中的应用部分有提及。

1. **多层特征提取：** 在 VGGT 的实现细节中，为了生成高分辨率的密集输出（如深度图和点图），模型将来自 DINOv2 骨干网络不同阶段的特征提供给 **DPT（密集预测 Transformer）头**进行处理。具体而言，VGGT 会提取 DINOv2（ViT-L/14）中**第 4、11、17 和 23 块（blocks，实际上就是layer）**的令牌（tokens），并将这些中间层的特征输入 DPT 进行上采样。
2. **与 DINOv3 结合时的用法：** 在 DINOv3 的后续实验中，研究人员将 VGGT 的图像特征提取器更换为 DINOv3 ViT-L。在这种配置下，他们同样使用了 **4 个中间层特征的拼接（concatenation）** 作为下游模块的输入，而不是仅使用最后一层。实验发现，这种使用多个中间层的方法对 DINOv3 带来了性能提升。

## DPT

R. Ranftl, A. Bochkovskiy, and V. Koltun, "Vision transformers for dense prediction," in *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 12173-12183.

![image-20251226103424453](/images/2025-12-26-dino/image-20251226103424453.png){: .img-fluid}

### Encoder

DPT共需要4层的特征，如果用ViT/dino，那这四层的尺寸是一样的。 

### Reassemble

   Read->Concatenate->Resample

为了更清楚地理解，可以看它在完整流程中的作用：

1. **Read：** 处理 CLS 令牌（这里可能涉及拼接投影等线性运算）。
2. **Concatenate：** 将令牌**摆放**回网格位置。
3. **Resamples：** 使用 $$1 \times 1$$ 和 $$3 \times 3$$ 卷积进行通道投影及空间缩放（这里才进行真正的**卷积运算**）。

**总结：** Concatenate 阶段就像是**拼图**。Read 操作决定了每一块拼图（token）上的内容，而 Concatenate 只是**按照位置把拼图摆好**。真正的“修图”和“放大”工作是由接下来的 Resamples 卷积层完成的。

#### Read

在 DPT（Dense Prediction Transformer）架构中，**Read 操作**是其核心组件“重组操作”（Reassemble）的**第一阶段**。它的主要任务是处理 Vision Transformer (ViT) 输出的特殊令牌，并将令牌序列转换为可以进行空间排列的形式。

以下是 Read 操作的详细介绍：

##### 1. 核心定义与目的

在 ViT 中，输出包含 $$N_p$$ 个图像块令牌（patch tokens）和 **1 个特殊的“读取令牌”（readout token，通常指 CLS token）**。

- **输入：** $$N_p + 1$$ 个令牌。
- **目的：** 将这 $$N_p + 1$$ 个令牌映射回 $$N_p$$ 个令牌，以便后续的“拼接操作”（Concatenate）能将它们按照原始图像位置还原成特征图。
- **数学表达式：** $$Read: \mathbb{R}^{(N_p+1) \times D} \to \mathbb{R}^{N_p \times D}$$。

##### 2. 三种实现方案

DPT 论文评估了处理读取令牌（$$t_0$$）与图像块令牌（$$t_1, \dots, t_{N_p}$$）之间关系的三种不同方式：

- **Readignore（忽略）：** 直接**丢弃读取令牌**，只保留 $$N_p$$ 个图像块令牌。这是最简单的方法，即 $$Read_{ignore}(t) = {t_1, \dots, t_{N_p}}$$。
- **Readadd（相加）：** 将读取令牌的信息**加到所有其他令牌上**。即 $$Read_{add}(t) = {t_1 + t_0, \dots, t_{N_p} + t_0}$$。
- **Readproj（投影/默认方案）：** 通过**拼接后投影**的方式融合信息。将读取令牌与每个图像块令牌拼接，然后通过一个线性层（MLP）将维度投影回原始大小 $$D$$。其公式为：$$Read_{proj}(t) = {mlp(cat(t_1, t_0)), \dots, mlp(cat(t_{N_p}, t_0))}$$。

##### 3. 性能表现与结论

根据来源中的消融实验结果：

- **Readproj 是默认的最优方案**，在单目深度估计等任务中表现略优于其他方案，因为它能更有效地捕获并分配全局信息。
- 相比之下，`Readadd` 的效果甚至差于完全忽略令牌的 `Readignore`。

**总结：** **Read 操作**就像是一个“令牌筛选与融合器”，它决定了如何将 Transformer 学习到的**全局图像表示（读取令牌）\**回馈给各个\**局部特征（图像块令牌）**，从而确保在进入后续的卷积解码阶段前，每个像素级别的特征都已融入了全局上下文信息。

#### Concatenate    

在 DPT（Dense Prediction Transformer）的 **Reassemble** 操作中，**Concatenate（拼接）** 阶段本身**不涉及任何复杂的数学运算或学习参数**，它的本质是一个**形状变换（Reshape/Rearrange）**过程。

#### Resample

这一步才真正涉及空间缩放。

- **输入：** 空间排列好的特征图，尺寸为 $$\frac{H}{p} \times \frac{W}{p}$$，通道数为 $$D$$。
- **输出：** 缩放后的特征图，尺寸为 $$\frac{H}{s} \times \frac{W}{s}$$，通道数为 $$\hat{D}$$（DPT 默认 $$\hat{D} = 256$$）。

Resample 通过两步卷积运算来完成变换：

1. **通道投影：** 首先使用 **1x1 卷积**。这一步负责将来自不同 Transformer 层的令牌维度（如 ViT-Large 的 1024 维）投影到解码器所需的统一维度 $$\hat{D}$$。

2. 空间缩放：

    随后根据目标缩放比例 $$s$$ 与初始图像块大小 $$p$$ 的关系，使用 

   3x3 卷积

    进行调整：

   - **下采样（$$s \ge p$$）：** 使用**步长（strided）为 3x3 的卷积**来降低分辨率。
   - **上采样（$$s < p$$）：** 使用**步长为 3x3 的转置卷积（transpose convolution）**来提升分辨率。

### Fusion

每级上采样2倍。

## Multi-task image restoration guided by robust DINO features

X. Lin, C. Ren, K. C. Chan, L. Qi, J. Pan, and M. H. Yang, "Multi-task image restoration guided by robust DINO features," *arXiv preprint arXiv:2312.01677*, 2023 (v3 revised 2024).

![image-20251226133529550](/images/2025-12-26-dino/image-20251226133529550.png){: .img-fluid}

其核心思路是：传统的图像恢复模型在任务数量增加时性能会下降，而 DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节，因此可以作为一种**退化无关的表示**来引导恢复过程。

以下是 DINO-IR 的具体实现方法和核心组件：

### 1. 核心架构与模块

DINO-IR 基于 **Restormer** 架构，并集成了以下三个关键组件：

- 像素-语义融合模块 (PSF, Pixel-Semantic Fusion)：
  - **目的：** 动态融合 DINOv2 不同层级的特征。由于浅层包含像素信息，深层包含语义信息，该模块负责提取并加权这些特征。
  - **实现：** 采用**门控网络（Gating Network）\**和多个\**专家网络（Expert Networks）**。门控网络会根据输入图像自适应地学习浅层、中层和深层特征的权重，将对恢复任务最有益的特征赋予更高的权重进行融合。
- DINO-Restore (D-R) 适配与融合模块：
  - **目的：** 将 DINOv2 的特征集成到图像恢复主模型中。
  - **实现：** 首先通过适配层调整 PSF 融合特征的通道数和尺度，使其与恢复模型对齐。然后采用**基于自注意力的融合方式**：将适配后的 DINO 特征作为 **Query (Q)**，而将恢复模型的中间特征作为 **Key (K)** 和 **Value (V)**，通过交叉注意力机制实现特征融合。

### 2. DINO 感知对比损失 (DPC Loss)

为了约束模型训练，DINO-IR 提出了一种基于 DINOv2 特征空间的**对比学习损失**：

- **原理：** 提取恢复后的输出图像、原始清晰图像（正样本）和退化输入图像（负样本）在 DINOv2 隐藏层中的特征。
- **目标：** 强制要求输出图像的 DINO 特征在空间中尽可能**靠近清晰目标图像**，并尽可能**远离低质量输入图像**。这种损失利用了 DINOv2 特征区分图像质量的能力来提升视觉效果。

### 3. 方法优势

- **退化鲁棒性：** DINOv2 特征在不同噪声水平和退化类型下表现出极高的稳定性（方差远低于图像像素特征），这使得模型在处理冲突任务（如去噪需要滤除高频，而去模糊需要增强高频）时更加稳定。
- **泛化能力：** 实验证明 DINO-IR 在**未见过的退化级别**（如更高强度的噪声）和**未见过的测试数据集**上具有更好的泛化效果。
- **性能提升：** 在 deraining, denoising, deblurring, dehazing 四项任务的平均 PSNR 表现上，DINO-IR 优于 AirNet 和 PromptIR 等现有先进的多任务恢复方法。

### Note

作者有做额外实验，DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节。

It is known that the features extracted from shallow layersof DINOv2 (M, T, and T 2023) can discern low- and high-quality images。

根据 DINO-IR（基于鲁棒 DINO 特征引导的多任务图像恢复）的研究资料，其提出的 **DINO 感知对比损失（DINO Perception Contrastive Loss，简称 $$L_{DINO}$$ 或 DPC Loss）** 的公式及相关总损失公式如下：

该损失函数旨在通过对比学习，使恢复后的图像在 DINOv2 的特征空间中靠近清晰图像，并远离退化的输入图像。公式表达为：

$$L_{DINO} = L(v, v^+, v^-) = \sum_{i=1}^n w_i \frac{D(\Psi_i(v), \Psi_i(v^+))}{D(\Psi_i(v), \Psi_i(v^-))}$$

**参数含义：**

- **$$v$$**：恢复模型生成的输出图像。
- **$$v^+$$**：正样本，即对应的**清晰目标图像（Ground Truth）**。
- **$$v^-$$**：负样本，即**低质量的退化输入图像**。
- **$$\Psi_i$$**：表示从固定的预训练 DINOv2 模型中提取的第 $$i$$ 个隐藏层特征。
- **$$D(x, y)$$**：表示 $$x$$ 与 $$y$$ 之间的 **$$L1$$ 距离**。
- **$$w_i$$**：对应层级的权重系数。

作者并没有给出是第几层。但太深的层应该没用。

## 处理任意输入大小图片

DINO（包括 DINOv2 和 DINOv3）处理任意大小图片的核心机制在于其 **Transformer 架构的灵活性**、**分块（Patchification）策略**以及**位置编码的动态插值或旋转机制**。

以下是具体的实现方式：

### 1. 灵活的序列长度处理（Set-to-set 架构）

DINO 系列模型基于 Vision Transformer (ViT)。与传统的卷积神经网络不同，Transformer 是一种**“集合到集合”（set-to-set）的架构**，它将图像视为一系列令牌（tokens）。

- **分块机制：** 图像被切分为固定大小的 patch（例如 DINOv2 使用 $$14 \times 14$$，DINOv3 使用 $$16 \times 16$$）。
- **令牌数量随分辨率变化：** 当输入图像变大时，模型只会产生更多的令牌，而 Transformer 的自注意力机制（Self-attention）天然可以处理任意长度的输入序列。

### 2. 位置编码的适配（核心技术）

由于 Transformer 本身无法感知令牌的空间顺序，必须加入位置编码。处理不同分辨率图像的关键在于如何让固定长度的位置编码适应变动的令牌数量：

- **线性插值（DINOv2/DPT 方案）：** 在 DINOv2 和 DPT 中，如果输入图像的分辨率与训练时的分辨率不同，模型会对预训练的**位置嵌入（Position Embeddings）进行线性插值**。这使得模型能够动态适配到新的令牌网格尺寸，确保每个令牌都能获得其在图像中相对位置的信息。
- **旋转位置编码（DINOv3 的 RoPE 机制）：** **DINOv3** 引入了更先进的 **RoPE（Rotary Positional Embeddings）** 机制。它将每个 patch 的坐标分配在一个归一化的 $$[-1, 1]$$ 框内，并在多头注意力操作中根据 patch 间的相对位置应用偏差。
- **无缝缩放：** 依靠 RoPE 和坐标框抖动（box jittering）技术，DINOv3 能够**在不进行任何适配的情况下无缝处理不同分辨率的图像**。实验显示，即使在远超训练分辨率（如 4k 分辨率）的情况下，DINOv3 仍能保持稳定的特征表现。

### 3. 全局感受野的维持

在卷积网络中，感受野随层数增加而受限，但在 DINO 中，由于使用了全局自注意力机制，**每一个阶段（stage）都拥有全局感受野**。

- 这意味着无论图像多大，每个令牌都能与图像中的所有其他令牌进行交互。
- 这种特性确保了模型在处理任意大小图片时，都能产生**全局连贯（globally coherent）**且精细的预测结果。

### 总结

DINO 处理任意大小图片的逻辑可以类比为**“拼图”**：

- **分块**是把图片切成小拼块，图越大拼块越多。
- **Transformer** 是拼图者，他能处理任意数量的拼块。
- **RoPE 或插值编码** 就像是在每一块拼图背面标注坐标的记号笔，通过动态缩放记号的刻度（坐标），拼图者总能知道每一块在大图中的精确位置。

### DINOv2 处理任意大小图像的代码

```python
import numpy as np
import torch
import cv2
import os
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.decomposition import PCA
from skimage.color import hsv2rgb
from transformers import AutoImageProcessor, AutoModel

# --- 1. 配置参数 ---
# 图像文件路径 (已设置为您的文件)
IMAGE_PATH = 'fruits.jpg' 
# DINOv2 模型 ID (Base 版本，公开且无需权限)
MODEL_ID = "facebook/dinov2-base" 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# DINOv2-base 默认 Patch size 为 14x14
PATCH_SIZE = 14 
# 设置一个最小的安全尺寸，防止原图太小
MIN_SIZE = 224 

def visualize_dinov2_features_variable_res(image_path, model_id, device, patch_size, min_size):
    """
    加载 DINOv2 模型，提取 Patch 特征，使用 PCA 降维并可视化。
    图像尺寸会调整到最接近原始尺寸且是 Patch Size 的整数倍。
    """
    if not os.path.exists(image_path):
        print(f"错误: 图像文件未找到于 '{image_path}'。请检查路径并重试。")
        return

    # --- 2. 初始化模型和处理器 ---
    print(f"正在加载 DINOv2 模型: {model_id} 到 {device}...")
    try:
        processor = AutoImageProcessor.from_pretrained(model_id)
        model = AutoModel.from_pretrained(model_id).to(device)
        model.eval() # 评估模式
    except Exception as e:
        print(f"模型加载失败。请检查模型 ID 或网络连接。\n错误信息: {e}")
        return

    # --- 3. 图像处理与特征提取 (重点修改部分) ---
    img = Image.open(image_path).convert("RGB")
    W_orig, H_orig = img.size
    print(f"原始图像尺寸 (W x H): {W_orig} x {H_orig}")

    # a. 计算目标输入尺寸 (必须是 PATCH_SIZE 的整数倍)
    # 取最接近原始尺寸且小于等于原始尺寸的 PATCH_SIZE 倍数
    H_target = (H_orig // patch_size) * patch_size
    W_target = (W_orig // patch_size) * patch_size
    
    # 确保尺寸不小于最小安全尺寸
    H_target = max(H_target, min_size)
    W_target = max(W_target, min_size)

    print(f"模型目标输入尺寸 (W x H): {W_target} x {H_target}")
    
    # b. 预处理
    # 显式传递 size 和 crop_size 参数，控制预处理器的缩放行为
    inputs = processor(
        images=img, 
        return_tensors="pt",
        size=(H_target, W_target), # 缩放或调整到目标尺寸
        crop_size=(H_target, W_target), # 确保不进行中心裁剪
        do_center_crop=False # 明确禁用中心裁剪
    ).to(device)

    # 实际输入模型张量的尺寸
    h_input = inputs['pixel_values'].shape[2]
    w_input = inputs['pixel_values'].shape[3]
    
    # 重新计算 Patch 网格尺寸 (H, W)
    h = h_input // patch_size
    w = w_input // patch_size
    
    # c. 提取特征
    with torch.no_grad():
        # **inputs 解包字典作为命名参数
        outputs = model(**inputs) 
        features = outputs.last_hidden_state.squeeze(0).cpu().numpy()

    # d. 移除 CLS Token
    if features.shape[0] == h * w + 1:
        features = features[1:] 
        print(f"已移除 CLS Token。剩余 Patch 特征形状: {features.shape}")
    else:
        print(f"Patch 特征形状: {features.shape}")

    # --- 4. PCA 降维 ---
    print("正在进行 PCA 降维...")
    pca = PCA(n_components=3).fit(features)
    pca_features = pca.transform(features)

    # 归一化到 [0, 1] 范围
    pca_min = pca_features.min(axis=0)
    pca_max = pca_features.max(axis=0)
    denominator = pca_max - pca_min
    denominator[denominator == 0] = 1e-8 
    pca_features_norm = (pca_features - pca_min) / denominator

    # --- 5. 可视化映射 ---

    # a. 重塑为网格形状 (H, W, 3)
    pca_grid = pca_features_norm.reshape(h, w, 3)

    # b. 映射到 HSV 颜色空间 
    hsv_image = np.zeros_like(pca_grid)
    hsv_image[..., 0] = pca_grid[..., 0]  # Hue (色调)
    hsv_image[..., 1] = 0.8              # Saturation (饱和度)
    hsv_image[..., 2] = pca_grid[..., 1]  # Value (亮度)

    # 转换为 RGB 颜色
    rgb_vis = hsv2rgb(hsv_image)
    
    # c. 缩放可视化结果到原始图像大小
    rgb_vis_upscaled = cv2.resize(
        rgb_vis, 
        (W_orig, H_orig), # 使用原图尺寸进行缩放
        interpolation=cv2.INTER_NEAREST # 最近邻插值保持 Patch 块状效果
    )

    # --- 6. 显示和保存结果 ---
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.title("原始图像 (Original Image: fruits.jpg)")
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(rgb_vis_upscaled)
    plt.title(f"DINOv2 特征 PCA 可视化 ({W_target}x{H_target} 输入)")
    plt.axis('off')

    plt.tight_layout()
    output_filename = "dinov2_fruits_pca_visualization.png"
    plt.savefig(output_filename)
    print(f"可视化结果已保存为 {output_filename}")


if __name__ == "__main__":
    visualize_dinov2_features_variable_res(IMAGE_PATH, MODEL_ID, DEVICE, PATCH_SIZE, MIN_SIZE)
```


---
layout: post
title: "Mamba"
date: 2025-12-25 22:03:54 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left # or right
typora-root-url: ../
---

## Mamba

Mamba 处理的是序列的建模问题，由输入 $$x(t)$$ 得到输出 $$y(t)$$，所以它更像是一个特征提取。它的优势是对于序列长度 $$L$$ 来说，复杂度线性。

S4 是 structured state space sequence model. 结构化指的是A 是 HiPPO 矩阵（是这样吗？）。但是它的参数依然是时不变的。

Mamba 是S6，加了 selective scan，使得参数时变，依赖于输入 $$x$$。

如果有一定的基础，可以直接看这个对比。其中 $$B,L,D,N$$ 分别是 batch，序列长度，输入（输出）的特征维度，内部特征的维度。

![image-20250904111607190](/images/2025-12-25-mamba/image-20250904111607190.png){: .img-fluid}

![image-20250904112651698](/images/2025-12-25-mamba/image-20250904112651698.png){: .img-fluid}

![image-20250904113042955](/images/2025-12-25-mamba/image-20250904113042955.png){: .img-fluid}

它会独立的把 $$x$$ 的每一通道到输出 $$y$$ 的每一维，中间通过一个更维的隐藏状态 $$h$$。如上图，$$x$$ 的每一通道是独立计算的，因此 $$\bar{A}$$，$$\bar{B}$$  的尺寸为 $$(B,L,D,N)$$。但是$$x$$ 的每一通道的参数和整个 $$x$$ 有关系。 $$\bar{A}$$，$$\bar{B}$$ 的离散化见下图公式（4），结合上面的表格，在计算中，尺寸有变化，因此在代码中，有大量的 einsum 操作。

由 $$A$$，$$B$$ 求 $$\bar{A}$$，$$\bar{B}$$ 的过程是离散化。这个和信号与系统的知识有关。它内在的过程还是连续的，但是取值的时间是离散的，所以是去算一个微分方程+初值在 $$\Delta$$ 时间后的状态。

![image-20250904111925419](/images/2025-12-25-mamba/image-20250904111925419.png){: .img-fluid}

### 矩阵 $$A$$

在 Mamba 模型中，矩阵 A 是对角阵（diagonal matrix），而非 HiPPO 阵。HiPPO 是一种用于初始化矩阵 A 的策略，主要出现在早期的 S4 模型中，但 Mamba 采用了对角结构并使用不同的初始化方式，以实现更高效的计算。

在 Mamba 中，虽然矩阵 A A A 被简化为对角矩阵，但其对角元素的初始化方式仍然受到 HiPPO 矩阵的启发，具体体现在以下几个方面：

1. 特征值的分布
   - HiPPO 矩阵的特征值通常被设计为负实部，以确保系统的稳定性。在 Mamba 中，对角矩阵 A A A 的对角元素（即其特征值）被初始化为负值，模仿 HiPPO 矩阵的稳定性特性。这确保了 Mamba 在处理长序列时不会出现数值不稳定的问题。
   - 具体来说，Mamba 的对角元素通常被初始化为负的、对数分布的值（如 −1,−2,−4,…-1, -2, -4, \ldots−1,−2,−4,… 或类似的分布），这与 HiPPO 矩阵的特征值分布有相似的动机，即通过控制特征值的范围来平衡短期和长期记忆。
2. 动态生成对角元素
   - Mamba 的对角矩阵 A A A 的对角元素是由网络参数动态生成的，而不是固定的。这些参数在训练开始时会根据 HiPPO 的思想进行初始化。例如，Mamba 可能通过对数尺度（log-scale）初始化对角元素，以模拟 HiPPO 矩阵在不同时间尺度上的记忆能力。
   - 这种初始化方式使得 Mamba 能够在训练初期就具备捕捉长程依赖的能力，而无需像 S4 那样依赖稠密的 HiPPO 矩阵。
3. 高效性与简化
   - HiPPO 矩阵通常是稠密的，计算成本较高。Mamba 通过将 A A A 限制为对角矩阵，极大地降低了计算复杂度（从 O(N2) O(N^2) O(N2) 降到 O(N) O(N) O(N)，其中 N N N 是状态维度）。
   - 尽管结构上简化了，Mamba 仍然通过借鉴 HiPPO 的初始化策略，保留了其在序列建模中的核心优势，如对长序列的记忆能力和稳定性。

### GPU SRAM + GPU HBM

这一点也是 mamba 效率的关键。

参见1 [A Visual Guide to Mamba and State Space Models](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state) Hardware-aware Algorithm

以及原论文 sec3.3.2

## 参考文献

### 博客

1. [A Visual Guide to Mamba and State Space Models](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)

### 代码

#### 自然语言处理

1. [GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.](https://github.com/johnma2006/mamba-minimal/tree/master)

   这个代码给出了 mamba 的简化结构代码，没有训练，但是有调用预训练模型（[需要从 huggingface 上下载，大小1.5G](https://huggingface.co/state-spaces/mamba-370m/tree/main)）。

   可以理解它的底层代码。

   

2. [Building Mamba from Scratch: A Comprehensive Code Walkthrough](https://readmedium.com/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049)

这个代码是整套的，包含底层结构和训练，使用 enwiki8 数据。可以运行，我把它放到了 colab 上。

![image-20250904095945491](/images/2025-12-25-mamba/image-20250904095945491.png){: .img-fluid}

刚开始的 loss 和初始化比较有关系。（昨天因为这个原因，在 M4 上暂停了，回头再跑一次）。

#### 图像

[GitHub - pprp/Vision-Mamba-CIFAR10](https://github.com/pprp/Vision-Mamba-CIFAR10)
---
layout: post
title: "Proximal"
date: 2024-12-30 00:25:27 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

### 近端分裂方法要解决的优化问题

近端分裂方法（Proximal Splitting Methods）主要针对凸优化问题，特别是那些可以表述为多个凸函数之和的形式。具体来说，它旨在求解如下优化问题：

$$
\min_{x \in \mathbb{R}^n} \, f(x) + g(x)
$$

其中，$$ f(x) $$ 是一个光滑的、可微分的凸函数（例如，具有Lipschitz连续梯度），而 $$ g(x) $$ 是一个凸函数但可能非光滑（例如，L1范数正则化项或指示函数，用于强制约束）。更一般地，该方法可以扩展到多个函数的和，如 $$ \min_x \sum_{i=1}^m h_i(x) $$，其中每个 $$ h_i $$ 是凸的。这种问题常见于信号处理、图像恢复、机器学习等领域，例如压缩感知或稀疏回归，其中非光滑项用于促进稀疏性或鲁棒性。近端分裂方法的核心是通过近端算子（proximity operator）来处理非光滑部分，将复杂问题分解成易于求解的子问题，从而实现高效迭代求解。

在更广泛的框架下，该方法可以处理带约束的凸优化问题，通过引入分裂技术（如Douglas-Rachford分裂）来统一多种算法，确保在凸假设下全局收敛。

### FBS（Forward-Backward Splitting，前向-后向分裂算法）的展开

FBS 是近端分裂方法的一种基础实现，专门针对上述优化问题 $$ \min_x f(x) + g(x) $$，其中 $$ f $$ 是光滑凸函数（梯度Lipschitz连续），$$ g $$ 是凸但可能非光滑的函数。该算法将问题拆分成两个步骤：前向步骤处理光滑部分（使用梯度下降），后向步骤处理非光滑部分（使用近端算子）。它是一种迭代方法，适用于大规模问题，如信号去噪或稀疏优化。

#### 算法步骤
FBS 的基本迭代形式如下（假设步长 $$ \gamma > 0 $$，且 $$ 0 < \gamma < 2/L $$，其中 $$ L $$ 是 $$ \nabla f $$ 的Lipschitz常数）：

1. **初始化**：选择起始点 $$ x^0 \in \mathbb{R}^n $$，设置迭代次数 $$ k = 0 $$。
2. **前向步骤**：计算光滑函数的梯度更新：
   $$
   y^{k+1} = x^k - \gamma \nabla f(x^k)
   $$
   这类似于梯度下降，朝最小化 $$ f $$ 的方向移动。
3. **后向步骤**：应用非光滑函数的近端算子：
   $$
   x^{k+1} = \prox_{\gamma g}(y^{k+1})
   $$
   其中，近端算子定义为：
   $$
   \prox_{\gamma g}(y) = \arg\min_z \left( g(z) + \frac{1}{2\gamma} \\mid z - y\\mid ^2 \right)
   $$
   这确保了非光滑约束的满足，例如对于 $$ g(x) = \\mid x\\mid _1 $$，近端算子对应软阈值操作。
4. **迭代**：增加 $$ k = k + 1 $$，重复步骤2-3，直到收敛准则满足（如 $ \|x^{k+1} - x^k\| < \epsilon \））。

FBS 的收敛性：在凸假设下，该算法全局收敛到最优解，且函数值序列以 \( O(1/k) $$ 的速率收敛。可以通过加速变体（如FISTA）提升到 $$ O(1/k^2) $。它可以扩展到准牛顿版本，使用Hessian近似来加速。

#### 应用示例
- **信号处理**：用于Lasso回归或总变差去噪。
- **机器学习**：稀疏学习或时间序列建模。

### ADMM（Alternating Direction Method of Multipliers，交替方向乘子法）的展开

ADMM 是一种更通用的近端分裂方法变体，针对带线性约束的凸优化问题：

$$
\min_{x,z} \, f(x) + g(z) \quad \text{s.t.} \quad Ax + Bz = c
$$

其中，$$ f $$ 和 $$ g $$ 是凸函数（可能非光滑），$$ A, B $$ 是线性算子。该算法结合了增广拉格朗日方法和分裂技术，通过交替更新变量来求解子问题，便于并行计算，适用于分布式优化、大规模问题如模型预测控制（MPC）或图像重建。

#### 算法步骤
ADMM 使用增广拉格朗日函数：

$$
L_\rho(x, z, y) = f(x) + g(z) + y^T (Ax + Bz - c) + \frac{\rho}{2} \\mid Ax + Bz - c\\mid ^2
$$

其中，$$ y $$ 是拉格朗日乘子，$$ \rho > 0 $$ 是罚参数。迭代步骤如下：

1. **初始化**：选择起始点 $$ x^0, z^0, y^0 $$，设置 $$ k = 0 $$。
2. **x-更新**：固定 $$ z^k, y^k $$，最小化关于 $$ x $$ 的子问题：
   $$
   x^{k+1} = \arg\min_x L_\rho(x, z^k, y^k)
   $$
   这通常通过闭形式解或近端算子求解（例如，如果 $$ f $$ 是二次的，则为线性系统）。
3. **z-更新**：固定 $$ x^{k+1}, y^k $$，最小化关于 $$ z $$ 的子问题：
   $$
   z^{k+1} = \arg\min_z L_\rho(x^{k+1}, z, y^k)
   $$
   同样，常使用近端算子。
4. **乘子更新**：更新双变量以强制约束：
   $$
   y^{k+1} = y^k + \rho (A x^{k+1} + B z^{k+1} - c)
   $$
5. **迭代**：增加 $$ k = k + 1 $$，重复步骤2-4，直到残差 $$ \\mid A x^{k+1} + B z^{k+1} - c\\mid  < \epsilon $$ 和对偶残差满足收敛条件。

ADMM 的收敛性：在凸假设下，全局收敛到最优解，且对 $$ \rho $$ 的选择鲁棒。可以通过共识形式扩展到分布式设置。

#### 应用示例
- **优化控制**：MPC中的二次规划。
- **统计学习**：鲁棒PCA或分布式机器学习。
- **信号处理**：扩散器相机重建或张量完成。
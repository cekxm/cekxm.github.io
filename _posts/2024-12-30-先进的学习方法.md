---
layout: post
title: "先进的学习方法"
date: 2024-12-30 00:13:46 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

## 先进的学习方法

### Self-Supervised Learning

The motivation behind Self-supervised learning is to learn useful representations of the data from **unlabelled pool of data** using self-supervision first and then fine-tune the representations with **few labels** for the supervised downstream task. The downstream task could be as simple as image classification or complex task such as semantic segmentation, object detection, etc.

The fundamental idea for self-supervised learning is to create some auxiliary **pretext task** for the model from the input data itself such that while solving the auxiliary task, the model learns the underlying structure of the data(for instance the structure of the object in case of image data).

自监督只是说要从未标定数据中学习得到数据的结构。但是如何学习并没有指定。Constrastive Learning 是一种学习方法。

#### pretext task

pretext 是托词的意思。

翻译成代理任务，或前置任务

### Constrastive Learning

Constrastive Learning 算是自监督学习的一种方法。暂时总结为：**Contrastive learning 是一种使用 metric learning 的自监督学习。**

![1_fdAU4VJtnclv0rGrfzUu4g](/images/2024-12-30-先进的学习方法/1_fdAU4VJtnclv0rGrfzUu4g.png){: .img-fluid}

需要如下东西：

- x1, x2 为 positive， x3 为 negative。既然是 unlabelled data，需要一个 pretext task 来建立 positive 和 negative pair. 比如由图像的一部分预测图像的另一部分，则预测结果patch和相应位置的patch就是 positive pair，预测结果patch和其他位置或其他图片的patch就是 negative pair。
- 有一个编码器 f，把patch编码到表示空间。
- 还需要一个 Loss，来表示 f(x1) and f(x2) should be similar to each other and for a negative input x3, f(x1) and f(x2) both should be dissimilar to f(x3).

#### Loss

- 可以是 pair 的 loss，但效果不理想
- triplet loss
- 一对多（比如一个p，多个 n） loss，类似与 softmax

### [(Supervised ) Deep Metric Learning](https://hav4ik.github.io/articles/deep-metric-learning-survey)

![image-20220419093811628](/images/2024-12-30-先进的学习方法/image-20220419093811628.png){: .img-fluid}

可以看出，Deep Metric Learning 是有监督的。实际上 contrastive learning 用到了 Deep Metric Learning，在做这个深度度量学习之前，得构造出 positive 和 negative。

Deep metric learning 的核心就是

- 编码器
- Loss

### 知识蒸馏

Data-Distortion Guided Self-Distillation for Deep Neural Networks 中对知识蒸馏的分类。

![image-20220421072617110](/images/2024-12-30-先进的学习方法/image-20220421072617110.png){: .img-fluid}

这个分类不一定被广泛接受。

### 自知识蒸馏

这个应该还没有明确的定义。取[这篇博文](https://developer.aliyun.com/article/791081)的分类：

自蒸馏(self knowledge distillation)是指不通过新增一个大模型的方式找到一个教师模型，同样可以提供有效增益信息给学生模型，这里的教师模型往往不会比学生模型复杂，但提供的增益信息对于学生模型是有效的增量信息，以提升学生模型效率。该方式可以避免使用更复杂的模型，也可以避免通过一些聚类或者是元计算的步骤生成伪标签。目前该方法在学术界较为新颖，从2020年开始逐渐有顶会浮现相关论文，主要探索任务也较为丰富，在CV、NLP、GNN上均有尝试、任务类型也包括self supervised、unsupervised、semi supervised。

由于没有现成综述论文，在对学术界近两年所有自蒸馏先关论文阅读后做下述粗糙概述，便于对该方向更深入地理解。

根据目前了解到的信息，自蒸馏的方法可以从“增益信息的来源”为维度进行分类，主要分为三大类：

- 伪孪生网络。孪生网络是指两个weighted share的网络，自监督任务中较为流行；伪孪生网络便是两个较为相似且权重独立的网络，在自蒸馏中，一般伪孪生网络使用的teacher和student模型是同一个模型 在这个大类中，可以在时间维度细分为两个子类：

	- 同步蒸馏。例如类似自监督学习的方式，在同一个step中，使用两个一样的model作为伪孪生网络进行自蒸馏
	- 多阶段蒸馏。例如可以使用前几个epoch的model作为teacher蒸馏后几个epoch的student model
- 类Deep Supervision。即将模型中较深层网络结构作为teacher去蒸馏原模型中较浅层的网络结构
- 第三类就是上述两类的混合使用。

#### 同步蒸馏

##### 1 Distill on the Go: Online knowledge distillation in self-supervised learning

2021 CVPRW

![image-20220419115454518](/images/2024-12-30-先进的学习方法/image-20220419115454518.png){: .img-fluid}

本文的目标是学习得到 feature representation，即 embedder. 常规做法就是通过 contrastive learning。论文用了随机数据增强，同一张图像的不同随机增强算是 positive pair，不同图像的随机增强是 negative pair。

论文的创新点：在自监督学习的基础上进行同步蒸馏。具体做法：有两个 embedder，一个 embedder 对同一个sample 产生两个 embeddings Z', Z''. 这两个之间有相似性衡量 sim(Z', Z'')。自知识蒸馏要求两个embedder 的相似性衡量的 KL 很小。

We believe that the additional supervision in KD regarding the relative differences in similarity between the
reference sample and other sample pairs and the collaboration between multiple models can assist the optimization of the smaller model. 

实际在训练时，loss 是整合在一起的，并没有分阶段训练。

![image-20220419124453914](/images/2024-12-30-先进的学习方法/image-20220419124453914.png){: .img-fluid}

![image-20220419124507017](/images/2024-12-30-先进的学习方法/image-20220419124507017.png){: .img-fluid}

其中，$$L_{cl}$$ 是对比学习的损失函数。

##### Data-Distortion Guided Self-Distillation for Deep Neural Networks

![image-20220421072722950](/images/2024-12-30-先进的学习方法/image-20220421072722950.png){: .img-fluid}

这个思路和对比学习很像，但是它不分正样本和负样本，只有正样本。loss 有三部份组成。

- 特征的 MMD
- logit 的 KL
- 分类的 cross entroy（监督的来源）

对照一下代码

```python
def DDGSD(net, inputs, targets, criterion_cls, criterion_div):
    loss_div = torch.tensor(0.).cuda()
    loss_cls = torch.tensor(0.).cuda()

    inputs = torch.cat(inputs, dim=0)
    batch_size = inputs.size(0) // 2
    logit, features = net(inputs, embedding=True)
    loss_cls += criterion_cls(logit, torch.cat([targets, targets], dim=0)) / 2
    loss_div += criterion_div(logit[:batch_size], logit[batch_size:].detach())
    loss_div += criterion_div(logit[batch_size:], logit[:batch_size].detach())
    loss_div += 5e-4 * (features[:batch_size].mean()-features[batch_size:].mean()) ** 2
    logit = (logit[batch_size:] + logit[:batch_size]) / 2
    return logit, loss_cls, loss_div
```

可以看到，网络只有一个，部分 teacher / student。loss 也只有一个，整个 batch 分为两部分，即 $$B^a$$, $$B^b$$。



#### 多阶段蒸馏

![image-20220419115525189](/images/2024-12-30-先进的学习方法/image-20220419115525189.png){: .img-fluid}

#### 类Deep Supervision

![image-20220419114908102](/images/2024-12-30-先进的学习方法/image-20220419114908102.png){: .img-fluid}

- 已 resnet 为例，将 resnet 分阶段，引出不同阶段的特征送入多个分类器。图中，resnet 本身是最深的，是推断时要用的。还有三个浅层特征+分类器（bottleneck+fc+softmax），浅层分类器在推断时是可去除的。
- resnet 本身是教师
- 学生网络是那三个浅层特征对应的分类器
- Loss。包含三部分
  - Cross entropy loss from labels to not only the deepest classifier, but also all the shallow classifiers
  - KL (Kullback-Leibler) divergence loss under teacher’s guidance. The KL divergence is computed using softmax outputs between students and teachers, and introduced to the softmax layer of each shallow classifier
  - L2 loss from hints. It can be obtained through computation of the L2 loss between features maps of the deepest classifier and each shallow classifier.  应该是指教师网络的final 特征和 bottleneck 的输出之间

#### 和 Constrastive Learning 的区别

- 一般自知识蒸馏里面会用到 Constrastive Learning。是不是主要特征呢？
- 不管如何，蒸馏要有教师和学生，而Constrastive Learning没有这个要求。


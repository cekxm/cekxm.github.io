---
layout: post
title: "Lifeifei"
date: 2024-12-30 00:31:35 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

## 计算机视觉

### 李飞飞 on 计算机视觉的发展方向

计算机视觉的未来走向何方？其他指路明灯正在召唤。其中最重要的领域之一是具身人工智能：即用于导航、操作和指令遵循等任务的机器人技术。这不一定意味着要创造点头哈腰、双腿行走的类人机器人；任何在空间中移动的有形智能机器都是具身人工智能的一种形式，无论是自动驾驶汽车、扫地机器人还是工厂中的机械臂。正如ImageNet旨在代表广泛多样的真实世界图像一样，具身人工智能的研究需要应对人类任务的复杂多样性，从叠衣服到探索一座新城市。

另一个指路明灯是视觉推理：例如，理解二维场景中的三维关系。想想为了遵循看似简单的指令——“把金属杯子拿回到麦片碗的左边”——所需的视觉推理。遵循这样的指令当然不仅仅需要视觉，但视觉是一个必不可少的组成部分。

理解场景中的人物，包括社会关系和人类意图，又增加了一个复杂性层次，这种基本的社会智能是计算机视觉的另一个指路明灯。例如，即使是五岁的孩子也能猜到，如果一个女人抱着一个小女孩坐在腿上，那么这两个人很可能是母女，如果一个男人打开冰箱，他可能饿了；但计算机还没有足够的智能来推断这些事情。计算机视觉和人类视觉一样，不仅仅是感知；它还具有深刻的认知性。毫无疑问，所有这些指路明灯都是巨大的挑战，比ImageNet曾经面临的挑战更大。审查照片以试图识别狗或椅子是一回事，而思考和驾驭无限的人和空间世界则是另一回事。但这绝对是一系列值得追求的挑战：随着计算机视觉智能的展开，世界可以变得更美好。医生和护士将拥有额外的不 fatigue 的眼睛来帮助他们诊断和治疗病人。汽车将更安全地行驶。机器人将帮助人类勇敢地进入灾区，拯救被困和受伤的人。科学家们在能够看到人类看不到的东西的机器的帮助下，将发现新的物种、更好的材料和未知的领域。

### VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models

语言是人类提炼和交流他们对世界的知识和经验的压缩媒介。大型语言模型 (LLM) 已成为一种很有前途的方法来捕捉这种抽象，通过投射到语言空间来学习表示世界 [14]。虽然这些模型被认为可以将可泛化的知识内化为文本，但如何利用它来使具身智能体在现实世界中进行物理行动仍然是一个问题。我们着眼于将抽象的语言指令（例如，“布置餐桌”）在机器人动作中进行接地的[5]问题。先前的工作利用词汇分析来解析指令 [6–8]，而最近，语言模型已被用于将指令分解为文本步骤序列 [9–11]。然而，为了实现与环境的物理交互，现有方法通常依赖于一组预定义的运动基元（即技能），这些基元可以由 LLM 或规划器调用，并且由于缺乏大规模的机器人数据，对个体技能习得的这种依赖通常被认为是系统的主要瓶颈。因此，问题就出现了：我们如何在更精细的动作层面上利用 LLM 丰富的内化知识用于机器人，而无需为每个单独的基元进行费力的数据收集或手动设计？

为了应对这一挑战，我们首先注意到，LLM 以文本形式直接输出控制动作是不切实际的，这些控制动作通常由高维空间中的高频控制信号驱动。然而，我们发现 LLM 擅长推断语言调节的示能性和约束，并且通过利用其代码编写能力，它们可以通过编排感知调用（例如，通过 CLIP [12] 或开放词汇检测器 [13–15]）和数组操作（例如，通过 NumPy [16]）来组合将它们在视觉空间中接地的密集 3D 体素图。例如，给定指令“打开最上面的抽屉，并注意花瓶”，可以提示 LLM 推断：1) 应该抓住最上面的抽屉把手，2) 需要将把手向外平移，以及 3) 机器人应该远离花瓶。通过生成 Python 代码来调用感知 API，LLM 可以获得相关物体或部分的空间几何信息，然后操纵 3D 体素，以在观察空间中的相关位置指定奖励或成本（例如，把手区域被分配高值，而花瓶周围被分配低值）。最后，组合的值图可以作为运动规划器的目标函数，直接合成实现给定指令的机器人轨迹，而无需为每个任务或为 LLM 提供额外的训练数据。图 1 显示了一个说明图和我们考虑的一部分任务。

我们将这种方法称为 VOXPOSER，这是一种从 LLM 中提取示能性和约束，以在观察空间中组合 3D 值图以指导机器人交互的公式。该方法没有依赖通常数量或变异性有限的机器人数据，而是利用 LLM 进行开放世界推理，并利用 VLM 在基于模型的规划框架中进行可泛化的视觉接地，从而直接实现物理机器人动作。我们展示了其针对各种日常操作任务的开放集指令和开放集物体的零样本泛化能力。我们进一步展示了 VoxPoser 如何也能从有限的在线交互中受益，以有效地学习涉及丰富接触交互的动力学模型。

### Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making

问题：我们的目标是评估大型语言模型 (LLM) 在具身决策方面的表现。虽然大量工作一直在利用 LLM 进行具身环境中的决策，但我们仍然缺乏对其性能的系统性理解，因为它们通常应用于不同的领域，用于不同的目的，并基于不同的输入和输出构建。此外，现有评估往往只依赖于最终成功率，这使得我们难以 pinpoint LLM 缺少什么能力以及问题出在哪里，这反过来又阻碍了具身智能体有效且有选择地利用 LLM。

方法：为了解决这些局限性，我们提出了一个通用接口（具身智能体接口），它支持各种类型任务的形式化以及基于 LLM 的模块的输入输出规范。具体来说，它允许我们统一：1) 涉及状态和时间扩展目标的广泛的具身决策任务；2) 四种常用的基于 LLM 的决策模块：目标解释、子目标分解、动作排序和转换建模；以及 3) 一系列精细的指标，这些指标将评估分解为各种类型的错误，例如幻觉错误、可供性错误、各种类型的规划错误等。

结论：总而言之，我们的基准测试对 LLM 在不同子任务中的表现进行了全面而系统的评估，指出了基于 LLM 的具身人工智能系统的优势和劣势，并为在具身决策中有效且有选择地使用 LLM 提供了见解。
---
layout: post
title: "World Models, JEPA"
date: 2026-02-11 20:32:53 +0800
categories: []
description: Marble, world labs, world model, JEPA
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---
## Marble, World Labs
[李飞飞世界模型公司一年估值暴涨5倍！正洽谈新一轮5亿美元融资](https://mp.weixin.qq.com/s/x63d39mcbJuT5dMCD_8Prw)
> Marble可以根据文本或图像提示，直接生成可探索的3D世界。
>
> 在传统流程中，3D内容通常从**人工构建的多边形网格（polygon meshes）**开始：
>
> 场景由大量微小的三角形拼接而成，再交由渲染引擎处理，工程成本高、制作周期长，也很难规模化。
>
> Marble则采用了3D高斯溅射（3D Gaussian Splatting，3DGS）技术，用数以百万计的半透明点来表示场景结构，从而在保持较高渲染效率的同时，实现更细腻的视觉效果。
>
> 更重要的是，Marble并不只追求“看起来真实”。它还会同时输出所谓的“碰撞网格（collider meshes）”——
>
> 这是一种牺牲外观细节、换取计算速度的几何表示，专门用于物理仿真和机器人模拟。
>
> 在交互层面，Marble还提供了**Chisel**工具：用户可以先用简单几何体快速“搭骨架”，再生成不同风格的细节版本，这是朝着可控、可编辑的世界构建迈出的一步。
>
> 此外，World Labs近期还开放了World API，开发者可以在应用中通过文本、图像或视频，直接生成可探索的3D世界，把“世界模型”嵌入到具体产品之中。
>
> 在战略层面，李飞飞本人也曾多次明确表态：
>
> **世界模型是实现空间智能的关键，是继大语言模型（LLM）之后，AI下一个十年的核心主线。**
>
> 在她的框架中，真正具备空间智能的世界模型，至少需要三种能力：
>
> - 生成（Generative）：能创造遵守物理定律、空间一致的世界；
> - 多模态（Multimodal）：能处理从图像、视频到动作的多模态输入；
> - 交互（Interactive）：能预测世界随时间演变或互动的状态。
>
> 也正因如此，世界模型被视为一种**“基础设施型能力”**。
>
> 一旦成立，它的影响将不只局限在单一应用场景，而会向多个方向扩散：
>
> - **AIGC：3D资产生成、游戏世界构建、影视制作与虚拟拍摄。**
> - **具身智能：机器人、仿真环境、现实世界任务执行。**
>
> 以具身智能为例，世界模型提供的是对环境的可预测表示，在此之上，才能叠加真正有效的决策与控制。
>
> 包括DP（Diffusion policy）、RL等控制方法，以及对智能体行为进行系统性评估（如光轮智能)。
> 要理解 **Chisel 工具中的简单几何体（Proxy Geometry）** 与 **3DGS（3D Gaussian Splatting）中的高斯球（Gaussians）** 之间的关系，我们可以从“骨架”与“血肉”的比喻切入。
> 它们之间并非简单的替代关系，而是一种**约束与填充、宏观与微观**的耦合关系。




### 1. 空间上的约束关系：容器与内容
在 Marble 的工作流中，Chisel 提供的简单几何体（如方块、圆柱、球体）充当了**“空间容器”**。
- **简单几何体（骨架）：** 定义了场景的宏观布局和物理边界。它告诉 AI：“这里应该有一张桌子，高度是多少，长宽是多少。”
- **3DGS 高斯球（血肉）：** AI 会在这些几何体定义的空间范围内，填充数以百万计的 3D 高斯点。这些点带有颜色、不透明度、协方差（形状）等信息，负责渲染出大理石的纹理、木头的色泽或是光线的反射。

### 2. 数学上的参数关联
Chisel 操作的几何体实际上是在为生成模型提供**先验条件（Prior）**。它们的关系可以从以下三个维度理解：
| **维度**       | **Chisel 简单几何体 (Proxies)**            | **3DGS 高斯球 (Splatting)**          |
| -- |  |  |
| **密度控制**   | 几何体的体积决定了高斯点生成的**范围**。   | 在几何体内部或表面生成高斯点。       |
| **初始化引导** | 几何体的表面法线可以引导高斯球的**朝向**。 | 确保高斯球的扁平方向贴合几何体表面。 |
| **物理对应**   | 直接转化为 **Collider Mesh（碰撞网格）**。 | 仅负责视觉呈现，不参与物理碰撞计算。 |

### 3. 为什么不直接用高斯球建模？
你可能会问：既然 3DGS 这么强大，为什么还要 Chisel 去搭几何体？
- **可控性（Controllability）：** 3DGS 本质上是一堆“云朵”，很难直接像推拉积木一样精确修改。通过 Chisel 修改几何体，AI 就能实时重新分布高斯球，实现“改动骨架，皮肤随之重塑”的效果。
- **语义对齐：** 简单几何体通常带有语义标签（比如“墙”、“门”）。当你在 Chisel 里移动一堵“墙”时，系统知道要把与之关联的所有高斯点一起平移，而不是漫无目的地移动点云。

### 4. 总结：一种“引导式生成”
**简单来说，Chisel 的几何体是 3DGS 的“脚手架”。** 在生成过程中，Marble 利用扩散模型（Diffusion Models）将文本提示词与 Chisel 的几何拓扑结构相结合。几何体提供了**空间一致性**（确保桌子不会飘在空中），而 3DGS 提供了**视觉真实感**。
这种“几何引导的高斯生成”正是李飞飞所强调的**“空间智能”**的具体体现——AI 不再是随机生成像素，而是在理解物理空间的基础上，往特定的“模具”里填注细节。
### Point World
[PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://www.youtube.com/watch?v=XPOsCwrYdk0&t=120s)
《PointWorld》论文的核心实现是将**状态（State）**与**动作（Action）**统一在三维物理空间中，通过 3D 点流（3D Point Flows）来建模环境动力学。
以下是该论文具体实现的简要概述：

#### 1. 输入（Inputs）
模型在推理时主要接收以下两类输入：
- **静态场景点云 (Scene Points)**：通过单张或多张标定的 RGB-D 图像构建而成。这些点通过**投影**到 2D 视图中，并使用冻结的 **DINOv3** 编码器提取丰富的语义特征。
- **机器人动作点流 (Robot Flows)**：动作不再被表示为特定于机器人的关节坐标，而是表示为机器人几何体（通常是夹持器）在空间中的 3D 轨迹。这些轨迹是通过机器人的 **URDF 文件**和给定的**关节动作序列**，利用正运动学（Forward Kinematics）预先计算出来的。

#### 2. 预测内容（What it Predicts）
- **全场景 3D 点流 (Full-Scene 3D Point Flows)**：模型预测场景中每个点在未来一段时间（即预测时界 $$H$$）内的 **3D 位移向量**。
- 这种预测不仅包含受机器人直接接触影响的点，还隐式地包含了物体的物理属性（如刚性、形变、关节连接）以及受重力影响的动力学演化。

#### 3. 具体实现架构
- 特征化 (Featurization)：
  - **场景点**：使用 DINOv3 提取 2D 特征。
  - **机器人点**：使用**时间嵌入 (Temporal Embedding)** 来表征动作的先后顺序。
- **骨干网络 (Backbone)**：将场景点和机器人动作点拼接在一起，输入到高性能的 3D 点云骨干网络 **PointTransformerV3 (PTv3)** 中。
- **分块预测 (Chunked Prediction)**：不同于自回归模型，PointWorld 在一次前向传播中同时预测未来多个时间步（如 10 步，总计 1 秒）的状态，这极大地提高了实时推理速度（单次推理约 0.1 秒）。
- **下游应用**：该模型可直接集成到**模型预测控制 (MPC)** 框架中，通过在“想象”的点流中评估不同的候选动作序列，为真实机器人寻找最优操纵方案。
总结来说，PointWorld **通过“当前的 3D 场景点”和“预想的机器人 3D 动作流”，来预测“整个 3D 场景将如何随时间位移”**。

#### Scene points 特征提取

>  Scene points are featurized with frozen DINOv3 [153, 154] by projecting them to 2D views, while robot points are featurized with temporal embedding.
>  在《PointWorld》论文中，这句话描述了模型如何为不同类型的点（场景点和机器人点）提取初始特征。虽然 **DINOv3** 本身确实是一个用于二维图像特征提取的模型，但论文通过**几何投影（Projection）**的方法巧妙地将其应用于三维点云。
>  以下是具体的理解与实现流程：



##### 1. DINOv3 如何用于 Point Cloud？
模型并不是直接把点云输入 DINOv3，而是利用**相机标定参数**建立 2D 图像与 3D 点云之间的桥梁：
- **投影（Projection）**：利用相机的内参（Intrinsics）和外参（Extrinsics），将第一帧中的每个 3D 场景点坐标投影到对应的 2D 图像平面上，得到该点在图像中的像素坐标 ($$u, v$$)。
- **特征采样（Sampling）**：将 RGB 图像输入预训练并冻结的 **DINOv3** 编码器，生成 2D 特征图。然后，根据投影得到的像素坐标，通过**双线性插值（Bilinear Interpolation）**从 2D 特征图中提取对应的特征向量（Patch Tokens）。
- **多视图融合**：如果一个 3D 点能被多个相机观测到，模型会将来自不同视角的 DINO 特征进行平均聚合，并将这个融合后的特征附加到该 3D 点上，作为它的初始表征。

##### 2. 为什么要这么做？
- **引入语义先验**：虽然点云提供了精确的几何信息，但缺乏语义理解。论文指出，DINOv3 的密集特征能在无需显式分割的情况下提供**“物体性先验”（Objectness Priors）**，帮助模型识别哪些点属于同一个物体，或者物体的物理性质。
- **解决三维预训练稀缺问题**：高质量的 3D 预训练模型相对较少且效果有限，而 2D 视觉模型已经在海量数据上学习到了极其强大的特征表达能力，通过这种投影方式可以“借用” 2D 视觉模型的语义能力。

##### 3. 场景点 vs. 机器人点的特征提取差异
论文对不同来源的点采用了不同的表征策略，以满足动态建模的需求：
- **场景点（Scene Points）**：通过上述方式关联 **DINOv3 2D 特征**。这些特征是时间常数（Time-constant）的，主要用于刻画物体的身份和属性。
- **机器人点（Robot Points）**：使用**时间嵌入（Temporal Embedding）**。机器人点的轨迹是由已知的正运动学（URDF）生成的，因此它们的特征重点在于体现随时间变化的动作信息（位置、速度、加速度等），而不是语义外观。
**总结来说**，DINOv3 在这里充当了一个**“3D 点的语义着色器”**：它通过几何映射，把 2D 图像中蕴含的物体语义信息“贴”到了 3D 点云上，从而增强了模型对物理交互和场景演化的预测能力。
除了你提到的《PointWorld》，参考资料中涉及的另外三篇论文分别是 **PhysGaussian**、**SuGaR** 和 **UniSim**。它们各自的工作内容和实现路径如下：

### PhysGaussian: 物理集成的 3D 高斯流体动力学
- **主要工作**：该论文旨在将物理规律（如牛顿动力学）无缝集成到 **3D Gaussian Splatting (3DGS)** 框架中，以实现高质量的动态效果合成。它遵循**“所见即所模拟”（What You See Is What You Simulate, WS2）**的原则，使 3D 高斯点既是渲染单元，也是物理模拟的粒子。
- 具体实现：
  - **输入**：静态场景的 3D 高斯表征和用户指定的物理参数（如密度、杨氏模量等）。
  - **预测/模拟内容**：利用自定义的**物质点法（MPM）**和连续介质力学原理，模拟 3D 高斯内核随时间的形变和应变属性。
  - **实现效果**：能够模拟弹性体、塑性金属、非牛顿流体和碎石材料的物理交互，无需传统的三角形或四面体网格即可生成 photo-realistic 的动态动画。

### SuGaR: 表面对齐的高斯喷溅实现高效网格重建
- **主要工作**：SuGaR 解决了从数百万个无序的 3D 高斯点中提取高质量、可编辑的三维**网格（Mesh）**的难题。
- 具体实现：
  - **输入**：多视图图像。
  - 实现机制：
    1. **正则化项**：在优化过程中引入约束，强制高斯点与场景表面对齐并扁平化。
    2. **网格提取**：对对齐后的高斯点云进行采样，利用**泊松重建（Poisson Reconstruction）**在几分钟内提取出三角形网格。
    3. **高斯绑定**：将新的“薄高斯”点绑定到网格表面，并进行联合优化。
  - **实现效果**：允许用户使用传统的网格编辑、动画和重新布光工具来操纵 3D 高斯场景，同时保持极高的渲染质量。

### UniSim: 通用真实世界交互模拟器
- **主要工作**：UniSim 构建了一个**“动作输入-视频输出”（Action-in-video-out）**的通用模拟器，通过生成式框架来模拟真实世界的物理交互。
- 具体实现：
  - **输入**：有限的历史观测（视频帧）和特定的机器人动作序列。
  - **预测内容**：使用**视频扩散模型（Video Diffusion Model）**预测未来的视觉观测结果，并通过自回归方式生成长时程视频。
  - 实现效果：
    - 模拟长时程交互（如连续打开多个抽屉、移动物体），并保持时间上的一致性。
    - **跨越 Sim-to-Real 鸿沟**：机器人可以纯粹在 UniSim 生成的“视频环境”中训练高级语言策略或低级控制策略，然后直接部署到真实世界中。

### 总结对比
| 论文名称         | 核心媒介                | 核心目标                 | 实现手段                         |
| ---- | ----- |  | -- |
| **PhysGaussian** | 3D 高斯点               | 物理动力学模拟           | 集成连续介质力学与物质点法 (MPM) |
| **SuGaR**        | 3D 高斯点 + 网格 (Mesh) | 快速提取高质量可编辑网格 | 表面对齐正则化 + 泊松重建        |
| **UniSim**       | 视频帧 (2D)             | 构建端到端通用模拟器     | 视频扩散模型 + 动作条件生成      |

## JEPA
[盘点｜4年JEPA进化之路，12篇核心突破！LeCun(杨立昆)如何用它重构AI表征学习？](https://mp.weixin.qq.com/s/HRMVvZcChlcwPkHdK6sUXw)
[V-JEPA & V-JEPA 2 Explained: The Self-Supervised Revolution in Video Understanding](https://www.youtube.com/watch?v=Jt-m3gho0_0&list=WL&index=2&t=677s) 这个不错，看一下
[V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video (Explained)](https://www.youtube.com/watch?v=7UkJPwz_N_0&list=WL&index=3&t=191s)

你可以把它们的关系看作是：**一个底座（V-JEPA-2-AC）** + **一次深度体检（2512.24497）** + **一个导航插件（2601.00844）**。

### 1. V-JEPA-2-AC：物理世界的“底座”
这是 Meta 发布的**基础模型**。它的核心贡献是证明了通过大规模视频预训练（使用 Masking 机制），模型可以学到一个带动作条件的（Action-Conditioned）世界模型。
- **它解决了：** “如果我在这段视频的这一帧做一个‘推’的动作，下一帧的**特征**会变成什么样？”
- **它的角色：** 提供了一个能够模拟物理演化的**模拟器（Simulator）**。

### 2. [2512.24497]：深度的“科学诊断”
*What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?*
这篇论文是对 V-JEPA 这一类模型的**深度复盘和理论分析**。

- **核心研究点：** 它在问“为什么 JEPA 在物理规划上比别的模型强？”
- **结论：** 它通过实验证明，成功的关键不在于把像素还原得多么清晰（像生成模型那样），而在于 **Latent Space（潜在空间）的动态一致性**。
- **与 V-JEPA-2-AC 的关系：** 它解释了 V-JEPA 这种架构之所以能做规划，是因为它的 Masking 预训练任务迫使模型抓住了物理守恒律（如动量、重力），而不是表面的视觉细节。

### 3. [2601.00844]：动作规划的“算法升级”
*Value-guided action planning with JEPA world models*
这篇论文是在前两者的基础上，提出了**如何更好、更快地使用这个世界模型**。

- **核心改进：** 之前的模型在规划时（比如想让机器人抓球），可能需要在潜在空间里随机尝试成千上万种动作序列。这篇论文引入了 **Value Function（价值函数）**。
- **技术突破：** 它通过在 JEPA 的特征上叠加一个价值评估器，让模型在“脑补”未来时，能直接朝着“成功率更高”的方向去预测，而不是盲目乱猜。
- **与前二者的关系：** 它利用了 V-JEPA-2-AC 的特征和 [2512.24497] 验证的物理一致性，最终实现了一个高效的闭环控制算法。

### 三者关系的直观对比表
| **维度**       | **V-JEPA-2-AC**             | **[2512.24497] (Success Factors)** | **[2601.00844] (Value-guided)** |
| -- | --- | ---- | - |
| **身份**       | 预训练好的**模型本体**      | 对模型的**诊断报告/说明书**        | 对规划算法的**功能升级包**      |
| **解决的问题** | 赋予 AI 物理直觉            | 搞清楚物理直觉哪来的               | 让物理直觉变现为高效行动        |
| **关键词**     | Action-Conditioned, Masking | Latent Dynamics, Robustness        | IQL, Value-guided Planning      |

### 总结
这三篇工作的逻辑非常清晰：
1. **V-JEPA-2-AC** 说：“我通过看视频和动作，学会了物理世界的变化规律。”
2. **2512.24497** 说：“我研究了上面那个家伙，发现它强在‘特征空间预测’，而不是‘画图’。”
3. **2601.00844** 说：“既然它物理直觉这么好，我给它加个‘价值目标’，让它在脑子里规划动作时不再跑偏。”

### LeJEPA
**V-JEPA-2 侧重于“规模与能力（Scale & Action）”，而 LeJEPA 侧重于“理论与效率（Theory & Heuristic-free）”。**
以下是它们关系的深度解析：

#### 1. 什么是 LeJEPA (Latent-Euclidean JEPA)?
**LeJEPA**（发表于 2025 年 11 月）是 JEPA 架构在**训练算法**上的重大突破。
- **核心贡献：** 它引入了 **SIGReg (Sketched Isotropic Gaussian Regularization)** 目标函数。
- **解决的痛点：** 传统的 JEPA（包括第一代 V-JEPA）依赖很多“训练技巧（Heuristics）”来防止特征崩溃，比如 **Teacher-Student 网络、EMA（指数移动平均）更新、Stop-gradient** 等。
- **突破点：** LeJEPA 证明了只要让特征分布符合“各向同性高斯分布”，就可以扔掉那些复杂的技巧，直接进行稳定的大规模训练。

#### 2. V-JEPA-2 与 LeJEPA 的区别与联系
你提到的 V-JEPA-2（通常指 Assran 等人在 2025 年推出的版本）和 LeJEPA 的关系如下表：
| **特性**     | **V-JEPA-2 / V-JEPA-2-AC**                                   | **LeJEPA (arXiv:2511.08544)**                                |
|  |  |  |
| **主攻方向** | **时空规模与动作感知**。研究如何处理超长视频、加入 Action 条件做规划。 | **训练目标函数优化**。研究如何去掉启发式插件，实现数学上的最优训练。 |
| **训练配方** | 通常沿用“Teacher-Student”架构（类似 I-JEPA 的升级版）。      | **无教师架构**。使用 SIGReg 直接约束 Latent Space。          |
| **模型规模** | 侧重于 ViT-L, ViT-H 等大规模视频底座。                       | 证明了其算法在 1.8B (ViT-g) 规模下依然不需要技巧也能稳定。   |
| **相互关系** | **目前的“能力底座”**。                                       | **未来的“训练引擎”**。                                       |

#### 3. V-JEPA-2 用到 LeJEPA 了吗？
从目前的论文发布顺序看：
- **技术上：** V-JEPA-2-AC 在发布时（2025 年中）主要还是基于改进的掩码机制和动作条件分支。
- **趋势上：** 随着 LeJEPA 在 2025 年底发布，Meta 的后续工作（包括正在研发的更高版本）正在将 **LeJEPA 的 SIGReg 损失函数** 引入到 V-JEPA 的视频训练中。
**结论是：** V-JEPA-2 是 JEPA 架构在**视频理解领域**的巅峰之作；而 LeJEPA 是对所有 JEPA 模型的一次**底层算法重构**。你可以理解为 V-JEPA-2 证明了 JEPA 能看懂世界，而 LeJEPA 找到了让它学得更稳、更高效的数学方法。

#### 为什么 LeJEPA 对你关注的“视频增强”或“规划”很重要？
如果你关注的是 **arXiv:2512.24497**（物理规划）那篇论文，你会发现一个趋势：**特征空间的质量直接决定了规划的成功率。**
- **LeJEPA** 强制让特征变成圆球状的高斯分布（Isotropic Gaussian），这使得在特征空间计算“距离”和“代价（Value Function）”变得非常简单且准确。
- 相比之下，传统的 V-JEPA 特征空间可能不够“圆”，在做复杂物理规划时容易产生偏差。

## 机会

### 1. 真正讨论 JEPA 引导生成的关键论文
如果你对“用 JEPA 引导 Diffusion”感兴趣，这篇是目前最权威的：
- **论文标题：** 《**VideoREPA: Video Representation Learning for Video Generation**》
- **arXiv 编号：** [arXiv:2412.15214](https://arxiv.org/abs/2412.15214)
- **核心内容：** 这篇论文直接回应了你的疑问。它系统地研究了不同的视频表征（包括类似 JEPA 的预测性表征）如何影响视频生成模型。结论是：**具备物理演化逻辑的特征比纯语义特征更能提升视频生成的质量和稳定性。**

### 2. 真正讨论 JEPA 物理一致性的论文
关于 JEPA 为什么能提供物理一致性（解决抖动和形变）：
- **论文标题：** 《**Revisiting Feature Prediction for Learning Visual Representations from Video**》 (V-JEPA 官方论文)
- **arXiv 编号：** [arXiv:2404.08471](https://arxiv.org/abs/2404.08471)
- **核心内容：** 虽然这是底座论文，但它详述了 **Masking 策略**如何迫使模型理解物体的“跨帧关联”。这是所有后续“视频增强”任务利用其作为先验的理论基础。

### 3. 关于“世界模型”与潜在空间预测的深入讨论
关于“Scaling Laws”和潜在空间预测对生成任务的增益：
- **论文标题：** 《**Learning and Leveraging World Models in Visual Representation Learning**》
- **arXiv 编号：** [arXiv:2403.00504](https://arxiv.org/abs/2403.00504)
- **核心内容：** 讨论了像 JEPA 这样的潜在空间预测架构如何规模化（Scaling），以及这种预测能力如何转化为对视频内容的深刻理解。

### 视频增强
在目前的学术前沿，V-JEPA-2 及其 AC（Action-Conditioned）变体主要被用于**具身智能**和**物理规划**（正如你提到的 2512.24497 和 2601.00844）。
而在**视频增强/生成**领域，研究者通常不直接搜索“V-JEPA 视频增强”，而是搜索：
1. **"SSL (Self-Supervised Learning) Priors for Video Diffusion"**
2. **"Latent Dynamics for Consistent Video Synthesis"**
这是一个非常敏锐的技术直觉。在 2025-2026 年的学术语境下，**JEPA (理解世界规律)** 与 **Gaussian Splatting (重建世界表观)** 的结合，被认为是通往“具身智能”和“高度真实模拟器”的必经之路。
如果说 **Gaussian Splatting (3DGS/4DGS)** 提供了物理世界的**“皮囊”**（极其逼真的视觉重建），那么 \**JEPA\** 就提供了**“灵魂”**（底层的物理逻辑和语义演化）。
以下是这两者结合的三个核心机会点和最新的研究方向分析：

#### 1. 物理增强的 4D 变形场 (Physics-Aware 4DGS)
目前的 4D Gaussian Splatting（动态场景重建）主要依赖于**像素对齐**。当视频中出现快速运动或剧烈遮挡时，Gaussian 点云的运动轨迹往往会“崩坏”，产生不符合物理的形变。
- **机会点：** 利用 **V-JEPA-2-AC** 的动作预测能力。
- **实现逻辑：** 将 V-JEPA 学习到的特征轨迹作为 4DGS 变形场（Deformation Field）的 **Regularizer（正则项）**。
- **价值：** 即使摄像头看不清某个物体的背面，V-JEPA 提取的物理特征也能“指导” Gaussian 点如何按照重力、惯性进行位移，从而解决动态重建中的“伪影”问题。

#### 2. 语义高斯分布 (Semantic Splatting with JEPA Features)
3DGS 最大的缺陷是它“只懂颜色，不懂物体”。每个 Gaussian 点只是一团带有颜色的椭圆。
- **机会点：** 将 JEPA 的 **Latent Embeddings（潜在嵌入）** 蒸馏（Distill）到每个 Gaussian 点中。
- **研究现状：** 类似于 *Language Embedded Gaussians*，但使用 JEPA 特征。
- **应用：**
  - **交互式编辑：** 因为 Gaussian 点带有了 JEPA 的语义特征，你可以直接下达指令“把那个杯子移走”，模型知道哪些点属于“杯子”。
  - **机器人操作：** 机器人可以通过 3DGS 看到高质量的画面，同时通过附着的 JEPA 特征知道哪个部位是可以抓取的，哪个部位是脆弱的。

#### 3. 基于 JEPA 的“脑内”交互模拟器 (Action-Conditioned Simulator)
这是目前最符合 **V-JEPA-2-AC** 与 **LeJEPA** 发展趋势的方向。
- **痛点：** 传统的机器人训练依赖于庞大的物理引擎（如 Isaac Sim），但这些引擎的视觉真实感往往不够。
- **解决方案：**
  1. **静态/动态重建：** 用 Gaussian Splatting 快速扫描现实场景，建立高保真环境。
  2. **动态预测：** 接入 **V-JEPA-2-AC**。当你输入一个“推”的动作，JEPA 在特征空间预测物体的后续状态。
  3. **渲染反馈：** 预测出的特征驱动 3DGS 进行实时渲染更新。
- **机会：** 建立一个**“所见即所得”的自监督模拟器**。机器人直接在 3DGS 重建的现实场景中“试错”，而不需要人工建模。

### 2025-2026 最新论文趋势分析
结合你之前提到的几篇论文（2512.24497 和 2601.00844），我们可以看到以下融合趋势：
| **融合维度** | **关键技术**                  | **代表性机会**                                       |
|  | ----- | ---- |
| **空间维度** | 3D-JEPA + 3DGS                | 实现**自监督**的 3D 场景解析，无需人工标注标签。     |
| **时间维度** | V-JEPA-2 + 4DGS               | 解决长序列动态视频重建中的**时空不一致**问题。       |
| **动作维度** | V-JEPA-AC + Dynamic Splatting | 创造出能根据不同动作指令**实时形变**的数字孪生场景。 |

### 总结：你的研究机会在哪里？
如果你正在寻找论文题目或项目方向，我建议关注：**“如何利用 LeJEPA (2511.08544) 的各向同性特征空间来加速 Gaussian Splatting 的收敛速度”。**
- **核心假设：** 如果特征空间更平滑、更符合高斯分布（LeJEPA 的贡献），那么在进行特征蒸馏时，Gaussian 点能更快地学习到物体的边界和物理属性，从而减少重建所需的视角数量（Few-shot Reconstruction）。
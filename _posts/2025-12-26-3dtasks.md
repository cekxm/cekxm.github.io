---
layout: post
title: "3D 任务：SFM, MVS, NVS, VO, VIO, SLAM"
date: 2025-12-26 +0800
categories: []
description: 多视角几何，新视角生成
tags:
thumbnail:
toc:
  sidebar: left # or right
typora-root-url: ../
---

## 多视角几何（MVS），新视角生成（NVS）对比

![SfM_NeRF](/images/2025-12-26-3dtasks/d3dcd8d3-35cb-403b-998d-64256b21ba06.png){: .img-fluid}



### (MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表

| **维度**         | **(MVS) SfM (如 COLMAP)**                                    | **VGGT (Visual Geometry Grounded Transformer)**              | **NVS (NeRF / 3D GS)**                                       |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **核心任务目标** | **三维重建**：求解精确的相机姿态和场景几何（点云/深度）。    | **统一几何推断**：一站式、秒级预测相机、点图、深度图和追踪。 | **新视角合成**：在未拍摄过的角度生成照片级逼真的图像。       |
| **底层表示**     | **离散几何**：稀疏或稠密的 3D 点云、深度图。                 | **稠密几何图**：Point Maps ($$H \times W \times 3$$) 和 Depth Maps。 | **光场表示**：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。 |
| **运行机制**     | **优化驱动**：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。 | **前馈推理 (Feed-forward)**：一次性通过 Transformer 模型直接“看”出几何。 | **视图对齐优化**：通过渲染结果与原图的颜色误差来反向训练。   |
| **对相机的需求** | **未知或已知**：通常用于解算未知相机参数。                   | **无需预设**：直接预测相机的 9 维参数（内外参）。            | **必须已知**：通常依赖 SfM (COLMAP) 提供位姿初始化。         |
| **几何精确度**   | **高（度量级）**：数学推导严谨，但易受弱纹理、模糊影响。     | **高且鲁棒**：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。 | **中/低**：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。 |
| **渲染视觉效果** | **差**：只有离散点，无法生成连续、真实的图像。               | **中**：提供稠密几何，但主要用于几何任务而非美学渲染。       | **极高**：支持照片级渲染、阴影、反射和透明效果。             |
| **处理速度**     | **极慢**：通常需要数分钟到数小时。                           | **极快**：全流程通常在 **1 秒以内**。                        | **训练慢/渲染快**：NeRF 训练慢，GS 渲染极快，但都需要初始化。 |

简而言之：**(MVS)SfM** 是传统的“几何尺子”；**VGGT** 是现代的“几何大模型”；而 **NVS** 是“虚拟照相机”。

根据您上传的论文内容及相关技术背景，**ESVO2 是一个 VIO（视觉惯性里程计）系统**，它是基于纯视觉算法 ESVO 的增强版，引入了惯性测量单元（IMU）来提高鲁棒性。

以下是关于 ESVO2 的定位及其与 SLAM 区别的详细介绍：

## VIO 与 SLAM 的区别

虽然两者都旨在解决“我在哪”的问题，但它们在功能覆盖和目标重点上有所不同：

| **特性**     | **视觉惯性里程计 (VIO)**                  | **同步定位与建图 (SLAM)**                    |
| ------------ | ----------------------------------------- | -------------------------------------------- |
| **主要组成** | 视觉处理 + IMU 预积分 + 后端优化          | VIO (前端) + **回环检测** + 全局优化         |
| **误差累计** | 会随着位移增加而产生**累计漂移（Drift）** | 通过回环检测修正累计误差，具有**全局一致性** |
| **地图规模** | 通常只维护一个局部地图（滑动窗口内）      | 维护全局地图，允许机器人回到已知点时重定位   |
| **计算消耗** | 相对较低，适合实时性要求极高的场景        | 较高，需要存储和检索大量历史关键帧数据       |
| **典型代表** | VINS-Mono (VIO模式), ESVO2, OKVIS         | ORB-SLAM3, VINS-Fusion (含回环)              |

## SLAM

在 SLAM（同步定位与建图）框架中，**回环检测（Loop Closure Detection）**和**全局优化（Global Optimization）**是消除累计误差、保证地图全局一致性的核心机制。

如果没有这两个部分，系统仅仅是一个**里程计（Odometry）**，位姿误差会随着时间的推移不断增加，轨迹最终会“漂散”。

------

### 回环检测 (Loop Closure Detection)

回环检测的任务是：**识别机器人是否回到了曾经到过的地方。**

- 为什么要检测？

  里程计每一步都会引入微小的误差，经过长距离运行后，系统估计的当前位置与真实位置可能相差巨大。如果能识别出“旧地重游”，就能通过这个历史“锚点”来纠正累积误差。

- **如何实现？（主流方法：词袋模型 BoW）**

  1. **特征提取**：提取当前图像的特征点（如 ORB）。
  2. **词袋描述**：将特征点转化为一个数值向量（类似一篇文章的关键词提取）。
  3. **相似度检索**：在历史图像库中寻找与当前图像最相似的帧。
  4. **几何验证**：通过对极几何等方法，确认两张图不仅长得像，而且在空间逻辑上也是匹配的，从而防止“感知歧义”（例如两间长得一模一样的办公室）。

------

### 全局优化 (Global Optimization)

一旦回环检测发现当前帧 $i$ 与历史帧 $j$ 匹配成功，系统就获得了一个闭环约束。接下来需要通过全局优化来调整整个轨迹。

#### **位姿图优化 (Pose Graph Optimization)**

在回环发生后，为了保持实时性，通常不再优化复杂的 3D 空间点，而仅仅优化**相机的位姿节点**。

- **节点（Nodes）**：机器人每一时刻的位姿。
- **边（Edges）**：
  - **里程计边**：相邻帧之间的约束（由 VIO 提供）。
  - **回环边**：当前帧与历史帧之间的约束（由回环检测提供）。
- **优化目标**：调整所有节点，使得所有“边”的残差总和最小。这就像是一个弹簧网，回环边强行把漂移的轨迹拉回到历史位置，而其他轨迹点则像弹簧一样跟随调整。

------

###  区别总结：VIO vs SLAM

| **模块**   | **VIO (如 ESVO2, VINS 前端)** | **SLAM (如 VINS 后端, ORB-SLAM3)** |
| ---------- | ----------------------------- | ---------------------------------- |
| **漂移**   | 随距离/时间线性增长           | 遇到回环时清零                     |
| **一致性** | 局部一致                      | 全局一致                           |
| **地图**   | 瞬时、局部                    | 持久、可重用                       |

**COLMAP** 通常不被认为是一个 **VO（视觉里程计）** 方法，它是一个标准的 **SfM（运动恢复结构）** 框架。

虽然两者底层都使用了相似的技术（如特征提取、对极几何、Bundle Adjustment），但它们在设计目标、运行方式和应用场景上有显著区别。

------

## SfM 与 VO 的本质区别

| **特性**     | **COLMAP (SfM)**                                             | **Visual Odometry (VO)**                                   |
| ------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **处理时效** | **离线 (Offline)**：处理一组预先采集好的图片或视频序列。     | **实时 (Real-time)**：随着相机的移动即时计算位姿。         |
| **图像顺序** | **无序/有序**：可以处理乱序的照片（如不同人的街拍），通过特征匹配寻找联系。 | **必须有序**：依赖视频流的连续性进行帧间追踪。             |
| **优化范围** | **全局 (Global)**：通常对所有相机位姿和所有点进行全局优化（Global BA）。 | **局部 (Local)**：通常只在滑动窗口内进行优化以维持实时性。 |
| **应用目标** | 追求**极致精度**和高质量的 3D 重建（如制作模型、地图）。     | 追求**低延迟**和机器人的实时定位（如无人机飞行）。         |
| **计算消耗** | 极高：可能需要数小时或数天处理大型场景。                     | 较低：需要在嵌入式设备或普通 CPU/GPU 上实时运行。          |

------

### 它们之间的联系

虽然它们定位不同，但有很深的血缘关系：

- **技术基础一致**：它们都遵循“特征提取 $\rightarrow$ 特征匹配 $\rightarrow$ 位姿估计 $\rightarrow$ 三角化建图 $\rightarrow$ 后端优化”的流程。
- **增量式 SfM**：COLMAP 属于“增量式（Incremental）”SfM，它的工作方式是一张一张地把新照片注册到现有地图中。这种“一张张增加”的逻辑在形式上非常接近 VO，只是 COLMAP 每增加一张都会做大量的全局检查以保证精度，而 VO 只看局部。
- **VO 是 SfM 的子集**：你可以把 VO 看作是一种追求实时性、牺牲全局一致性、且只能处理有序序列的特殊 SfM。

------

### 为什么不把 COLMAP 当作 VO 使用？

如果你尝试用 COLMAP 跑实时定位，会遇到以下问题：

1. **速度太慢**：COLMAP 的每一步（尤其是匹配和全局优化）都非常耗时，无法做到每秒处理 30 帧。
2. **内存消耗**：COLMAP 试图建立和维护完整的场景模型，随着照片增加，内存开销会爆炸。
3. **缺乏状态估计**：VO 通常会融合 **IMU（惯性测量单元）** 来处理快速运动（如 VINS-Fusion），而 COLMAP 主要是纯视觉处理。

### 总结

**COLMAP 是用来“建图”的利器，而 VO 是用来“带路”的工具。**

- 如果你有 1000 张从各个角度拍的景区照片，想做一个 3D 模型，用 **COLMAP**。
- 如果你有一个机器人正在屋里跑，需要知道它现在在哪，用 **VINS-Mono** 或 **ORB-SLAM3**。

## 研究方向

在 2025 年的时间节点上，**Novel View Synthesis (NVS)** 在学术热度和资本市场显然更“火”，但 **Multi-View Stereo (MVS)** 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。

这两者并非孤立竞争，而是呈现出一种**深度融合**的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：

------

### 1. 热门程度：Novel View Synthesis (NVS) 占据 C 位

**核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D**

- **学术热度：** 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 **3DGS (3D 高斯溅射)** 和 **生成式新视角合成** 的论文数量呈爆炸式增长。
- **AIGC 助力：** 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 **Generative NVS**）成了最热门的方向。
- **用户感知度：** NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。

### 2. 发展前景：MVS 正在向“几何大模型”进化

**核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry**

- **从“工具”到“大脑”：** 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 **VGGT** 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个**前馈网络（Feed-forward）**。
- **解决“不可能任务”：** 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。
- **工业刚需：** 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的**精确绝对坐标（Metric Geometry）**。

------

### 3. 2025 年的关键趋势：两者边界的模糊（融合）

如果你在考虑职业发展或研究方向，**“几何感知的 NVS” (Geometry-aware NVS)** 是 2025 年最具前景的方向。

1. **MVS 为前，NVS 为后：** 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。
2. **可推广性 (Generalizability)：** 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 **LRM (Large Reconstruction Models)**，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。
3. **动态场景：** 静态场景的重建基本解决，2025 年的蓝海是**动态 4D 重建**（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。

------

### 总结建议

- **如果你追求“视觉震撼”和“快速产出”：** 选择 **Novel View Synthesis (尤其是 3DGS)**。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。
- **如果你追求“底层技术”和“稳健性”：** 选择 **MVS/几何大模型**。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。
- **最具潜力的路径：** 研究**如何将 MVS 的几何约束引入 NVS**（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。

**结论：** **NVS 更“热门”（Hotter）**，但 **MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）**。

---
layout: post
title: "Flow"
date: 2025-12-25 21:44:12 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left # or right
typora-root-url: ../
---

# Flow matching

## 参考资料

### 论文和博客

[1] [Flow Matching For Generative Models From Scratch \| by Nikolaus Correll \| Toward Humanoids \| Medium](https://medium.com/@nikolaus.correll/flow-matching-for-generative-models-from-scratch-8264bad4e0ba)

[2] [A Visual Dive into Conditional Flow Matching \| ICLR Blogposts 2025](https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/)

### toy example

[GitHub - Whalefishin/Latent_Flow_Matching_MNIST: A minimal example for training a flow matching model in a pretrained VAE's latent space to generate MNIST digits.](https://github.com/Whalefishin/Latent_Flow_Matching_MNIST)

[GitHub - lebellig/discrete-fm: Educational implementation of the Discrete Flow Matching paper](https://github.com/lebellig/discrete-fm)

## 理论

### Continuous Normalizing Flows

以下主要参考 [2]

![image-20250911165058417](/images/2025-12-25-flow/image-20250911165058417.png){: .img-fluid}

首先要理解这个图。

- 概率路径 $$p_t$$ 指 $$t$$ 时刻 $$x$$ 的概率分布 $$p_t(x)$$，一般令 $$p_0(t)$$ 是一个标准的高斯分布，$$p_1(t)$$ 是我们要建模的、未知的分布。概率路径就是从0时刻到1时刻，概率分布的变化过程。

- 速度场 $$u(x,t)$$ 比较好理解

- flow 就是流，类似水流，水流中 $$x$$ 位置的一个水滴（假设它有独特的标记），经过 $$t$$ 时候，会跟随水流移动到 $$f(x,t)$$ 位置。 $$f(x,t)$$ 可以看成一个映射，$$f^u: \mathbb{R}^d \times[0, 1] \to \mathbb{R}^d$$

- 和 flow 有关系的另一个变量是 $$x(t)$$ ,也是表示原处于 $$x_0$$ 位置的点随时间t的变化，见下面这个常微分方程（ODE），它很好理解，速度是位置的导数。

$$
\begin{cases}
  x(0) = x_0 \\
  \partial_t x(t) = u(x(t), t) \quad \forall t \in [0, 1]
  \end{cases}
$$

在文献[2]中，$$\partial_t$$ 就是指 $$\frac{\partial}{\partial t}$$。上式也叫 initial value problem（初值问题）。flow $$f^u(x,t)$$ defined as the solution at time $$t$$ to the initial value problem driven by $$u$$ with initial condition $$x(0)=x$$.

上面这个图，三个要素形成三角关系。

- flow 和速度场之间是 ODE 的关系，知道速度场，可以求解 flow。$$f^u(x,t)$$ 是它的解，$$x$$ 实际上是初值。
- 概率路径和速度场之间满足连续性方程

$$
\begin{equation}\label{eq:continuity_eq}
  \partial_t p_t + \nabla\cdot u_t p_t = 0
\end{equation}
$$

其中 $$\nabla\cdot$$ 表示散度。这个方程保证了概率质量的守恒：概率密度在空间中的变化（通过时间导数 $$\frac{\partial p_t(x)}{\partial t}$$）由概率流量的散度决定。

- 概率路径和flow之间满足 *change-of-variable formula*，也称为 pushforward $$p_t = f^u(\cdot, t)\# p_0$$。就是说通过这个流，把 $$p_0$$ 经时间t，变为 $$p_t$$。

以下是AI关于change-of-variable formula的介绍。

**Change-of-variable formula** 描述了概率密度如何随这种映射变化。对于一个可逆的映射 $$ \phi_t: x_0 \mapsto x_t $$ （可逆映射在这边就是flow，它是一个映射，并且是可逆的），概率密度 $$ p_t(x_t) $$ 与初始密度 $$ p_0(x_0) $$ 的关系为：
$$
p_t(x_t) = p_0(x_0) \cdot \left| \det\left( \frac{\partial \phi_t^{-1}}{\partial x_t} \right) \right| = p_0(\phi_t^{-1}(x_t)) \cdot \left| \det\left( \frac{\partial x_0}{\partial x_t} \right) \right|
$$
其中：

- $$ \phi_t^{-1} $$ 是逆映射，从 $$ x_t $$ 映射回 $$ x_0 $$。
- $$ \det\left( \frac{\partial x_0}{\partial x_t} \right) $$ 是逆映射的雅可比行列式的绝对值。

在流匹配中，雅可比行列式反映了流映射如何缩放空间，从而影响概率密度。

> 我之前有疑问，从 $$x_0$$ 到 $$x_t$$ 可能经过了很长的距离变化，怎么这两个位置之间的密度能建立联系？答案可能是因为流是连续的，$$x_0$$ 附近的质量都会流到 $$x_t$$ 附近，这个变化由雅可比行列式来决定。

### Conditional Flow Matching

可能，flow matching 就是指  conditional flow matching.

CFM 核心思想是选择一个条件变量 $$z$$，以及一个条件概率路径 $$p(x\mid t,z)$$，满足两点

1. 由 $$p(x\mid t,z)$$​ 推导出的全局概率路径  $$p(x\mid t)$$​ 可以把 $$p_0$$​ 转化为 $$p_{data}$$​。即要求 $$p(x\mid t,z)$$ 在 t=0,t=1 的边际概率 $$p(x\mid t=0)$$，$$p(x\mid t=1)$$符合
   
$$
   \begin{align*}
   \forall x \space E_z [ p(x \vert z, t=0) ] = p_0(x) , \\
   \forall x \space E_z [ p(x \vert z, t=1) ] = p_{data}(x).
   \end{align*}
   
$$

2. $$p(x\mid t,z)$$  对应的条件速度场 $$u^{cond}(x,t,z)$$ （回忆一下，二者的关系是连续性方程）具有一个解析的形式，这是因为要使用一个神经网络来回归条件速度场。

文中用下图进行概括：

![image-20250911233204539](/images/2025-12-25-flow/image-20250911233204539.png){: .img-fluid}

- 首先，要做出 choice 1

- 然后做出 choice 2（必须满足要求1）

- $$p(x\mid t,z)$$  能够确定  $$u^{cond}(x,t,z)$$ ，它们之间满足连续性方程

- $$p(x\mid t,z)$$ 边际化 $$z$$ 可以获得 $$p(x\mid t)$$

- $$p(x\mid t)$$ 和速度场  $$u(x,t)$$ 之间满足连续性方程

-  $$u(x,t)$$可由  $$u^{cond}(x,t,z)$$​ 显式的表达，这个关系就是文中的Theorem 1:

$$
\begin{align}
    \forall t, x, \, \, u(x,t) &= E_{z\mid x, t} {u^{cond} }(x,t,z)
  \end{align}
$$

- 实际计算中，并不是通过这个表达式去获得 $$u(x,t)$$。而是使用一个神经网络来回归条件速度场。那回归条件速度场有什么用呢？这个解释是文中的 Theorem 2. 即使用下面这个Loss来回归条件速度场，

$$
\begin{aligned}
\mathcal{L}^{\mathrm{CFM}}(\theta) & \overset{\mathrm{def}}{=}
E_{
  \substack{t \sim \mathcal{U}([0, 1]) \\
            z \sim p_z \\
            x \sim p( \cdot \mid  t, z) } }{\lVert u_\theta^{CFM}(x,t) -
\underbrace{u^{cond}(x,t,z)}_{\substack{
  \text{chosen to be} \\
  \text{explictly defined}, \\
  \text{cheap to compute}, \\
  \text{e.g., } x_1 - x_0}} \rVert^2} \enspace,
\end{aligned}
$$

等价于回归不可知的速度场

$$
\begin{align*}
  \mathcal{L}^{\mathrm{CFM}}(\theta)
   & \underset{(\text{proof below})}{=}
   E_{\substack{ t \sim \mathcal{U}([0, 1]) \\ x \sim p_t} } \Vert{u_\theta^{CFM}(x,t) - \underbrace{u(x,t)}_{\substack{\text{implicitly defined,} \\ \text{hard/expensive} \\ \text{to compute}}}}\Vert^2
  + \underbrace{C}_{\text{indep. of } \theta}
  \end{align*}
$$

Theorem 2 的证明使用了Theorem 1.

因此，我们用神经网络去逼近条件速度 $$u^{cond}(x,t,z)$$，最终学习得到的是经过点 $$(x,t)$$ 的所有轨迹的平均速度，这个平均速度也就是 $$u(x,t)$$。

# Mean flow

## mean flow 论文中的 flow matching 定义

flow 的 0 时刻为 $$p_{data}(x)$$，1 时刻为 $$p_{prior}(\epsilon)$$。flow 是把数据映射为先验（一版是高斯噪声），和之前定义的映射反过来了。给定$$x\sim p_{prior}(\epsilon)$$， $$x\sim p_{data}(x)$$，定义 flow path: $$z_t=a_t x+b_t \epsilon$$，其中 $$a_t$$ , $$b_t$$ 是 predifined schedules.

> 可能，只要满足 $$z_0=\epsilon$$, $$z_1=x$$ 就行。

比较常用的是，$$a_t=1-t$$, $$b_t=t$$。由于速度 $$v_t  = z'_t=a'_t x+b'_t \epsilon$$，因此 $$v_t  = \epsilon - x$$。

给定速度场 $$v(z_t,t)$$，通过求解 ODE 来进行数据采样 $$z_t$$:
$$
\frac{d}{dt} z_t=v(z_t,t)
$$
starting from $$z_1=\epsilon$$。注意，这个初值问题的初值是 $$z_1$$。所以是从 $$t=1$$ 倒推的。解可以写成
$$
z_r=z_t - \int_r^t v(z_r,r)dr
$$
 实现时，是用数值解，比如欧拉法
$$
z_{t_{i+1}}=z_{t_{i}}+(t_{i+1}-t_i)v(z_{t_{i}},t_i)
$$
注意，这两个式子没有矛盾。倒推时，$$t_{i+1}-t_i<0$$，速度取的时间是 $$t_i$$。

## mean flows

定义平均速度 $$u$$：
$$
u(z_t,r,t) \triangleq \frac{1}{t-r}\int_r^tv(z_r,r)dr
$$
用一个神经网络 $$u_\theta(z_t,r,t)$$ 来预测平均速度 $$u$$。在训练时，就需要它的真值。经过推导可得
$$
u(z_t,r,t) = v(z_t,t)-(t-r)\frac{d}{dt}u(z_t,r,t)
$$

$$
\frac{d}{dt}u(z_t,r,t) =v(z_t,t)\partial_z u +\partial_t u
$$

可以看出，要知道真值，需要真值对时间的微分，这没法获得。所以实际使用的真值是
$$
u_{tgt} = v(z_t,t)-(t-r)(v(z_t,t)\partial_z u_\theta +\partial_t u_\theta)
$$
即在计算微分时，用参数化的 $$\partial u_\theta$$ 来代替 $$\partial u$$。并且回顾一下神经网络的训练，真值用于计算 loss，它本身一般是一个固定值。而在这边真值和  $$u_\theta$$  有关系，所以需要额外设定它不参与微分计算，否则就会 “double backpropagation”。
$$
\mathcal{L}(\theta)=E\parallel u_\theta(z_t,r,t) - sg(u_{tgt})\parallel_2^2
$$
![image-20251012141549566](/images/2025-12-25-flow/image-20251012141549566.png){: .img-fluid}

## Mean Flows with Guidance

classifier-free guidance

推导看得不是很懂，但是训练过程，如下

![image-20251013095354856](/images/2025-12-25-flow/image-20251013095354856.png){: .img-fluid}

有区别的地方在于式（19），$$v_t$$ 是 $$\epsilon -x$$，$$u_\theta^{cfg}(z_t,t,t)$$注意后面的两个时间都是 t，它是一个速度。

上面 $$\omega $$ 表示引导的强度。对照原版的CFG，原版的CFG还在训练时是没有  $$\omega $$ 的，只在采样时使用，在训练时，有一步骤是以一定概率不给class，即 $$c=\empty$$。此处，同样有这步。

# 2025 CVPR Reversing Flow for Image Restoration

论文强调了**不确定范围（uncertainty scope）**的概念（如图1所示）：从HQ到LQ的退化过程遵循数据处理不等式（Data Processing Inequality, DPI），即HQ与中间图像之间的互信息（mutual information）逐渐减少。随着退化加深，LQ图像的“不确定范围”扩大——多个HQ图像可能退化到相似的LQ图像（例如，不同清晰图像添加雾霾后变得相似）。图1直观描绘了这一过程：灰色区域表示从中间状态的不确定范围，随着从LQ向HQ逆转，不确定范围缩小，互信息增加。

现有生成模型（如扩散模型或分数匹配模型）通常将退化过程建模为**随机变换（stochastic transformation）**，从高斯噪声开始逆向生成HQ图像。这引入了不必要的复杂性和计算开销（如数百步采样），因为**LQ图像已提供结构信息，无需从纯噪声重构**。论文指出，这种随机性导致训练和推理效率低下，且忽略了退化过程的确定性本质。

ResFlow的核心动机：将退化过程重新定义为**确定性路径（deterministic path）**，使用连续归一化流（Continuous Normalizing Flows）实现可逆映射，从而高效逆转退化，仅需少于4步采样。

> 这是论文的第一个评论，扩散模型属于随机前向过程，论文认为是不好的。flow matching 是确定性前向过程。但这个评论和从高斯噪声开始采样不是同一个问题。
>
> 论文认为，从高斯噪声开始采样没有必要，因为LQ图像已提供结构信息。
>
> 有一些扩散模型的方法从LQ图像开始恢复，比如论文第二页罗列的一些。但是论文说这些方法依然是随机前向过程。However, these approaches still treat the degradation process as a progressively diffusing stochastic forward process, which seems unnecessary and introduces additional complexity and inefficiency. Given that the degraded image is already known, the degradation process could be redefined as a deterministic forward process.

| 方法               | t=0 分布（起始/目标分布）                                    | t=T 分布（结束/噪声分布）                                    | 关键特点                                                     |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **DDRM [38]**      | 清晰图像分布（HQ/clean image distribution）。恢复过程的最终输出 x_0 是估计的 HQ 图像。 | 近似噪声分布（approximated noise distribution，通常高斯噪声 N(0, I)）。x_T 是 Markov 链的起始，条件于退化观测 y = H x + z（H 是退化算子，z 是已知噪声）。 | 基于预训练 DDPM，解决线性逆问题（如去模糊、超分辨率）。前向从 HQ 扩散到噪声；逆向从噪声恢复 HQ，条件于 LQ（degraded image）。 |
| **IR-SDE [64]**    | 清晰图像分布（HQ/high-quality image x(0)）。                 | 退化图像的噪声版本（LQ + fixed Gaussian noise, μ + ε，其中 μ 是 LQ，ε ~ N(0, σ²I)）。 | 使用 mean-reverting SDE 建模退化过程，从 HQ 扩散到 noisy LQ。逆向从 noisy LQ 开始恢复 HQ。非 Markov 链，而是连续 SDE。 |
| **I2SB [55]**      | 一个给定分布（通常清晰图像分布 HQ/clean data distribution）。 | 另一个给定分布（退化图像分布 LQ/degraded data distribution）。 | 构建 Schrödinger bridge（扩散桥），直接连接两个分布（clean 和 degraded）。非线性扩散过程，从 LQ 桥接到 HQ。边界对：t=0 为 clean，t=T 为 degraded（或反之）。 |
| **ResShift [112]** | 高分辨率图像分布（HR/high-resolution image distribution）。初始状态近似 HR。 | 低分辨率图像分布（LR/low-resolution image distribution）。最终状态近似 LR。 | 通过 shifting residual 在 HR 和 LR 之间构建 Markov 链。针对超分辨率（SR），减少步数；t=0 为 HR，t=T 为 LR。 |
| **RDDM [57]**      | 目标图像分布（HQ/target image distribution）。               | 纯噪声分布（pure noise, N(0, I)）用于生成；或噪声携带的输入图像（noise-carrying input, LQ + noise）用于恢复。 | 双重扩散：residual diffusion（从 HQ 到 LQ 的定向扩散）和 noise diffusion（随机扰动）。统一生成和恢复；t=T 根据任务调整（纯噪声或 noisy LQ）。 |
| **Resfusion [89]** | 原图像分布（ground truth/original image distribution）。     | noisy degraded 图像分布（noisy degraded images, LQ + weighted residual noise）。 | 将 residual term 引入前向过程，从 noisy LQ 开始逆过程。预测 resnoise（weighted residual + noise）；t=0 为 HQ，t=T 为 noisy LQ。统一训练/推理。 |

因此论文，要 reverses the deterministic paths between HQ and LQ images for image restoration。即要使用flow matching 方法，同时从 LQ开始进行恢复。这样的问题是前面谈到的不确定范围。

但直接用flow来建模HQ到LQ的退化是不可以的。这是因为退化导致互信息下降（信息处理不等式），而flow对应的ODE却会保持互信息不变。见命题1

## 命题1

flow 的 OED 保持互信息不变。
$$
MI(z_{t_1}, r) = MI(z_{t_2}, r),
$$
其中：

$$ z_t $$ 是随时间 $$ t $$ 变化的随机过程（random process），由普通微分方程（ODE）定义：$$\frac{\partial z_t}{\partial t} = v(z_t, t)$$。
$$ r $$ 是任意参考随机变量（reference random variable），可以是 $$ z_t $$ 的任意状态（如 $$ z_0 $$ 或其他 $$ z_{t'} $$）。
$$ MI(\cdot, \cdot) $$ 表示互信息，定义为两个随机变量之间的共同信息量，数学上等价于：
$$
MI(X, Y) = H(X) + H(Y) - H(X, Y),
$$
其中 $$ H(X) $$ 和 $$ H(Y) $$ 分别是 $$ X $$ 和 $$ Y $$ 的熵，$$ H(X, Y) $$ 是联合熵。

## 逆转流的基本形式（Reversing Flow for Image Restoration）

退化过程定义为随机过程{zt | 0 ≤ t ≤ 1}上的ODE：
$$
\frac{\partial z_t}{\partial t} = v(z_t, t); \quad 0 \leq t \leq 1,
$$
其中v是速度场（velocity field），z_0对应HQ图像x_HQ，z_1对应LQ图像x_LQ。

为使过程可逆，引入辅助过程{yt | 0 ≤ t ≤ 1}，增强状态：

$$
z_t^T = [x_t^T; y_t^T], \quad z_0^T = [x_{HQ}^T; y_0^T], \quad z_1^T = [x_{LQ}^T; y_1^T].
$$

yt编码“信息丢失”，与不确定范围耦合：当x_t接近x_LQ时，y_t与x_0的互信息增加，以保持整体MI(z_t, z_0)恒定

。图2框架：zt由数据组件x_t（HQ到LQ）和辅助组件y_t（不确定范围缩小）组成。前向过程通过插值定义，逆向过程通过匹配速度场学习。神经网络v_θ估计速度：
$$
\frac{\partial [x_t^T; y_t^T]^T}{\partial t} = v_\theta(x_t, y_t, t).
$$

在实际实现中，$$y_0=0$$, $$y_1\sim N(0,I)$$.从 $$y_0$$ 到 $$y_1$$ 熵是增加的。

## ResFlow 训练过程算法步骤

论文中的训练过程基于速度场匹配（velocity field matching），通过最小化Eq. (9)的损失函数实现。采用U-Net架构作为v_θ，Adam优化器，训练在256分辨率图像crops上进行。以下是算法步骤伪代码：
text算法: ResFlow 训练过程

输入: HQ-LQ图像对数据集 {(x0, x1)}，超参数 β=10, γ=1.75, 学习率 (详见Appendix C)
输出: 训练好的速度场网络 v_θ

1. 初始化神经网络 v_θ (采用DDPM的U-Net架构，timestep t 通过adaptive layer normalization嵌入)

2. 对于每个训练epoch:
   a. 从数据集采样一个batch的HQ-LQ对 (x0, x1)
   b. 对于每个样本:
      i. 设置 y0 = 0 (零向量)
      ii. 采样 y1 ~ N(0, I) (标准高斯分布)
      iii. 定义退化调度:
          α^x_t = 1 - t, σ^x_t = t  (对于数据组件 x)
          α^y_t = 1 - σ^y_t, σ^y_t = β / (1 - t + β)  (对于辅助组件 y, 熵保持)
      iv. 计算路径点 (geodesics):
          x_t = α^x_t * x0 + σ^x_t * x1
          y_t = α^y_t * y0 + σ^y_t * y1
          z_t = [x_t; y_t]  (增强状态)
          ˙z_t = [˙α^x_t * x0 + ˙σ^x_t * x1; ˙α^y_t * y0 + ˙σ^y_t * y1]  (真实速度)
      v. 通过网络预测速度: v_θ(x_t, y_t, t)
      vi. 计算时间权重: λ(t) = [cos(π/2 * (t - 2)) + 1]^γ  (强调t接近1)
      vii. 计算损失: L = ∫_0^1 λ(t) * ||v_θ(x_t, y_t, t) - ˙z_t||^2_2 dt  (积分近似或蒙特卡罗采样t)
   c. 平均batch损失
   d. 使用Adam优化器更新 θ (反向传播)

3. 重复步骤2直到收敛 (详见Appendix C的超参数，如学习率、batch size)
注意：损失优化确保凸传输成本非增，且无需模拟ODE（与传统流方法不同）。训练强调t接近1的困难样本，以平衡梯度。
ResFlow 采样过程算法步骤
论文中的采样（推理）过程基于逆向求解ODE Eq. (6)，从LQ图像开始，仅需4步采样（uniform time schedule）。以下是算法步骤伪代码：
text算法: ResFlow 采样过程 (图像恢复)

输入: 低质量图像 x1 (LQ), 训练好的 v_θ, 步数 N=4 (默认)
输出: 恢复的高质量图像 ˆx0 (HQ)

1. 采样辅助变量: y1 ~ N(0, I)  (标准高斯分布)

2. 初始化增强状态: z1 = [x1; y1]

3. 设置时间步: t 从1到0，分N=4步 (uniform schedule, e.g., Δt = 1/N)

4. 对于每个时间步 i 从1到N:
   a. 当前 t = 1 - (i-1)/N
   b. 预测速度: v = v_θ(z_t 的 x 组件, z_t 的 y 组件, t)
   c. 更新状态: z_{t-Δt} = z_t - v * Δt  (Euler方法或更高阶如Heun求解ODE dz/dt = v(z, t))
   d. (可选) 替换中间 ˆy_t 为 ground-truth y_t = α^y_t * 0 + σ^y_t * y1  (基于Eq.(5)，但概念上丢弃 ˆy_t)

5. 从最终 z0 提取 ˆx0 (丢弃 ˆy0)

6. 输出 ˆx0 作为恢复图像 (全分辨率测试)
注意：采样是确定性的（无随机噪声注入），通过辅助y消除不确定性。论文实验显示此过程在<4步内完成，适用于实时应用。

# PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING

pnp 方法的洞见是 *the proximal* step on the regularization term is effectively a denoising operation. 因此近端算子可以用BM3D或神经网络。

本文的出发点是，近来生成模型提供了智能的框架来从数据直接学习 priors，可以超越人工设计或神经网络去噪器。

## 图像恢复的数学问题

在论文的引言部分，图像恢复（image restoration）问题被表述为从退化观测（degraded observation）$$y$$ 恢复未知图像 $$x$$ 的逆问题（inverse problem），其中
$$
y = Hx + \xi
$$
这里，$$H$$ 是一个（线性）退化算子（degradation operator），$$\xi$$ 表示加性噪声（additive noise）模型。由于该问题是病态的（ill-posed）和高维的，求解具有挑战性。
论文假设图像 $$x$$ 来自具有密度 $$p_X$$ 的随机变量 $$X$$，观测 $$y$$ 来自具有密度 $$p_Y$$ 的随机变量 $$Y$$。然后，使用最大后验（maximum a posteriori, MAP）估计器求解具有最高后验概率的值：
$$
\arg\max_{x \in \mathbb{R}^d} \left[ \log p_{X\mid Y=y}(x) \right] = \arg\max_{x \in \mathbb{R}^d} \left[ \log p_{Y\mid X=x}(y) + \log p_X(x) \right],
$$
其中右侧第一项是数据保真（fidelity to the data），第二项是图像的先验分布（prior distribution）。
由于 $$p_X$$ 通常未知，且缺乏训练数据，论文转而考虑一个正则化优化问题（regularized optimization problem）：
$$
\arg\min_{x \in \mathbb{R}^d} \left\{ F(x) + R(x) \right\},
$$
其中
$$
F(x) := -\log p_{Y\mid X=x}(y)
$$
表示数据保真项（data-fidelity term），$$R : \mathbb{R}^d \to \mathbb{R}$$ 通常强制对解的一些假设（enforces some assumptions on the solution），以确保（唯一）最小化器的存在。例如，对于高斯噪声 $$\mathcal{N}(0, \sigma^2 I_d)$$，数据保真项对应
$$
F(x) = \frac{1}{2\sigma^2} \\mid Hx - y\\mid ^2.
$$
该优化问题可以通过近端分裂方法（proximal splitting methods）有效求解。

## PNP方法

![image-20251014191145906](/images/2025-12-25-flow/image-20251014191145906.png){: .img-fluid}

## *P**N**P* *MEETS* FLOW *MATCHING*

PnP-Flow定义时间相关去噪器：
$$
D_t := \mathrm{Id} + (1 - t) v^\theta_t, 
$$

> 这个去噪器的前提是直线路径，所以训练这个去噪器使用OT coupling 或 reflow. $$ \pi $$ 是耦合（optimal transport耦合可产生直线路径）。

>  对于最优传输（OT）耦合，有：

$$
v_t(f(t, x)) = T(x) - x, \tag{5}
$$

>  其中 $$ T $$ 是Monge映射。

在理想情况下：
$$
D_t(x) = \mathbb{E}[X_1 \mid  X_t = x],
$$
其中 $$ X_t = (1-t)X_0 + t X_1 $$。对于直线路径，去噪损失为0（Proposition 1）。

![image-20251014220345497](/images/2025-12-25-flow/image-20251014220345497.png){: .img-fluid}

注意：

1. PnP flow 中的flow 是预训练的。上述算法没有训练，而是迭代的图像恢复过程。
2. 其中 $$t_n$$  从小到大。其中 $$\tilde{z}^n$$ 这一步有使用插值，使用插值的原因在下一小节。$$t_n$$ 越小，插值时，加的噪声越大。

![image-20251014230941949](/images/2025-12-25-flow/image-20251014230941949.png){: .img-fluid}

注意 $$t=0$$ 时，$$\tilde{z}^n=\epsilon$$，从噪声直接去噪，再指向 $$y$$。

## 为什么要插值

在PnP-Flow算法中，插入插值步（interpolation step）的目的是确保去噪器 $$ D_t $$ 能够有效工作，这与Flow Matching（FM）模型的特性以及算法的迭代过程密切相关。以下是详细的解释：

### 原因与背景

PnP-Flow结合了PnP框架与FM模型，其中去噪器 $$ D_t $$ 是基于预训练的FM速度场 $$ v^\theta_t $$ 定义的：
$$
D_t = \mathrm{Id} + (1 - t) v^\theta_t. \tag{6}
$$
理想情况下，$$ D_t(x) $$ 将路径上的点 $$ X_t = (1-t)X_0 + t X_1 $$ 投影回目标分布 $$ P_1 $$ 的样本 $$ X_1 $$，其中 $$ X_0 \sim P_0 $$（潜在分布），$$ X_1 \sim P_1 $$（数据分布），$$ (X_0, X_1) \sim \pi $$（耦合）。然而，传统PnP-FBS算法的迭代点（通过梯度步更新后）不一定位于FM路径 $$ X_t $$ 上，而 $$ D_t $$ 的设计假设输入点在该路径上。如果输入点偏离 $$ X_t $$ 的支持集，去噪效果会显著下降。

### 插值步的必要性

论文在第5页（Section 3.2）中指出，经典PnP-FBS直接在梯度步后应用去噪器，但由于 $$ D_t $$ 针对 $$ X_t $$ 优化，若梯度步输出的 $$ z $$ 不位于 $$ X_t $$ 支持集，效果不佳。因此，引入插值步将 $$ z $$ “投影”回FM路径。

### 技术细节与动机

- **路径一致性**：FM模型（尤其是OT-FM）产生直线路径 $$ X_t = (1-t)X_0 + t X_1 $$。插值步确保迭代点与此路径对齐，从而利用 $$ D_t $$ 的最佳性能（Proposition 1表明直线路径下去噪损失为0）。
- **避免退化**：若不插值，$$ D_t $$ 可能将 $$ z^n $$ 映射到不相关区域，特别是在 $$ t $$ 接近1时，$$ D_t $$ 趋于恒等变换（$$ D_1 = \mathrm{Id} $$），导致算法退化为仅依赖数据保真项。
- **噪声引入**：$$ \epsilon \sim P_0 $$ 的随机性模拟FM的潜在分布采样，防止 $$ D_t $$ 简单地将 $$ \tilde{z}^n $$ 映射回 $$ z^n $$（若 $$ \epsilon $$ 与 $$ z^n $$ 耦合，效果会抵消，详见论文Remark 2）。

## 讨论

pnp 方法，需要知道 $$H$$，这个比较难办?

# POSTERIOR-MEAN RECTIFIED FLOW

论文《Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration》（PMRF）关注照片真实图像恢复（Photo-Realistic Image Restoration, PIR）问题，即从退化测量（如噪声、模糊图像）中重建视觉上吸引人的图像。该领域算法通常通过失真度量（如PSNR、SSIM、LPIPS）和感知质量度量（如FID、KID、NIQE）评估，目标是实现最低失真而不牺牲感知质量（即重建图像看起来自然）。
现有方法的局限性：

后验采样（Posterior Sampling）：许多扩散或流模型（如DPS、DDS）尝试从后验分布 $$ p_{X\mid Y} $$ 采样，理论上可实现完美感知指数（即重建分布 $$ p_{\hat{X}} = p_X $$，其中 $$ p_X $$ 是真实图像分布）。然而，根据Blau & Michaeli (2018)，其MSE（均方误差）是无约束最小MSE（MMSE）的两倍，即 $$ \mathbb{E}[\\mid X - \hat{X}\\mid ^2] = 2 \times \mathrm{MMSE} $$。
GAN+失真损失：优化失真（如MSE）和感知（如GAN）损失的加权和，可遍历失真-感知权衡曲线（Distortion-Perception Tradeoff），但GAN优化困难，尤其当感知损失权重较大时，实际性能不如后验采样。

论文的核心动机：针对完美感知指数约束下最小化MSE的最优估计器 $$ \hat{X}_0 $$，定义为：
$$
\hat{X}_0 = \arg\min_{p_{\hat{X}\mid Y}} \mathbb{E}[\\mid X - \hat{X}\\mid ^2] \quad \mathrm{s.t.} \quad p_{\hat{X}} = p_X.
$$
Freirich et al. (2021)证明，$$ \hat{X}_0 $$ 可通过先预测后验均值 $$ \hat{X}^* = \mathbb{E}[X\mid Y] $$（MMSE估计），然后将其最优传输（Optimal Transport）到真实分布 $$ p_X $$ 来构建。该MSE通常严格小于后验采样的MSE（如图1所示）。
受此启发，PMRF提出一个简单高效算法：使用整流流（Rectified Flow）近似最优传输地图，将后验均值预测传输到高质量图像。论文通过理论分析（如Proposition 1）和实验证明，PMRF在去噪、超分辨率、补全、着色和盲面部恢复等任务中优于基线方法。
训练过程
PMRF训练分为两个阶段（见Algorithm 1），假设 $$ X $$ 是真实图像随机向量，$$ Y $$ 是退化测量。

阶段1：后验均值预测
训练模型 $$ f_\omega $$ 最小化MSE损失，近似后验均值 $$ \mathbb{E}[X\mid Y] $$：
$$
\omega^* = \arg\min_{\omega} \mathbb{E}\left[ \\mid X - f_\omega(Y)\\mid ^2 \right].
$$

此阶段可使用现成高PSNR模型跳过。
实际中，$$ f_\omega $$ 可为CNN或Transformer架构，优化目标是重建接近真实图像的平滑预测。


阶段2：整流流模型训练
训练向量场 $$ v_\theta $$ （Rectified Flow模型），最小化流匹配损失：
$$
\theta^* = \arg\min_{\theta} \int_0^1 \mathbb{E}\left[ \\mid (X - Z_0) - v_\theta(Z_t, t)\\mid ^2 \right] \, dt,
$$
其中：

$$ Z_t = t X + (1-t) Z_0 $$，为直线路径前向过程。
$$ Z_0 = f_{\omega^*}(Y) + \sigma_s \epsilon $$，$$ \epsilon \sim \mathcal{N}(0, I) $$，$$ \sigma_s $$ 是小噪声超参数（缓解源/目标分布维数不匹配引起的奇异性）。
$$ t \sim \mathcal{U}[0,1] $$。
此损失训练 $$ v_\theta $$ 预测从后验均值到真实图像的直线方向，近似最优传输地图。



训练中，$$ v_\theta $$ 通常为U-Net架构，输入包括 $$ Z_t $$ 和时间 $$ t $$（通过位置编码嵌入）。
推理过程
推理时，给定退化测量 $$ y $$，PMRF解决ODE生成重建图像 $$ \hat{x} $$：
$$
\frac{d \hat{Z}_t}{dt} = v_{\theta^*}(\hat{Z}_t, t), \quad \hat{Z}_0 = f_{\omega^*}(y) + \sigma_s \epsilon,
$$
其中 $$ \epsilon \sim \mathcal{N}(0, I) $$。
使用Euler方法离散求解（K步）：

采样 $$ \epsilon \sim \mathcal{N}(0, I) $$。
初始化 $$ \hat{x} = f_{\omega^*}(y) + \sigma_s \epsilon $$。
对于 $$ i = 0 $$ 到 $$ K-1 $$：
$$
\hat{x} \leftarrow \hat{x} + \frac{1}{K} v_{\theta^*}\left( \hat{x}, \frac{i}{K} \right).
$$

返回 $$ \hat{x} $$。

此过程从后验均值开始，通过整流流逐步“校正”到真实分布，生成低失真、高感知质量图像。论文实验显示，PMRF在CelebA-Test盲面部恢复基准上达到SOTA（如表1所示）。
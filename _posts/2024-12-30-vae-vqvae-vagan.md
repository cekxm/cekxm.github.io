---
layout: post
title: "Vae Vqvae Vagan"
date: 2024-12-30 00:09:07 +0800
categories: []
description: 简要介绍
tags: 
thumbnail: 
toc:
  sidebar: left
typora-root-url: ../
---

## 变分

在数学中，“variational”这个术语通常用于变分法（calculus of variations）的背景下，它是数学分析领域处理最大化或最小化泛函（functional）的一个领域。泛函是以函数为输入的函数。例如，变分法中的经典问题是找到光线在反射于表面时所需的最短时间路径。

“variational methods”中的“variational”指的就是这个概念：它是关于找到某个函数的最佳值（最小化或最大化）的概念。

在机器学习和变分自编码器（variational autoencoders，VAEs）的背景下，“variational”一词来自于训练过程中使用的变分推断方法（variational inference）。变分推断是一种贝叶斯推断方法，将问题转化为一个优化问题。它的目标是在可处理更容易的一族分布中找到对真实后验分布（通常难以计算）的最佳近似。

在变分自编码器中，我们使用变分推断来训练网络并学习模型的参数。训练过程的目标是找到最大化给定参数的数据似然性的参数，同时满足一些约束条件。通常，这被表达为最小化“损失”函数，该函数衡量预测值与实际值之间的差异。

因此，尽管所优化的具体细节不同，但基本概念是相同的：找到某个函数的最佳（最小或最大）值。

## VAE

Tutorial on Variational Autoencoders by CARL DOERSCH

VAE 的目标是为了学习得到数据的分布，并且能够从这个分布中采样得到一个数据。

理论依据是似然最大化。

下面是简要的一些原理，推荐阅读上面的Tutorial。

首先有一个X的数据集D，目标是要对X进行建模，或者说，要得到一个生成模型，通过这个生成模型来采样得到类似D中的X。

采用的方法是似然最大化。假设$$\theta$$是模型参数，则最大化似然函数
$$
\theta = \arg \max P(X;\theta )
$$
注意，上式中 $$X$$ 是一个随机变量。而我们有的是一个数据集，所以实际上是
$$
\theta = \arg \max E_{X\sim D}[P(X;\theta )]
$$
在推导中，认为X是一个随机变量。

### Latent Variable Models

为了完成这个目标，在生成模型中引入隐藏变量$$z$$。可以认为是一种数学手段，也可以从理论上理解。隐藏变量是从生成模型采样得到一个数据时的某种设置。比如要生成手写数字，隐藏变量表示具体的数值，笔画风格，笔画粗细等等。

引入隐藏变量后，令生成函数（生成器）为 $$X=f(z;\theta )$$。$$f$$ 是一个神经网络或其他确定性的函数。但这个确定性的生成模型不能使用梯度下降来优化参数，因为此时 $$P(X\mid z)$$ 是一个狄拉克函数。所以必须放宽一下，认为 X 是以$$f(z;\theta )$$为中心的高斯随机变量。
$$
P(X\mid z)=N(X\mid f(z;\theta),\sigma^2 *I)
$$
有了生成模型，可以求得似然函数：
$$
P(X)=\int P(X\mid z;\theta )P(z)dz
$$
上式中，还涉及到确定 $$P(z)$$是什么样的才是合理的。理论上，$$z$$的分布是复杂的，**但我们倾向于不去手动设计z的分布。一般可以设为正态分布 $$N(z\mid 0,I)$$。**这是因为任何复杂分布的随机变量，能够用正态分布随机变量的函数来生成。所以可以认为，生成器的前几层是生成符合实际的隐藏变量，后边的层再由隐藏变量生成数据。

为了求得上面的似然，可以对$$z$$进行采样，然后用**采样平均进行计算**（注意，是对每一个D中的X，用采样平均来近似统计平均）。但会碰到一个问题：在高维空间中，必须采样大量$$z$$，才能计算准确的$$P(X)$$。因为大多数$$z$$，$$P(X\mid z)$$都会非常小，对于计算$$P(X)$$起不到作用。

### Evidence lower bound (ELBO)

**变分自编码器（VAE）的核心思想是尝试从可能生成 $$X$$的潜在变量 $$z$$中采样，并仅基于这些 $$z$$计$$P(X)$$。**为了实现这一点，我们需要引入一个新的函数 *Q*(*z*∣*X*)，它能够接收输入 *X*并输出一个概率分布，该分布描述了可能生成 *X*的 *z*值的范围。

上面这段话，也就是说如果有这样的一个函数 *Q*(*z*∣*X*)，我们可以高效的计算 $$E_{z\sim Q}P(X\mid z)$$。但这个期望并不是所需要的似然。所以要通过推导了解二者之间的关系。

经过推导可以得到：
$$
\log P(X) -\mathcal{D}[Q(z\mid X)\parallel P(z\mid X)]=E_{z\sim Q}[\log P(X\mid z)]-\mathcal{D}[Q(z\mid X)\parallel P(z)]
$$
上式右边称为ELBO，是似然函数对数的下限，因为等式左边第二项 KL 是大于0的。假设通过最大化ELBO，能使得$$Q(z\mid X) = P(z\mid X)$$，则ELBO就等于似然了。当然，实际上是不可能的，因为为了可以最优化，我们选择了特定形式的$$Q(z\mid X)$$，即高斯分布。

引入 $$Q(z\mid X)$$ 后，VAE 通过最大化ELBO来求参数 $$\theta$$。最大数ELBO意味着要让似然函数大，同时要让 $$Q$$ 和 $$P(z\mid X)$$ 的KL距离小。

### 如何最大化 ELBO

我们有X的一个数据集，可以使用梯度下降法来最大化。

先来看看ELBO的第二项 $$\mathcal{D}[Q(z\mid X)\parallel P(z)]$$。在实现中，选择 $$Q(z\mid X)$$为高斯分布 $$N(z\mid \mu(X;\theta),\Sigma(X,\theta))$$，并且限制协方差矩阵为对角阵。由X获得z即为VAE中的编码器，首先计算得到 $$\mu,\Sigma$$，这是确定性的函数，可通过神经网络实现。然后再采样得到z。由于 $$Q(z\mid X)$$和$$P(z)$$都是高斯的，它们的KL距离可以直接计算，即有解析式，可以计算梯度。

再来看第一项。$$\log P(X\mid z)$$ 的梯度是可以计算的。对于每一个X实例，可以通过 $$Q(z\mid X)$$采样得到多个 z，则可以计算 $$E_{z\sim Q}[\log P(X\mid z)]$$的梯度。

注意，我们有的是X的数据集D，所以要对D中每一个实例，计算它对应的梯度，再把所有这些梯度求平均。

但上述这样计算效率不高。实际上是**采用SGD**。

即每一个迭代的step只计算一个X实例的梯度，并且**只对z采样一次，只计算一个z采样值的梯度**。这个是SGD的做法。注意这边有两个层次的SGD，一个是X的，一个是z的。

## VQ-VAE

[VQ-VAE解读](https://zhuanlan.zhihu.com/p/91434658)

## VQGAN

[详解VQGAN（一）\| 结合离散化编码与Transformer的百万像素图像生成](https://zhuanlan.zhihu.com/p/515214329)


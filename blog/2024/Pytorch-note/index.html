<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pytorch Note | 计算机视觉 </title> <meta name="author" content="Erkang Chen"> <meta name="description" content="简要介绍"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cekxm.github.io/blog/2024/Pytorch-note/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> 计算机视觉 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pytorch Note</h1> <p class="post-meta"> Created in December 29, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tensor">Tensor</h2> <p>tensors 和 numpy ndarrays 类似，额外的好处是可以放入GPU，用于加速计算。ndarrays 不能放入gpu。</p> <p>tenor &lt;–&gt; numpy b = a.numpy() a = torch.from_numpy(b) 可以进行互换，注意互换行为类似于建立新的view，存储是同一份，修改一个，会改变另一个。</p> <p>运算可以新生成一个 tensor，也可以 in-place。in-place 运算的名称要带一个后缀 <em>，比如 x.t</em>()</p> <p>x.view((d1,d2,…)) 函数用于 reshape，注意不同的view所对应的存储是同一份，修改其中一个view，会改变所有view.</p> <p>cuda tensor x=x.to(device) 函数可以在cpu和gpu间进行转移 to 支持改变 dtype</p> <p>tensor 的attrib .requires_grad 是否需要梯度。只能改变 leaf node 的这个值 .grad 记录梯度。我测试只能保存 leaf node 的梯度？ .grad_fn 记录生成这个 tensor 的函数</p> <h3 id="dtype">dtype</h3> <ul> <li>torch.float32 / torch.float</li> <li>torch.float64 / torch.double</li> <li>torch.float16 / torch.half</li> <li>torch.uint8</li> <li>torch.int8</li> <li>torch.int16 / torch.short</li> <li>torch.int32 / torch.int</li> <li>torch.int64 / torch.long</li> <li>torch.bool</li> </ul> <h3 id="基本运算">基本运算</h3> <p>可以使用以下两种风格</p> <ul> <li>Tensor.op，比如 b = a.flattern()，inplace 操作后缀 _</li> <li>torch.op，比如 b= torch.flattern(a)</li> </ul> <h3 id="view">view()</h3> <p>改变tensor维数或维数的尺寸，不改变内存 <strong>view 并不是任何时候都能用，还要看tensor 是不是内存连续的！</strong> 见下面的例子。 view(-1) 类似于 flatten</p> <h3 id="contiguous">contiguous</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">).</span><span class="nf">view</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="nf">tensor</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="nf">tensor</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">.</span><span class="nf">is_contiguous</span><span class="p">()</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nc">Traceback </span><span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="sh">"</span><span class="s">&lt;stdin&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">view</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">compatible</span> <span class="k">with</span> <span class="nb">input</span> <span class="n">tensor</span><span class="sh">'</span><span class="s">s size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
</span></code></pre></div></div> <p>b 无法转为一维的，因内存不连续。所以实际上，不少 tensor操作会首先执行 contiguous()</p> <h3 id="flatten">flatten</h3> <p>b 无法转为一维的，因内存不连续。所以实际上，不少 tensor操作会首先执行 contiguous()，</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="nf">tensor</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">.</span><span class="nf">is_contiguous</span><span class="p">()</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span>
<span class="nf">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">=-</span><span class="mi">1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span>
<span class="nf">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">]])</span>
</code></pre></div></div> <h3 id="reshape">reshape</h3> <p>有了 view 为什么还要shape呢？原因同上，当内存不连续时，实际上进行了 deepcopy。</p> <h3 id="squeezeunsqueeze">squeeze/unsqueeze</h3> <p>squeeze 把所有为size==1的维度去掉。</p> <p>torch.unsqueeze(input, dim, out=None) 在第 dim （从1开始算）个维度后增加size=1的1维。dim=0，即为 prepend 维度1. e.g. (3,5) -&gt; (1,3,5)</p> <h2 id="parameter">Parameter</h2> <p>Parameter 是 Tensor 的子类，用于做 module 的参数。 Parameter的用处：</p> <ul> <li>在 Module 的 init 中声明。当它在 Module 中被声明为属性时，会自动加入到 module 的参数列表，也就是会在 Module.parameters iterator 中出现。如果在 init 中声明 Tensor 则没有这个效果。</li> <li>默认requires_grad 属性为 True</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">):</span>
        <span class="bp">...</span>
        <span class="k">if</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span>
                <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="sh">'</span><span class="s">bias</span><span class="sh">'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span>
</code></pre></div></div> <ul> <li>module 的参数要用 Parameter</li> <li>有时候我们希望在module中记录一些中间状态，但不希望注册，则用 Tensor</li> </ul> <p>parameter.data 就是 tensor <strong>注意，parameter.data.requires_grad</strong> 可能为False，不影响 parameter 的更新。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv2d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">conv2d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">conv2d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

</code></pre></div></div> <h3 id="参数注册和顺序">参数注册和顺序</h3> <p>在 <code class="language-plaintext highlighter-rouge">Module.__init__</code> 函数中声明层，会注册相应的参数。在 Module.parameters() 返回的参数迭代器中，按照 init 函数中声明的顺序来排序。改变声明顺序不会对网络的功能造成问题，只是要注意参数的顺序，可以打印参数的名称来查看顺序。</p> <p>named_parameters() 是一个迭代器，需要类似函数调用</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">para</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">para</span><span class="p">)</span>
</code></pre></div></div> <h3 id="参数初始化">参数初始化</h3> <p>参数初始化： 可使用两种方法</p> <ol> <li>nn.init</li> <li>tensor.xxx_ 函数 para.data.normal_(mean, stddev)</li> </ol> <h2 id="autograd">autograd</h2> <ul> <li>autograd 机制深入到 tensor 这一级，创建一个 tensor，它就有属性 grad, requires_grad。</li> <li>Torch 对 tensor 的运算自动支持 autograd机制，不需要其他操作</li> <li>调用 loss.backward() 会按照有向图的反方向自动计算梯度</li> <li>autograd 机制是低层的，所以用户Module代码可以没有 backward 函数</li> </ul> <p>该机制是深入到 tensor 这一层 例： x -&gt;y -&gt; out out.backward() 是计算 out 对以其为顶点的树的结点的梯度，比如对 x 的梯度 若 out -&gt; loss 假设 loss 对 out 的梯度为 do out.backward(do) 是计算 loss 对 x 的梯度</p> <p>梯度的计算用 jacobian y = f(x) z = g(y) 令 y 对 x 的Jacobian 为 J</p> <p><strong>missing</strong></p> <h3 id="解除记录梯度的方法">解除记录梯度的方法</h3> <p><strong>tensor</strong>解除记录梯度</p> <ol> <li>x.requires_grad_(False)</li> <li>x.detach()</li> <li>全局解除和全局恢复，一般和 with 搭配使用，有以下几个函数</li> </ol> <ul> <li>torch.no_grad()</li> <li>torch.enable_grad()</li> <li>torch.set_grad_enabled(True/False) 这三个和 torch.is_grad_enabled() 有关，它不是为每一个parameters设置，而是上层的一种统一机制。其中 no_grad, enable_grad 这两个是一对的。但一般在搭配 with 使用时，可以只使用 no_grad()，因为 with 机制的 exit 函数会恢复原来的设置。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>  <span class="c1">#当评估已经训练好的网络时，就不用跟踪梯度了
</span>    <span class="k">pass</span>
</code></pre></div></div> <h3 id="梯度清零">梯度清零</h3> <p>梯度计算是累积的，每一个 batch 迭代要清零梯度。 optimizer.zero_grad() <strong>注意，梯度清零是在 batch 级，而不是在 epoch 级。</strong></p> <h3 id="autogradfunction">autograd.Function</h3> <p>当需要一种新的<strong>运算</strong>，这种运算torch不提供的时候，就需要扩展 autograd。办法是通过继承 autograd.Function。Function 里面必须有 forward, backward 函数。通常 forward里面会使用torch的一些运算，但在反向传播时直接调用 backward函数，而不再根据forward中的运算进行反向传播计算。</p> <p>如下代码自定义了一个运算，注意它的backward和forward的运算没有关系，而是把梯度强性设置为 0.8。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SumFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_bias</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">if</span> <span class="n">ctx</span><span class="p">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span><span class="o">*</span><span class="mf">0.8</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">ctx</span><span class="p">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">grad_output</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_bias</span>
</code></pre></div></div> <h3 id="使用-autogradfunction">使用 autograd.Function</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum2</span> <span class="o">=</span> <span class="n">SumFunction</span><span class="p">.</span><span class="nb">apply</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">sum2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h2 id="torchnn">torch.nn</h2> <p>torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample. For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width. If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.</p> <p>params = list(net.parameters()) params[0] 是Parameter 类对象 params[0].data 是 tensor</p> <h2 id="nnmodule">nn.Module</h2> <p>以下都属于 Module。</p> <ul> <li>CNN 中的层，比如 Conv2d</li> <li>Sequential，是把许多层串接在一起</li> <li>model或net，一般网络就是一个 nn.Module 自定义 Module，必须要有 forward 函数，不需要 backward 函数，因为不管是 pytorch提供的运算还是自定义的运算（autograd.Function）,这部分已经处理好了。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">        In the constructor we instantiate two nn.Linear modules and assign them as        member variables.        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TwoLayerNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">        In the forward function we accept a Tensor of input data and we must return        a Tensor of output data. We can use Modules defined in the constructor as        well as arbitrary operators on Tensors.        </span><span class="sh">"""</span>
        <span class="n">h_relu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">h_relu</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div> <h2 id="nnfunctional">nn.functional</h2> <p><strong>nn.functional 提供的函数和 nn.Module提供的类功能是一样的</strong></p> <p>比如</p> <ul> <li>nn.functional.conv1d</li> <li>nn.Conv1d 注意大小写。让我们看下它们的实现代码。</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="nf">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="nf">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="nf">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="nf">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="bp">False</span><span class="p">,</span> <span class="nf">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">groups</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Expected 3D tensor as input, got {}D tensor instead.</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="nf">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="nc">ConvNd</span><span class="p">(</span><span class="nf">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="nf">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="nf">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="bp">False</span><span class="p">,</span>
               <span class="nf">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span><span class="p">,</span>
               <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</code></pre></div></div> <p>可见，Conv1d 调用了 conv1d，而 conv1d 调用了 ConvNd。</p> <p><strong>nn.functional 和 nn.Module 都可以用于定义CNN网络，也都支持 autograd 机制，区别是</strong></p> <p>使用 nn.functional，必须自己定义层的参数(weight, bias 等)并进行维护。而使用nn.Module，这些参数有类的机制进行定义和维护</p> <h3 id="建议">建议</h3> <ul> <li>对于有参数的层，使用 nn.Module</li> <li>对于无参数的层，建议使用 nn.Module，因为 pytorch 基本上都提供了响应的 Module。当然使用 nn.functional 也是可以的</li> <li>nn.functional 是更灵活的，在有些只需要运算，而不需要保持参数的情况下，使用 nn.functional</li> </ul> <h2 id="参数保存和读取">参数保存和读取</h2> <h3 id="state_dict"><code class="language-plaintext highlighter-rouge">state_dict</code></h3> <ul> <li>nn.Module 有<code class="language-plaintext highlighter-rouge">state_dict</code>，保存参数，key 是 <code class="language-plaintext highlighter-rouge">__init__</code> 声明的layer名称+参数名，比如 conv1.weight，value 是 参数 tensor</li> <li>optimizier 也有<code class="language-plaintext highlighter-rouge">state_dict</code>，保存它自己的状态，以及超参数</li> </ul> <h3 id="三个函数">三个函数</h3> <ul> <li>torch.save: 保存 dict</li> <li>torch.load: 读取 dict</li> <li>torch.nn.Module.load_state_dict: 加载参数</li> </ul> <h3 id="标准用法">标准用法</h3> <h4 id="train-and-test">train and test</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</code></pre></div></div> <p>For test/deploy</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <h4 id="checkpoint">checkpoint</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">epoch</span><span class="sh">'</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">model_state_dict</span><span class="sh">'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">'</span><span class="s">optimizer_state_dict</span><span class="sh">'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
            <span class="bp">...</span>
            <span class="p">},</span> <span class="n">PATH</span><span class="p">)</span>
</code></pre></div></div> <p>Load checkpoint:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">TheOptimizerClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">'</span><span class="s">model_state_dict</span><span class="sh">'</span><span class="p">])</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">'</span><span class="s">optimizer_state_dict</span><span class="sh">'</span><span class="p">])</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="sh">'</span><span class="s">epoch</span><span class="sh">'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">]</span>

<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span><span class="c1"># - or -model.train()
</span></code></pre></div></div> <h2 id="dataset">Dataset</h2> <h3 id="imagefolder">Imagefolder</h3> <p>文件夹结构</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- data
    - class 1
        - img1.jpg
        - img2.jpg
    - class 2
        - ...
data = torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms)
</code></pre></div></div> <p>执行后，Dataset getitem 返回 (sample, target)</p> <h3 id="自定义-dataset">自定义 Dataset</h3> <p>自定义Dataset是 torch.utils.data.Dataset 的子类，必须实现 <code class="language-plaintext highlighter-rouge">__getitem__</code> 和 <code class="language-plaintext highlighter-rouge">__len__</code> 方法。</p> <p><code class="language-plaintext highlighter-rouge">__getitem__</code> 方法使得 dataset 可以用索引 [n]。但它的输出是什么格式，是可以自己定义的。 比如 ImageFolder Dataset，返回 tuple (tensor, label)</p> <p>而 <a href='https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"&gt;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html' rel="external nofollow noopener" target="_blank">这个 tutorial </a>中，getitem 返回 {‘image’: image, ‘landmarks’: landmarks}</p> <h3 id="transform">Transform</h3> <p>transform 是可以被当作函数调用的类。</p> <p>transform 一般作为生成时 Dataset 时的参数，并且在 getitem 中使用。所以</p> <ul> <li>若使用pytorch提供的Dataset，transform 的接口和输出必须和它们的 getitem 兼容。</li> <li>使用 torchvision.transforms 中 PIL 相关的变换，注意它的接口是 PIL image，输出也是 PIL image。</li> <li>如果是自定义 Dataset，则可灵活处理。</li> </ul> <h4 id="torchvisiontransforms">torchvision.transforms</h4> <p>提供了以下几种：</p> <ul> <li>on PIL Image</li> <li>on torch.*Tensor</li> <li>conversion</li> <li>Lambda and Functional</li> </ul> <p>和图像相关的就是第一种了。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from PIL import Image
path = './data/animals/cats/cats_00001.jpg'
with open(path,'rb') as f:
    img = Image.open(f) #这是一个 lazy 操作，还没读取内容
    img = img.convert('RGB')

from torchvision import transforms
transform = transforms.RandomResizedCrop(200)
img2 = transform(img)
</code></pre></div></div> <ul> <li>注意，PIL 读取后像素值在[0,255]</li> </ul> <h5 id="totensor">ToTensor</h5> <p>该变换把 PIL image 或 numpy.ndarray (H x W x C) in the range [0, 255] 变换成 torch.FloatTensor of shape (C x H x W) in the range <strong>[0.0, 1.0]</strong>。</p> <h4 id="自定义-transform">自定义 transform</h4> <p>transform 是可以被当作函数调用的类，为 object 的子类，需实现 <code class="language-plaintext highlighter-rouge">__call__</code> 方法和<code class="language-plaintext highlighter-rouge">__init__</code> 方法。</p> <p>对于图像，出了PIL，还常用 skimage 读取。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from skimage import io
img = io.imread('./data/animals/cats/cats_00001.jpg')
</code></pre></div></div> <ul> <li>注意，io 读取后像素值在[0,255]</li> </ul> <h4 id="transform-串接">transform 串接</h4> <p>torchvision.transforms.Compose([…])</p> <h3 id="torchutilsdatadataloader">torch.utils.data.Dataloader</h3> <p>支持两种 dataset</p> <ul> <li>map style，即Dataset子类，实现len和getitem的</li> <li>iterable style， IterableDataset 子类，实现 <code class="language-plaintext highlighter-rouge">__iter__()</code> 方法。,</li> </ul> <h4 id="参数">参数</h4> <ul> <li>batch_size</li> <li>shuffle 每一次batch（含第一次batch），重新排序。默认是 False。此项和 sampler 不共同使用。</li> <li>sampler 从 data 中获取sample的方法</li> </ul> <h4 id="sampler">sampler</h4> <ul> <li>torch.utils.data.SubsetRandomSampler(indices)，从给定的 indices 中获取。可用于 train/validation 的划分，比如这个 <a href='https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb"&gt;https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb' rel="external nofollow noopener" target="_blank">gist</a> </li> </ul> <h4 id="automatic-batching">automatic batching</h4> <ul> <li>It always prepends a new dimension as the batch dimension.It automatically converts</li> <li>NumPy arrays and Python numerical values into PyTorch Tensors.</li> <li> <strong>It preserves the data structure</strong>, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for list s, tuple s, namedtuple s, etc. 即 dictionary 还是 dictionary，tuple 还是 tuple.</li> </ul> <h4 id="collate_fn"><code class="language-plaintext highlighter-rouge">collate_fn</code></h4> <h2 id="optimizer">Optimizer</h2> <ul> <li>torch.optim.Adadelta</li> <li>torch.optim.Adagrad</li> <li>torch.optim.Adam</li> <li>torch.optim.AdamW</li> <li>torch.optim.SGD</li> <li>…</li> </ul> <h2 id="pytorch-训练代码参考">pytorch 训练代码参考</h2> <p>注意：</p> <ul> <li>训练时，每个batch会更新一次参数，最终的train loss是在epoch这一级的，是所有batch的平均结果，因此实际上是不同的参数的平均结果。</li> <li>而验证时，一次epoch后，用最后的参数进行验证，loss 对应的参数是一致的</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 模型实例
</span><span class="n">model_ft</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model_ft</span> <span class="o">=</span> <span class="n">model_ft</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Observe that all parameters are being optimized
</span><span class="n">optimizer_ft</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">params_to_update</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># Setup the loss fxn
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">data_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([...]}</span>

<span class="c1"># 数据部分，以下是训练用
# 若有validation，则还需要响应的 data 和 loader
</span><span class="n">image_datasets</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span><span class="n">x</span><span class="p">),</span><span class="n">data_transforms</span><span class="p">)</span>
<span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">]</span>

<span class="n">val_acc_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
    
    <span class="c1"># 这个函数不是进行训练，而是设置模型的mode
</span>    <span class="c1"># 部分层在 train 和 val 时计算是不一样的，比如 dropout, bn
</span>    <span class="c1"># 通过设置 mode，告诉这些层要如何计算
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    
    <span class="c1"># 这两个参数是epoch 级别统计 loss 的 
</span>    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">running_corrects</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># 每个epoch先 train，需要梯度
</span>    <span class="c1"># 因为后面eval时设置了 no_grad
</span>    <span class="c1"># 这里得纠正回来
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">enable_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1"># 注意，zero_grad() 是在 batch 级别的
</span>            <span class="n">optimizer_ft</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="n">model_ft</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            
            <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#为了求 train loss
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer_ft</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">running_corrects</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">running_corrects</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{} Loss: {:.4f} Acc: {:.4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">))</span>
            
    <span class="c1"># 这个函数不是进行训练，而是设置模型的mode
</span>    <span class="n">model_ft</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">running_corrects</span> <span class="o">=</span> <span class="mi">0</span>
       
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">]:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="n">output</span> <span class="o">=</span> <span class="n">model_ft</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            
            <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># validation 就没有 backward 和 step 了
</span>            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">running_corrects</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">running_corrects</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{} Loss: {:.4f} Acc: {:.4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">'</span><span class="s">eval</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">))</span>
    
    <span class="c1"># 比较 evaluation 的 acc，保存最大的那个
</span>    <span class="k">if</span> <span class="n">epoch_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
       <span class="n">best_acc</span> <span class="o">=</span> <span class="n">epoch_acc</span>
       <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">model_ft</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
    <span class="n">val_acc_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_acc</span><span class="p">)</span>
        
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Best val Acc: {:4f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">best_acc</span><span class="p">))</span>
</code></pre></div></div> <h2 id="misc">misc</h2> <h3 id="进度条">进度条</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)):</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/DiTF/">Diffusion Transformer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/uie/">Under water enhancement</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/optical_flow/">Optical Flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Teaching-Tailored-to-Talent/">Teaching Tailored To Talent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/VINS-and-ESVO2/">Vins And Esvo2</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Erkang Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-diffusion-transformer",title:"Diffusion Transformer",description:"DiT, SD3, Flux, PixArt, DiTF",section:"Posts",handler:()=>{window.location.href="/blog/2026/DiTF/"}},{id:"post-under-water-enhancement",title:"Under water enhancement",description:"UIE, U-trans, DNnet, Five A+",section:"Posts",handler:()=>{window.location.href="/blog/2026/uie/"}},{id:"post-optical-flow",title:"Optical Flow",description:"RAFT, SEA-RAFT, Neuflow",section:"Posts",handler:()=>{window.location.href="/blog/2026/optical_flow/"}},{id:"post-teaching-tailored-to-talent",title:"Teaching Tailored To Talent",description:"Prompt, Depth-Anything, Diffusion, Image Restoration",section:"Posts",handler:()=>{window.location.href="/blog/2026/Teaching-Tailored-to-Talent/"}},{id:"post-vins-and-esvo2",title:"Vins And Esvo2",description:"VINS, ESVO, SFM",section:"Posts",handler:()=>{window.location.href="/blog/2025/VINS-and-ESVO2/"}},{id:"post-dino-\u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",title:"DINO \u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",description:"\u5982\u4f55\u4f7f\u7528DINO, DPT, RoPE",section:"Posts",handler:()=>{window.location.href="/blog/2025/dino/"}},{id:"post-3d-\u4efb\u52a1-sfm-mvs-nvs-vo-vio-slam",title:"3D \u4efb\u52a1\uff1aSFM, MVS, NVS, VO, VIO, SLAM",description:"\u591a\u89c6\u89d2\u51e0\u4f55\uff0c\u65b0\u89c6\u89d2\u751f\u6210",section:"Posts",handler:()=>{window.location.href="/blog/2025/3dtasks/"}},{id:"post-matplotlib\u8f93\u51fa\u4e2d\u6587",title:"Matplotlib\u8f93\u51fa\u4e2d\u6587",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"}},{id:"post-mamba",title:"Mamba",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/mamba/"}},{id:"post-flow",title:"Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/flow/"}},{id:"post-diffusion",title:"Diffusion",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-nerf-and-gaussian-splatting",title:"NeRF and Gaussian Splatting",description:"Gaussian Splatting, NeRF, Alpha-blending, Point-Based Rendering, Jacobian",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussianSplatting/"}},{id:"post-colmap",title:"COLMAP",description:"SFM",section:"Posts",handler:()=>{window.location.href="/blog/2025/sfm-mvs-rendering/"}},{id:"post-\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",title:"\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/impulse/"}},{id:"post-\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",title:"\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/math/"}},{id:"post-fisheye",title:"Fisheye",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/fisheye/"}},{id:"post-montecarlo",title:"Montecarlo",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/MonteCarlo/"}},{id:"post-\u5c0f\u6ce2\u5206\u6790",title:"\u5c0f\u6ce2\u5206\u6790",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/"}},{id:"post-lifeifei",title:"Lifeifei",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Lifeifei/"}},{id:"post-orgmode",title:"Orgmode",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/orgmode/"}},{id:"post-python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",title:"Python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Python%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%88%96%E6%A8%A1%E5%9D%97/"}},{id:"post-proximal",title:"Proximal",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/proximal/"}},{id:"post-visualtransformer",title:"Visualtransformer",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/VisualTransformer/"}},{id:"post-pytorch-note",title:"Pytorch Note",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Pytorch-note/"}},{id:"post-android-opencv-ndk",title:"Android Opencv Ndk",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/android-opencv-ndk/"}},{id:"post-repvgg",title:"Repvgg",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/RepVGG/"}},{id:"post-popularlibrary",title:"Popularlibrary",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/popularLibrary/"}},{id:"post-\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",title:"\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%85%88%E8%BF%9B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}},{id:"post-one-stage-detection",title:"One Stage Detection",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/one-stage-detection/"}},{id:"post-polarization",title:"Polarization",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/polarization/"}},{id:"post-vae-vqvae-vagan",title:"Vae Vqvae Vagan",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/vae-vqvae-vagan/"}},{id:"post-multiple-object-tracking",title:"Multiple Object Tracking",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Multiple-Object-Tracking/"}},{id:"post-\u77e9\u9635",title:"\u77e9\u9635",description:"Matrix",section:"Posts",handler:()=>{window.location.href="/blog/2024/Matrix/"}},{id:"post-typora-and-jekyll",title:"Typora and Jekyll",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/images/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%65%6B%78%6D@%71%71.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hWo1RTsAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
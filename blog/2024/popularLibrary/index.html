<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Popularlibrary | è®¡ç®—æœºè§†è§‰ </title> <meta name="author" content="Erkang Chen"> <meta name="description" content="ç®€è¦ä»‹ç»"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cekxm.github.io/blog/2024/popularLibrary/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> è®¡ç®—æœºè§†è§‰ </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Popularlibrary</h1> <p class="post-meta"> Created in December 29, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="æ·±åº¦å­¦ä¹ èµ„æ–™">æ·±åº¦å­¦ä¹ èµ„æ–™</h2> <h3 id="èµ„æºåº“">èµ„æºåº“</h3> <h4 id="github">github</h4> <h4 id="paperwithcode"><a href="https://paperswithcode.com/" rel="external nofollow noopener" target="_blank">paperwithcode</a></h4> <h4 id="huggingface"><a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">Huggingface</a></h4> <h3 id="ç»¼åˆå‹åº“">ç»¼åˆå‹åº“</h3> <h4 id="openmmlab"><a href="https://github.com/open-mmlab" rel="external nofollow noopener" target="_blank">OpenMMLab</a></h4> <p>openmmlab å¾ˆå¤šå·¥å…·å¼€ç®±å³å¯ç”¨ã€‚</p> <p>OpenMMLab ä¸ºé¦™æ¸¯ä¸­æ–‡å¤§å­¦-å•†æ±¤ç§‘æŠ€è”åˆå®éªŒå®¤ MMLab å¼€æºçš„ç®—æ³•å¹³å°ï¼Œä¸åˆ°ä¸¤å¹´æ—¶é—´ï¼Œå·²ç»åŒ…å«ä¼—å¤š SOTA è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚</p> <p>OpenMMLab åœ¨Githubä¸Šä¸æ˜¯ä¸€ä¸ªå•ç‹¬é¡¹ç›®ï¼Œé™¤äº†å¤§å®¶æ‰€ç†ŸçŸ¥çš„ Github ä¸Šä¸‡ star ç›®æ ‡æ£€æµ‹åº“ MMDetectionï¼Œè¿˜æœ‰å…¶ä»–æ–¹å‘çš„ä»£ç åº“å’Œæ•°æ®é›†ï¼Œéå¸¸å€¼å¾—ä»äº‹è®¡ç®—æœºè§†è§‰ç ”å‘çš„æœ‹å‹å…³æ³¨ã€‚</p> <p>è¿‘æœŸ OpenMMLab è¿›è¡Œäº†å¯†é›†æ›´æ–°ï¼Œæ–°å¢äº†å¤šä¸ªåº“ï¼Œå®˜æ–¹ç§°æ¶‰åŠè¶…è¿‡ 10 ä¸ªç ”ç©¶æ–¹å‘ï¼Œå¼€æ”¾è¶…è¿‡ 100 ç§ç®—æ³•å’Œ 600 ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œç›®å‰Githubæ€»æ˜Ÿæ ‡è¶…è¿‡ 1.7 ä¸‡ã€‚æ˜¯CVæ–¹å‘ç³»ç»Ÿæ€§è¾ƒå¼ºã€ç¤¾åŒºæ´»è·ƒçš„å¼€æºå¹³å°ã€‚</p> <p>è¿™äº›åº“å¤§éƒ¨åˆ†éƒ½åŸºäºæ·±åº¦å­¦ä¹  PyTorch æ¡†æ¶ï¼Œç®—æ³•ç´§è·Ÿå‰æ²¿ï¼Œæ–¹ä¾¿æ˜“ç”¨ï¼Œæ–‡æ¡£è¾ƒä¸ºä¸°å¯Œï¼Œæ— è®ºå¯¹äºç ”ç©¶è¿˜æ˜¯å·¥ç¨‹å¼€å‘çš„æœ‹å‹éƒ½å¾ˆå€¼å¾—äº†è§£ã€‚</p> <p><a href="https://zhuanlan.zhihu.com/p/159562429" rel="external nofollow noopener" target="_blank">åŒ…å«å¾ˆå¤šåº“</a></p> <ul> <li> <a href="https://github.com/open-mmlab/mmcv" rel="external nofollow noopener" target="_blank">MMCV</a>: OpenMMLab foundational library for computer vision.</li> <li> <a href="https://github.com/open-mmlab/mim" rel="external nofollow noopener" target="_blank">MIM</a>: MIM installs OpenMMLab packages.</li> <li> <a href="https://github.com/open-mmlab/mmclassification" rel="external nofollow noopener" target="_blank">MMClassification</a>: OpenMMLab image classification toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmdetection" rel="external nofollow noopener" target="_blank">MMDetection</a>: OpenMMLab detection toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmdetection3d" rel="external nofollow noopener" target="_blank">MMDetection3D</a>: OpenMMLabâ€™s next-generation platform for general 3D object detection.</li> <li> <a href="https://github.com/open-mmlab/mmrotate" rel="external nofollow noopener" target="_blank">MMRotate</a>: OpenMMLab rotated object detection toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmsegmentation" rel="external nofollow noopener" target="_blank">MMSegmentation</a>: OpenMMLab semantic segmentation toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmocr" rel="external nofollow noopener" target="_blank">MMOCR</a>: OpenMMLab text detection, recognition and understanding toolbox.</li> <li> <a href="https://github.com/open-mmlab/mmpose" rel="external nofollow noopener" target="_blank">MMPose</a>: OpenMMLab pose estimation toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmhuman3d" rel="external nofollow noopener" target="_blank">MMHuman3D</a>: OpenMMLab 3D human parametric model toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmselfsup" rel="external nofollow noopener" target="_blank">MMSelfSup</a>: OpenMMLab self-supervised learning Toolbox and Benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmrazor" rel="external nofollow noopener" target="_blank">MMRazor</a>: OpenMMLab Model Compression Toolbox and Benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmfewshot" rel="external nofollow noopener" target="_blank">MMFewShot</a>: OpenMMLab FewShot Learning Toolbox and Benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmaction2" rel="external nofollow noopener" target="_blank">MMAction2</a>: OpenMMLabâ€™s next-generation action understanding toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmtracking" rel="external nofollow noopener" target="_blank">MMTracking</a>: OpenMMLab video perception toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmflow" rel="external nofollow noopener" target="_blank">MMFlow</a>: OpenMMLab optical flow toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmediting" rel="external nofollow noopener" target="_blank">MMEditing</a>: OpenMMLab image and video editing toolbox.</li> <li> <a href="https://github.com/open-mmlab/mmgeneration" rel="external nofollow noopener" target="_blank">MMGeneration</a>: OpenMMLab Generative Model toolbox and benchmark.</li> <li> <a href="https://github.com/open-mmlab/mmdeploy" rel="external nofollow noopener" target="_blank">MMDeploy</a>: OpenMMlab deep learning model deployment toolset.</li> </ul> <h4 id="timm"><a href="https://github.com/rwightman/pytorch-image-models" rel="external nofollow noopener" target="_blank">timm</a></h4> <ul> <li>ä»¥åˆ†ç±»ä¸ºä»»åŠ¡ï¼ŒåŒ…å«å¤šç§åˆ†ç±»æ¨¡å‹</li> <li>å›¾åƒé¢„å¤„ç†</li> <li>optimizer</li> <li>scheduler</li> </ul> <h4 id="torchmetrics"><a href="https://torchmetrics.readthedocs.io/en/stable/" rel="external nofollow noopener" target="_blank">Torchmetrics</a></h4> <p>æŒ‡æ ‡ï¼Œä½†èƒ½ä½œä¸º Loss å—ï¼Ÿä¸çŸ¥é“ï¼Œè¿˜å¾—å†æ¢ç©¶ä¸€ä¸‹ã€‚</p> <h4 id="pytorch-accelerated"><a href="https://github.com/Chris-hughes10/pytorch-accelerated" rel="external nofollow noopener" target="_blank">pytorch-accelerated</a></h4> <ul> <li>å¤šGPUï¼Œå¤šè®¾å¤‡è®­ç»ƒ</li> <li>è®­ç»ƒæµç¨‹çš„ç®€åŒ–</li> </ul> <h4 id="transformers"><a href="https://huggingface.co/docs/transformers/index" rel="external nofollow noopener" target="_blank">transformers</a></h4> <p>Transformers provides APIs to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you time from training a model from scratch. The models can be used across different modalities such as:</p> <ul> <li>ğŸ“ Text: text classification, information extraction, question answering, summarization, translation, and text generation in over 100 languages.</li> <li>ğŸ–¼ï¸ Images: image classification, object detection, and segmentation.</li> <li>ğŸ—£ï¸ Audio: speech recognition and audio classification.</li> <li>ğŸ™ Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.</li> </ul> <p>å¯ä»¥çœ‹åˆ°å›¾åƒæ–¹é¢æ˜¯å›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²</p> <h3 id="è®­ç»ƒæ¡†æ¶å’ŒåŠ é€Ÿ">è®­ç»ƒæ¡†æ¶å’ŒåŠ é€Ÿ</h3> <h4 id="fastai"><a href="https://github.com/fastai/fastai" rel="external nofollow noopener" target="_blank">fastai</a></h4> <p>è¿™ä¸ªæ˜¯è®­ç»ƒçš„æ¡†æ¶åº“ï¼Œæœ‰ç‚¹ç±»ä¼¼ pyotrch-accelerated. å¯èƒ½æ›´é€šç”¨</p> <p>fastai includes:</p> <ul> <li>A new type dispatch system for Python along with a semantic type hierarchy for tensors</li> <li>A GPU-optimized computer vision library which can be extended in pure Python</li> <li>An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4â€“5 lines of code</li> <li>A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training</li> <li>A new data block API</li> <li>And much moreâ€¦</li> </ul> <h4 id="pytorch-lightning"><a href="https://github.com/PyTorchLightning/pytorch-lightning" rel="external nofollow noopener" target="_blank">pytorch-lightning</a></h4> <p>Lightning forces the following structure to your code which makes it reusable and shareable:</p> <ul> <li>Research code (the LightningModule).</li> <li>Engineering code (you delete, and is handled by the Trainer).</li> <li>Non-essential research code (logging, etcâ€¦ this goes in Callbacks).</li> <li>Data (use PyTorch DataLoaders or organize them into a LightningDataModule).</li> </ul> <p>Once you do this, you can train on multiple-GPUs, TPUs, CPUs, IPUs, HPUs and even in 16-bit precision without changing your code!</p> <h3 id="metric-learning">Metric Learning</h3> <h4 id="pytorch-metric-learning"><a href="https://github.com/KevinMusgrave/pytorch-metric-learning" rel="external nofollow noopener" target="_blank">pytorch-metric-learning</a></h4> <h5 id="miner-loss-reducer-regularizer-çš„å…³ç³»">Miner, loss, reducer, regularizer çš„å…³ç³»</h5> <p><img src="/images/2024-12-30-popularLibrary/high_level_loss_function_overview.png" alt="high_level_loss_function_overview" class="img-fluid"></p> <p>The miner finds positive and negative pairs that it thinks are particularly difficult.</p> <p>Loss functions can be customized using distances, reducers, and regularizers. In the diagram below, a miner finds the indices of hard pairs within a batch. These are used to index into the distance matrix, computed by the distance object. For this diagram, the loss function is pair-based, so it computes a loss per pair. In addition, a regularizer has been supplied, so a regularization loss is computed for each embedding in the batch. The per-pair and per-element losses are passed to the reducer, which (in this diagram) only keeps losses with a high value. The averages are computed for the high-valued pair and element losses, and are then added together to obtain the final loss.</p> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distances" rel="external nofollow noopener" target="_blank">Distances</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distances/#cosinesimilarity" rel="external nofollow noopener" target="_blank"><strong>CosineSimilarity</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distances/#dotproductsimilarity" rel="external nofollow noopener" target="_blank"><strong>DotProductSimilarity</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distances/#lpdistance" rel="external nofollow noopener" target="_blank"><strong>LpDistance</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distances/#snrdistance" rel="external nofollow noopener" target="_blank"><strong>SNRDistance</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a></td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses" rel="external nofollow noopener" target="_blank">Losses</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#angularloss" rel="external nofollow noopener" target="_blank"><strong>AngularLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1708.01682.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning with Angular Loss</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#arcfaceloss" rel="external nofollow noopener" target="_blank"><strong>ArcFaceLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1801.07698.pdf" rel="external nofollow noopener" target="_blank">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#centroidtripletloss" rel="external nofollow noopener" target="_blank"><strong>CentroidTripletLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/2104.13643.pdf" rel="external nofollow noopener" target="_blank">On the Unreasonable Effectiveness of Centroids in Image Retrieval</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#circleloss" rel="external nofollow noopener" target="_blank"><strong>CircleLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/2002.10857.pdf" rel="external nofollow noopener" target="_blank">Circle Loss: A Unified Perspective of Pair Similarity Optimization</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#contrastiveloss" rel="external nofollow noopener" target="_blank"><strong>ContrastiveLoss</strong></a></td> <td><a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" rel="external nofollow noopener" target="_blank">Dimensionality Reduction by Learning an Invariant Mapping</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#cosfaceloss" rel="external nofollow noopener" target="_blank"><strong>CosFaceLoss</strong></a></td> <td>- <a href="https://arxiv.org/pdf/1801.09414.pdf" rel="external nofollow noopener" target="_blank">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a> - <a href="https://arxiv.org/pdf/1801.05599.pdf" rel="external nofollow noopener" target="_blank">Additive Margin Softmax for Face Verification</a> </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#fastaploss" rel="external nofollow noopener" target="_blank"><strong>FastAPLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning to Rank</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#generalizedliftedstructureloss" rel="external nofollow noopener" target="_blank"><strong>GeneralizedLiftedStructureLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1703.07737.pdf" rel="external nofollow noopener" target="_blank">In Defense of the Triplet Loss for Person Re-Identification</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#intrapairvarianceloss" rel="external nofollow noopener" target="_blank"><strong>IntraPairVarianceLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#largemarginsoftmaxloss" rel="external nofollow noopener" target="_blank"><strong>LargeMarginSoftmaxLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1612.02295.pdf" rel="external nofollow noopener" target="_blank">Large-Margin Softmax Loss for Convolutional Neural Networks</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#liftedstructureloss" rel="external nofollow noopener" target="_blank"><strong>LiftedStructreLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1511.06452.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning via Lifted Structured Feature Embedding</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#marginloss" rel="external nofollow noopener" target="_blank"><strong>MarginLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1706.07567.pdf" rel="external nofollow noopener" target="_blank">Sampling Matters in Deep Embedding Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#multisimilarityloss" rel="external nofollow noopener" target="_blank"><strong>MultiSimilarityLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ncaloss" rel="external nofollow noopener" target="_blank"><strong>NCALoss</strong></a></td> <td><a href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf" rel="external nofollow noopener" target="_blank">Neighbourhood Components Analysis</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#normalizedsoftmaxloss" rel="external nofollow noopener" target="_blank"><strong>NormalizedSoftmaxLoss</strong></a></td> <td>- <a href="https://arxiv.org/pdf/1704.06369.pdf" rel="external nofollow noopener" target="_blank">NormFace: L2 Hypersphere Embedding for Face Verification</a> - <a href="https://arxiv.org/pdf/1811.12649.pdf" rel="external nofollow noopener" target="_blank">Classification is a Strong Baseline for DeepMetric Learning</a> </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#npairsloss" rel="external nofollow noopener" target="_blank"><strong>NPairsLoss</strong></a></td> <td><a href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf" rel="external nofollow noopener" target="_blank">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ntxentloss" rel="external nofollow noopener" target="_blank"><strong>NTXentLoss</strong></a></td> <td>- <a href="https://arxiv.org/pdf/1807.03748.pdf" rel="external nofollow noopener" target="_blank">Representation Learning with Contrastive Predictive Coding</a> - <a href="https://arxiv.org/pdf/1911.05722.pdf" rel="external nofollow noopener" target="_blank">Momentum Contrast for Unsupervised Visual Representation Learning</a> - <a href="https://arxiv.org/abs/2002.05709" rel="external nofollow noopener" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a> </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#proxyanchorloss" rel="external nofollow noopener" target="_blank"><strong>ProxyAnchorLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/2003.13911.pdf" rel="external nofollow noopener" target="_blank">Proxy Anchor Loss for Deep Metric Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#proxyncaloss" rel="external nofollow noopener" target="_blank"><strong>ProxyNCALoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1703.07464.pdf" rel="external nofollow noopener" target="_blank">No Fuss Distance Metric Learning using Proxies</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#signaltonoiseratiocontrastiveloss" rel="external nofollow noopener" target="_blank"><strong>SignalToNoiseRatioContrastiveLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#softtripleloss" rel="external nofollow noopener" target="_blank"><strong>SoftTripleLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf" rel="external nofollow noopener" target="_blank">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#spherefaceloss" rel="external nofollow noopener" target="_blank"><strong>SphereFaceLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/1704.08063.pdf" rel="external nofollow noopener" target="_blank">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#subcenterarcfaceloss" rel="external nofollow noopener" target="_blank"><strong>SubCenterArcFaceLoss</strong></a></td> <td><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560715.pdf" rel="external nofollow noopener" target="_blank">Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#supconloss" rel="external nofollow noopener" target="_blank"><strong>SupConLoss</strong></a></td> <td><a href="https://arxiv.org/abs/2004.11362" rel="external nofollow noopener" target="_blank">Supervised Contrastive Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#tripletmarginloss" rel="external nofollow noopener" target="_blank"><strong>TripletMarginLoss</strong></a></td> <td><a href="https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf" rel="external nofollow noopener" target="_blank">Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#tupletmarginloss" rel="external nofollow noopener" target="_blank"><strong>TupletMarginLoss</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning with Tuplet Margin Loss</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#vicregloss" rel="external nofollow noopener" target="_blank"><strong>VICRegLoss</strong></a></td> <td><a href="https://arxiv.org/pdf/2105.04906.pdf" rel="external nofollow noopener" target="_blank">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a></td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners" rel="external nofollow noopener" target="_blank">Miners</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#angularminer" rel="external nofollow noopener" target="_blank"><strong>AngularMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#batcheasyhardminer" rel="external nofollow noopener" target="_blank"><strong>BatchEasyHardMiner</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Xuan_Improved_Embeddings_with_Easy_Positive_Triplet_Mining_WACV_2020_paper.pdf" rel="external nofollow noopener" target="_blank">Improved Embeddings with Easy Positive Triplet Mining</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#batchhardminer" rel="external nofollow noopener" target="_blank"><strong>BatchHardMiner</strong></a></td> <td><a href="https://arxiv.org/pdf/1703.07737.pdf" rel="external nofollow noopener" target="_blank">In Defense of the Triplet Loss for Person Re-Identification</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#distanceweightedminer" rel="external nofollow noopener" target="_blank"><strong>DistanceWeightedMiner</strong></a></td> <td><a href="https://arxiv.org/pdf/1706.07567.pdf" rel="external nofollow noopener" target="_blank">Sampling Matters in Deep Embedding Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#embeddingsalreadypackagedastriplets" rel="external nofollow noopener" target="_blank"><strong>EmbeddingsAlreadyPackagedAsTriplets</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#hdcminer" rel="external nofollow noopener" target="_blank"><strong>HDCMiner</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf" rel="external nofollow noopener" target="_blank">Hard-Aware Deeply Cascaded Embedding</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#maximumlossminer" rel="external nofollow noopener" target="_blank"><strong>MaximumLossMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#multisimilarityminer" rel="external nofollow noopener" target="_blank"><strong>MultiSimilarityMiner</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#pairmarginminer" rel="external nofollow noopener" target="_blank"><strong>PairMarginMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#tripletmarginminer" rel="external nofollow noopener" target="_blank"><strong>TripletMarginMiner</strong></a></td> <td><a href="https://arxiv.org/pdf/1503.03832.pdf" rel="external nofollow noopener" target="_blank">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#uniformhistogramminer" rel="external nofollow noopener" target="_blank"><strong>UniformHistogramMiner</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers" rel="external nofollow noopener" target="_blank">Reducers</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#avgnonzeroreducer" rel="external nofollow noopener" target="_blank"><strong>AvgNonZeroReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#classweightedreducer" rel="external nofollow noopener" target="_blank"><strong>ClassWeightedReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#divisorreducer" rel="external nofollow noopener" target="_blank"><strong>DivisorReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#donothingreducer" rel="external nofollow noopener" target="_blank"><strong>DoNothingReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#meanreducer" rel="external nofollow noopener" target="_blank"><strong>MeanReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#peranchorreducer" rel="external nofollow noopener" target="_blank"><strong>PerAnchorReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#thresholdreducer" rel="external nofollow noopener" target="_blank"><strong>ThresholdReducer</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers" rel="external nofollow noopener" target="_blank">Regularizers</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#centerinvariantregularizer" rel="external nofollow noopener" target="_blank"><strong>CenterInvariantRegularizer</strong></a></td> <td><a href="http://www1.ece.neu.edu/~yuewu/files/2017/twu024.pdf" rel="external nofollow noopener" target="_blank">Deep Face Recognition with Center Invariant Loss</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#lpregularizer" rel="external nofollow noopener" target="_blank"><strong>LpRegularizer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#regularfaceregularizer" rel="external nofollow noopener" target="_blank"><strong>RegularFaceRegularizer</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_RegularFace_Deep_Face_Recognition_via_Exclusive_Regularization_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">RegularFace: Deep Face Recognition via Exclusive Regularization</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#sparsecentersregularizer" rel="external nofollow noopener" target="_blank"><strong>SparseCentersRegularizer</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf" rel="external nofollow noopener" target="_blank">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#zeromeanregularizer" rel="external nofollow noopener" target="_blank"><strong>ZeroMeanRegularizer</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</a></td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/samplers" rel="external nofollow noopener" target="_blank">Samplers</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/samplers/#mperclasssampler" rel="external nofollow noopener" target="_blank"><strong>MPerClassSampler</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/samplers/#hierarchicalsampler" rel="external nofollow noopener" target="_blank"><strong>HierarchicalSampler</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Metric Learning to Rank</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/samplers/#tuplestoweightssampler" rel="external nofollow noopener" target="_blank"><strong>TuplesToWeightsSampler</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/samplers/#fixedsetoftriplets" rel="external nofollow noopener" target="_blank"><strong>FixedSetOfTriplets</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers" rel="external nofollow noopener" target="_blank">Trainers</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#metriclossonly" rel="external nofollow noopener" target="_blank"><strong>MetricLossOnly</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#trainwithclassifier" rel="external nofollow noopener" target="_blank"><strong>TrainWithClassifier</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#cascadedembeddings" rel="external nofollow noopener" target="_blank"><strong>CascadedEmbeddings</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf" rel="external nofollow noopener" target="_blank">Hard-Aware Deeply Cascaded Embedding</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#deepadversarialmetriclearning" rel="external nofollow noopener" target="_blank"><strong>DeepAdversarialMetricLearning</strong></a></td> <td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Adversarial Metric Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#unsupervisedembeddingsusingaugmentations" rel="external nofollow noopener" target="_blank"><strong>UnsupervisedEmbeddingsUsingAugmentations</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#twostreammetricloss" rel="external nofollow noopener" target="_blank"><strong>TwoStreamMetricLoss</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/testers" rel="external nofollow noopener" target="_blank">Testers</a></p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/testers/#globalembeddingspacetester" rel="external nofollow noopener" target="_blank"><strong>GlobalEmbeddingSpaceTester</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/testers/#withsameparentlabeltester" rel="external nofollow noopener" target="_blank"><strong>WithSameParentLabelTester</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/testers/#globaltwostreamembeddingspacetester" rel="external nofollow noopener" target="_blank"><strong>GlobalTwoStreamEmbeddingSpaceTester</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p>Utils</p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/accuracy_calculation" rel="external nofollow noopener" target="_blank"><strong>AccuracyCalculator</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/logging_presets" rel="external nofollow noopener" target="_blank"><strong>HookContainer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/inference_models" rel="external nofollow noopener" target="_blank"><strong>InferenceModel</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/common_functions/#torchinitwrapper" rel="external nofollow noopener" target="_blank"><strong>TorchInitWrapper</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distributed/#distributedlosswrapper" rel="external nofollow noopener" target="_blank"><strong>DistributedLossWrapper</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distributed/#distributedminerwrapper" rel="external nofollow noopener" target="_blank"><strong>DistributedMinerWrapper</strong></a></td> <td>Â </td> </tr> </tbody> </table> <p>Base Classes, Mixins, and Wrappers</p> <table> <thead> <tr> <th>Name</th> <th>Reference Papers</th> </tr> </thead> <tbody> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#crossbatchmemory" rel="external nofollow noopener" target="_blank"><strong>CrossBatchMemory</strong></a></td> <td><a href="https://arxiv.org/pdf/1912.06798.pdf" rel="external nofollow noopener" target="_blank">Cross-Batch Memory for Embedding Learning</a></td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#genericpairloss" rel="external nofollow noopener" target="_blank"><strong>GenericPairLoss</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#multiplelosses" rel="external nofollow noopener" target="_blank"><strong>MultipleLosses</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#multiplereducers" rel="external nofollow noopener" target="_blank"><strong>MultipleReducers</strong></a></td> <td>Â </td> </tr> <tr> <td><strong>EmbeddingRegularizerMixin</strong></td> <td>Â </td> </tr> <tr> <td><strong>WeightMixin</strong></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#weightregularizermixin" rel="external nofollow noopener" target="_blank"><strong>WeightRegularizerMixin</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/distance/#basedistance" rel="external nofollow noopener" target="_blank"><strong>BaseDistance</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#basemetriclossfunction" rel="external nofollow noopener" target="_blank"><strong>BaseMetricLossFunction</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#baseminer" rel="external nofollow noopener" target="_blank"><strong>BaseMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#basetupleminer" rel="external nofollow noopener" target="_blank"><strong>BaseTupleMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/#basesubsetbatchminer" rel="external nofollow noopener" target="_blank"><strong>BaseSubsetBatchMiner</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/reducers/#basereducer" rel="external nofollow noopener" target="_blank"><strong>BaseReducer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/regularizers/#baseregularizer" rel="external nofollow noopener" target="_blank"><strong>BaseRegularizer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/#basetrainer" rel="external nofollow noopener" target="_blank"><strong>BaseTrainer</strong></a></td> <td>Â </td> </tr> <tr> <td><a href="https://kevinmusgrave.github.io/pytorch-metric-learning/testers/#basetester" rel="external nofollow noopener" target="_blank"><strong>BaseTester</strong></a></td> <td>Â </td> </tr> </tbody> </table> <h3 id="self-supervised-learning-and-contrastive-learning"> <strong>S</strong>elf-<strong>S</strong>upervised <strong>L</strong>earning and Contrastive learning</h3> <h4 id="vissl"><a href="https://github.com/facebookresearch/vissl" rel="external nofollow noopener" target="_blank">VISSL</a></h4> <p>VISSL is a computer <strong>VI</strong>sion library for state-of-the-art <strong>S</strong>elf-<strong>S</strong>upervised <strong>L</strong>earning research with <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a>. VISSL aims to accelerate research cycle in self-supervised learning: from designing a new self-supervised task to evaluating the learned representations. Key features include:</p> <ul> <li> <strong>Reproducible implementation of SOTA in Self-Supervision</strong>: All existing SOTA in Self-Supervision are implemented - <a href="https://arxiv.org/abs/2006.09882" rel="external nofollow noopener" target="_blank">SwAV</a>, <a href="https://arxiv.org/abs/2002.05709" rel="external nofollow noopener" target="_blank">SimCLR</a>, <a href="https://arxiv.org/abs/1911.05722" rel="external nofollow noopener" target="_blank">MoCo(v2)</a>, <a href="https://arxiv.org/abs/1912.01991" rel="external nofollow noopener" target="_blank">PIRL</a>, <a href="https://arxiv.org/pdf/1805.01978.pdf" rel="external nofollow noopener" target="_blank">NPID</a>, <a href="https://arxiv.org/abs/1912.01991" rel="external nofollow noopener" target="_blank">NPID++</a>, <a href="https://arxiv.org/abs/2006.09882" rel="external nofollow noopener" target="_blank">DeepClusterV2</a>, <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf" rel="external nofollow noopener" target="_blank">ClusterFit</a>, <a href="https://arxiv.org/abs/1803.07728" rel="external nofollow noopener" target="_blank">RotNet</a>, <a href="https://arxiv.org/abs/1603.09246" rel="external nofollow noopener" target="_blank">Jigsaw</a>. Also supports supervised trainings.</li> <li> <strong>Benchmark suite</strong>: Variety of benchmarks tasks including <a href="https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/linear_image_classification" rel="external nofollow noopener" target="_blank">linear image classification (places205, imagenet1k, voc07, food, CLEVR, dsprites, UCF101, stanford cars and many more)</a>, <a href="https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/fulltune" rel="external nofollow noopener" target="_blank">full finetuning</a>, <a href="https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/semi_supervised" rel="external nofollow noopener" target="_blank">semi-supervised benchmark</a>, <a href="https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/nearest_neighbor" rel="external nofollow noopener" target="_blank">nearest neighbor benchmark</a>, <a href="https://github.com/facebookresearch/vissl/tree/main/configs/config/benchmark/object_detection" rel="external nofollow noopener" target="_blank">object detection (Pascal VOC and COCO)</a>.</li> <li> <strong>Ease of Usability</strong>: easy to use using yaml configuration system based on <a href="https://github.com/facebookresearch/hydra" rel="external nofollow noopener" target="_blank">Hydra</a>.</li> <li> <strong>Modular</strong>: Easy to design new tasks and reuse the existing components from other tasks (objective functions, model trunk and heads, data transforms, etc.). The modular components are simple <em>drop-in replacements</em> in yaml config files.</li> <li> <strong>Scalability</strong>: Easy to train model on 1-gpu, multi-gpu and multi-node. Several components for large scale trainings provided as simple config file plugs: <a href="https://pytorch.org/docs/stable/checkpoint.html" rel="external nofollow noopener" target="_blank">Activation checkpointing</a>, <a href="https://arxiv.org/abs/1910.02054" rel="external nofollow noopener" target="_blank">ZeRO</a>, <a href="https://nvidia.github.io/apex/amp.html#o1-mixed-precision-recommended-for-typical-use" rel="external nofollow noopener" target="_blank">FP16</a>, <a href="https://arxiv.org/abs/1708.03888" rel="external nofollow noopener" target="_blank">LARC</a>, Stateful data sampler, data class to handle invalid images, large model backbones like <a href="https://arxiv.org/abs/2003.13678" rel="external nofollow noopener" target="_blank">RegNets</a>, etc.</li> <li> <strong>Model Zoo</strong>: Over <em>60 pre-trained self-supervised model</em> weights.</li> </ul> <h3 id="æ•°æ®å¢å¼º">æ•°æ®å¢å¼º</h3> <h4 id="timm-1">timm</h4> <h4 id="albumentations"><a href="https://github.com/albumentations-team/albumentations" rel="external nofollow noopener" target="_blank">Albumentations</a></h4> <p>ä¸»è¦æ˜¯é¢œè‰²ã€å¯¹æ¯”åº¦ã€æ¨¡ç³Šç­‰æ–¹é¢çš„</p> <p>Albumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.</p> <h3 id="å›¾åƒå˜æ¢æ¨¡å—">å›¾åƒå˜æ¢æ¨¡å—</h3> <h4 id="kornia"><a href="https://github.com/kornia/kornia" rel="external nofollow noopener" target="_blank">kornia</a></h4> <p>è¿™ä¸ªå’Œæ•°æ®å¢å¼ºä¸åŒï¼Œå®ƒæ˜¯å¯ä»¥æ”¾åœ¨æ¨¡å‹ä¸­çš„ã€‚</p> <p>Inspired by existing packages, this library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors.</p> <p>At a granular level, Kornia is a library that consists of the following components:</p> <table> <thead> <tr> <th><strong>Component</strong></th> <th><strong>Description</strong></th> </tr> </thead> <tbody> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/index.html" rel="external nofollow noopener" target="_blank">kornia</a></td> <td>a Differentiable Computer Vision library, with strong GPU support</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/augmentation.html" rel="external nofollow noopener" target="_blank">kornia.augmentation</a></td> <td>a module to perform data augmentation in the GPU</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/color.html" rel="external nofollow noopener" target="_blank">kornia.color</a></td> <td>a set of routines to perform color space conversions</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/contrib.html" rel="external nofollow noopener" target="_blank">kornia.contrib</a></td> <td>a compilation of user contrib and experimental operators</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/enhance.html" rel="external nofollow noopener" target="_blank">kornia.enhance</a></td> <td>a module to perform normalization and intensity transformation</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/feature.html" rel="external nofollow noopener" target="_blank">kornia.feature</a></td> <td>a module to perform feature detection</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/filters.html" rel="external nofollow noopener" target="_blank">kornia.filters</a></td> <td>a module to perform image filtering and edge detection</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/geometry.html" rel="external nofollow noopener" target="_blank">kornia.geometry</a></td> <td>a geometric computer vision library to perform image transformations, 3D linear algebra and conversions using different camera models</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/losses.html" rel="external nofollow noopener" target="_blank">kornia.losses</a></td> <td>a stack of loss functions to solve different vision tasks</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/morphology.html" rel="external nofollow noopener" target="_blank">kornia.morphology</a></td> <td>a module to perform morphological operations</td> </tr> <tr> <td><a href="https://kornia.readthedocs.io/en/latest/utils.html" rel="external nofollow noopener" target="_blank">kornia.utils</a></td> <td>image to tensor utilities and metrics for vision problems</td> </tr> </tbody> </table> <h3 id="ç‰©ä½“æ£€æµ‹">ç‰©ä½“æ£€æµ‹</h3> <h4 id="detectron2"><a href="https://github.com/facebookresearch/detectron2" rel="external nofollow noopener" target="_blank">Detectron2</a></h4> <p>Detectron2 is Facebook AI Researchâ€™s next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of Detectron and maskrcnn-benchmark. It supports a number of computer vision research projects and production applications in Facebook.</p> <h4 id="efficientdet"><a href="https://github.com/rwightman/efficientdet-pytorch" rel="external nofollow noopener" target="_blank">EfficientDet</a></h4> <p>EfficientDet: Scalable and Efficient Object Detection CVPR2020 çš„ä¸€ä¸ªå®ç°</p> <p>ä½œè€…æ˜¯ timm çš„ä½œè€…</p> <h3 id="è¯­ä¹‰åˆ†å‰²">è¯­ä¹‰åˆ†å‰²</h3> <h4 id="segementation-models"><a href="https://github.com/qubvel/segmentation_models.pytorch#architectures" rel="external nofollow noopener" target="_blank">Segementation Models</a></h4> <p>The main features of this library are:</p> <ul> <li>High level API (just two lines to create a neural network)</li> <li>9 models architectures for binary and multi class segmentation (including legendary Unet)</li> <li>113 available encoders (and 400+ encoders from <a href="https://github.com/rwightman/pytorch-image-models" rel="external nofollow noopener" target="_blank">timm</a>)</li> <li>All encoders have pre-trained weights for faster and better convergence</li> <li>Popular metrics and losses for training routines</li> <li>Architectures å¯ä»¥æ­é…ä¸åŒçš„ Encoders</li> </ul> <h5 id="architectures">Architectures</h5> <ul> <li>Unet [<a href="https://arxiv.org/abs/1505.04597" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#unet" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>Unet++ [<a href="https://arxiv.org/pdf/1807.10165.pdf" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#id2" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>MAnet [<a href="https://ieeexplore.ieee.org/abstract/document/9201310" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#manet" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>Linknet [<a href="https://arxiv.org/abs/1707.03718" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#linknet" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>FPN [<a href="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#fpn" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>PSPNet [<a href="https://arxiv.org/abs/1612.01105" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#pspnet" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>PAN [<a href="https://arxiv.org/abs/1805.10180" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#pan" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>DeepLabV3 [<a href="https://arxiv.org/abs/1706.05587" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#deeplabv3" rel="external nofollow noopener" target="_blank">docs</a>]</li> <li>DeepLabV3+ [<a href="https://arxiv.org/abs/1802.02611" rel="external nofollow noopener" target="_blank">paper</a>] [<a href="https://smp.readthedocs.io/en/latest/models.html#id9" rel="external nofollow noopener" target="_blank">docs</a>]</li> </ul> <h5 id="encoders">Encoders</h5> <p>è¿™è¾¹ä»–è¯´çš„ Encoders æ˜¯æŒ‡ä¸»å¹²æ¨¡å—ã€‚</p> <p>MMDetection</p> <h3 id="video-object-segmentation-vos">Video Object Segmentation (VOS)</h3> <p><a href="https://youtube-vos.org/" rel="external nofollow noopener" target="_blank">YouTube-VOS</a></p> <h3 id="è·Ÿè¸ª">è·Ÿè¸ª</h3> <ul> <li><strong>Pytracking</strong></li> <li><strong>Pysot</strong></li> <li>mmtracking</li> <li><strong>Trackit</strong></li> </ul> <p><strong>Pytracking</strong>æ–‡ä»¶æ¯”è¾ƒéš¾æ‡‚ï¼Œä½†æ˜¯å®¹æ˜“followï¼›</p> <p><strong>Pysot</strong> é›†æˆSiameseç³»åˆ—ç®—æ³•ï¼Œè¯¥å·¥å…·åŒ…åŠå¹´æœªæ›´æ–°ï¼Œæœªå¿…ä¸€å®šè¦åœ¨æ­¤åŸºç¡€å±•å¼€å·¥ä½œï¼Œ<strong>ä½†æ˜¯å¦‚æœå­¦ä¹ siameseä»£ç ï¼Œå¿…é¡»è¦çœ‹æ­¤ä»£ç å¹¶å­¦ä¹ ï¼›</strong></p> <p><strong>Trackit</strong> é›†æˆäº†TensorRT,å¯å¯¹æ¨¡å‹åŠ é€Ÿ<strong>ã€‚</strong></p> <h4 id="pysot"><a href="https://github.com/STVIR/pysot" rel="external nofollow noopener" target="_blank">pysot</a></h4> <p>The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking <em>research</em>. It is designed to be flexible in order to support rapid implementation and evaluation of novel research. PySOT includes implementations of the following visual tracking algorithms:</p> <ul> <li><a href="https://arxiv.org/abs/1812.05050" rel="external nofollow noopener" target="_blank">SiamMask</a></li> <li><a href="https://arxiv.org/abs/1812.11703" rel="external nofollow noopener" target="_blank">SiamRPN++</a></li> <li><a href="https://arxiv.org/abs/1808.06048" rel="external nofollow noopener" target="_blank">DaSiamRPN</a></li> <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html" rel="external nofollow noopener" target="_blank">SiamRPN</a></li> <li><a href="https://arxiv.org/abs/1606.09549" rel="external nofollow noopener" target="_blank">SiamFC</a></li> </ul> <p>using the following backbone network architectures:</p> <ul> <li><a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">ResNet{18, 34, 50}</a></li> <li><a href="https://arxiv.org/abs/1801.04381" rel="external nofollow noopener" target="_blank">MobileNetV2</a></li> <li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" rel="external nofollow noopener" target="_blank">AlexNet</a></li> </ul> <p>Additional backbone architectures may be easily implemented. For more details about these models, please see <a href="https://github.com/STVIR/pysot#references" rel="external nofollow noopener" target="_blank">References</a> below.</p> <p>Evaluation toolkit can support the following datasets:</p> <p>ğŸ“ <a href="http://faculty.ucmerced.edu/mhyang/papers/pami15_tracking_benchmark.pdf" rel="external nofollow noopener" target="_blank">OTB2015</a> ğŸ“ <a href="http://votchallenge.net/" rel="external nofollow noopener" target="_blank">VOT16/18/19</a> ğŸ“ <a href="http://votchallenge.net/vot2018/index.html" rel="external nofollow noopener" target="_blank">VOT18-LT</a> ğŸ“ <a href="https://arxiv.org/pdf/1809.07845.pdf" rel="external nofollow noopener" target="_blank">LaSOT</a> ğŸ“ <a href="https://arxiv.org/pdf/1804.00518.pdf" rel="external nofollow noopener" target="_blank">UAV123</a></p> <h5 id="æ•™ç¨‹"><a href="https://blog.csdn.net/weixin_42495721/article/details/110732592?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163695830416780255266434%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=163695830416780255266434&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-110732592.first_rank_v2_pc_rank_v29&amp;utm_term=pysot&amp;spm=1018.2226.3001.4187" rel="external nofollow noopener" target="_blank">æ•™ç¨‹</a></h5> <h4 id="pytracking"><a href="https://github.com/visionml/pytracking" rel="external nofollow noopener" target="_blank">pytracking</a></h4> <p>pytracking æ˜¯ç”±è‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢ Computer Vision Lab å‡ºå“ã€‚åŒ…å«è¯¥å®éªŒåœ¨è·Ÿè¸ªæ–¹é¢çš„æŒç»­çš„å·¥ä½œã€‚åŒæ—¶ä»–ä¹Ÿæ˜¯ä¸€ä¸ªè·Ÿè¸ªçš„æ¡†æ¶åº“ã€‚</p> <p>æ¯”å¦‚ <a href="https://github.com/researchmm/Stark" rel="external nofollow noopener" target="_blank">Stark</a> çš„å®ç°å°±ç”¨åˆ°äº†è¿™ä¸ªåº“ã€‚</p> <h4 id="mmtracking"><a href="https://github.com/open-mmlab/mmtracking" rel="external nofollow noopener" target="_blank">mmtracking</a></h4> <p>Supported methods of video object detection:</p> <ul> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/vid/dff" rel="external nofollow noopener" target="_blank">DFF</a> (CVPR 2017)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/vid/fgfa" rel="external nofollow noopener" target="_blank">FGFA</a> (ICCV 2017)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/vid/selsa" rel="external nofollow noopener" target="_blank">SELSA</a> (ICCV 2019)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/vid/temporal_roi_align" rel="external nofollow noopener" target="_blank">Temporal RoI Align</a> (AAAI 2021)</li> </ul> <p>Supported methods of multi object tracking:</p> <ul> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/mot/deepsort" rel="external nofollow noopener" target="_blank">SORT/DeepSORT</a> (ICIP 2016/2017)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/mot/tracktor" rel="external nofollow noopener" target="_blank">Tracktor</a> (ICCV 2019)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/mot/qdtrack" rel="external nofollow noopener" target="_blank">QDTrack</a> (CVPR 2021)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/mot/bytetrack" rel="external nofollow noopener" target="_blank">ByteTrack</a> (arXiv 2021)</li> </ul> <p>Supported methods of single object tracking:</p> <ul> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/sot/siamese_rpn" rel="external nofollow noopener" target="_blank">SiameseRPN++</a> (CVPR 2019)</li> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/sot/stark" rel="external nofollow noopener" target="_blank">STARK</a> (ICCV 2021)</li> </ul> <p>Supported methods of video instance segmentation:</p> <ul> <li> <a href="https://github.com/open-mmlab/mmtracking/blob/master/configs/vis/masktrack_rcnn" rel="external nofollow noopener" target="_blank">MaskTrack R-CNN</a> (ICCV 2019)</li> </ul> <h5 id="stark">Stark</h5> <h3 id="çŸ¥è¯†è’¸é¦">çŸ¥è¯†è’¸é¦</h3> <h4 id="torchdistill"><a href="https://github.com/yoshitomo-matsubara/torchdistill" rel="external nofollow noopener" target="_blank">torchdistill</a></h4> <p><strong><em>torchdistill</em></strong> (formerly <em>kdkit</em>) offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code. Even when you need to extract intermediate representations in teacher/student models, you will <strong>NOT</strong> need to reimplement the models, that often change the interface of the forward, but instead specify the module path(s) in the yaml file. Refer to <a href="https://github.com/yoshitomo-matsubara/torchdistill#citation" rel="external nofollow noopener" target="_blank">this paper</a> for more details.</p> <p>In addition to knowledge distillation, this framework helps you design and perform general deep learning experiments (<strong>WITHOUT coding</strong>) for reproducible deep learning studies. i.e., it enables you to train models without teachers simply by excluding teacher entries from a declarative yaml config file. You can find such examples below and in <a href="https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/sample/" rel="external nofollow noopener" target="_blank">configs/sample/</a>.</p> <h4 id="repdistiller"><a href="https://github.com/HobbitLong/RepDistiller" rel="external nofollow noopener" target="_blank">repdistiller</a></h4> <p><strong>(1) covers the implementation of the following ICLR 2020 paper:</strong></p> <p>â€œContrastive Representation Distillationâ€ (CRD). <a href="http://arxiv.org/abs/1910.10699" rel="external nofollow noopener" target="_blank">Paper</a>, <a href="http://hobbitlong.github.io/CRD/" rel="external nofollow noopener" target="_blank">Project Page</a>.</p> <p><img src="/images/2024-12-30-popularLibrary/image-20220418124936487.png" alt="image-20220418124936487" class="img-fluid"></p> <p><strong>(2) benchmarks 12 state-of-the-art knowledge distillation methods in PyTorch, including:</strong></p> <p>(KD) - Distilling the Knowledge in a Neural Network (FitNet) - Fitnets: hints for thin deep nets (AT) - Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer (SP) - Similarity-Preserving Knowledge Distillation (CC) - Correlation Congruence for Knowledge Distillation (VID) - Variational Information Distillation for Knowledge Transfer (RKD) - Relational Knowledge Distillation (PKT) - Probabilistic Knowledge Transfer for deep representation learning (AB) - Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons (FT) - Paraphrasing Complex Network: Network Compression via Factor Transfer (FSP) - A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning (NST) - Like what you like: knowledge distill via neuron selectivity transfer</p> <h3 id="è‡ªè’¸é¦-self-knowledge-distillation">è‡ªè’¸é¦ Self Knowledge Distillation</h3> <p><a href="https://github.com/winycg/Self-Knowledge-Distillation-Lib" rel="external nofollow noopener" target="_blank">Self-Knowledge-Distillation-Lib</a></p> <p>Comparison of Self-KD methods on ResNet-50</p> <table> <thead> <tr> <th>Method</th> <th>Venue</th> <th>Accuracy(%)</th> </tr> </thead> <tbody> <tr> <td>Cross-entropy</td> <td>-</td> <td>77.79</td> </tr> <tr> <td>DDGSD [1]</td> <td>AAAI-2019</td> <td>81.73</td> </tr> <tr> <td>DKS [2]</td> <td>CVPR-2019</td> <td>80.75</td> </tr> <tr> <td>SAD [3]</td> <td>ICCV-2019</td> <td>78.33</td> </tr> <tr> <td>BYOT [4]</td> <td>ICCV-2019</td> <td>79.76</td> </tr> <tr> <td>Tf-KD-reg [5]</td> <td>CVPR-2020</td> <td>79.84</td> </tr> <tr> <td>CS-KD [6]</td> <td>CVPR-2020</td> <td>79.99</td> </tr> <tr> <td>FRSKD [7]</td> <td>CVPR-2021</td> <td>80.51</td> </tr> </tbody> </table> <p>Reference</p> <p>[1] DDGSD: Data-Distortion Guided Self-Distillation for Deep Neural Networks. AAAI-2019</p> <p>[2] DKS: Deeply-supervised Knowledge Synergy. CVPR-2019.</p> <p>[3] SAD: Learning Lightweight Lane Detection CNNs by Self Attention Distillation. ICCV-2019.</p> <p>[4] BYOT: Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation. ICCV-2019</p> <p>[5] Tf-KD-reg: Revisiting Knowledge Distillation via Label Smoothing Regularization. CVPR-2020.</p> <p>[6] CS-KD: Regularizing Class-wise Predictions via Self-knowledge Distillation. CVPR-2020.</p> <p>[7] FRSKD: Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation. CVPR-2021.</p> <p>Comparison of advanced regularization methods Self-KD methods on ResNet-50</p> <table> <thead> <tr> <th>Method</th> <th>Venue</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Cross-entropy</td> <td>-</td> <td>77.79</td> </tr> <tr> <td>Label Smoothing [1]</td> <td>CVPR-2016</td> <td>80.33</td> </tr> <tr> <td>Virtual Softmax [2]</td> <td>NeurIPS-2018</td> <td>79.68</td> </tr> <tr> <td>Focal Loss [3]</td> <td>ICCV-2017</td> <td>79.31</td> </tr> <tr> <td>Maximum Entropy [4]</td> <td>ICLR Workshops 2017</td> <td>78.11</td> </tr> <tr> <td>Cutout [5]</td> <td>ArXiv.2017</td> <td>80.42</td> </tr> <tr> <td>Random Erase [6]</td> <td>AAAI-2020</td> <td>80.64</td> </tr> <tr> <td>Mixup [7]</td> <td>ICLR-2018</td> <td>81.39</td> </tr> <tr> <td>CutMix [8]</td> <td>ICCV-2019</td> <td>82.47</td> </tr> <tr> <td>AutoAugment [9]</td> <td>CVPR-2019</td> <td>81.41</td> </tr> </tbody> </table> <p>[1] Label Smoothing: Rethinking the inception architecture for computer vision. CVPR-2016.</p> <p>[2] Virtual Softmax: Virtual class enhanced discriminative embedding learning. NeurIPS-2018.</p> <p>[3] Focal Loss: Focal loss for dense object detection. ICCV-2017.</p> <p>[4] Maximum Entropy: Regularizing neural networks by penalizing confident output distributions. ICLR Workshops 2017.</p> <p>[5] Cutout: Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552</p> <p>[6] Random Erase: Random erasing data augmentation. AAAI-2020.</p> <p>[7] Mixup: mixup: Beyond empirical risk minimization. ICLR-2018.</p> <p>[8] CutMix: Cutmix: Regularization strategy to train strong classifiers with localizable features. ICCV-2019.</p> <p>[9] Autoaugment: Autoaugment: Learning augmentation strategies from data. CVPR-2019.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Teaching-Tailored-to-Talent/">Teaching Tailored To Talent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/VINS-and-ESVO2/">Vins And Esvo2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/dino/">DINO å¦‚ä½•ç”¨äºå¯†é›†é¢„æµ‹</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/3dtasks/">3D ä»»åŠ¡ï¼šSFM, MVS, NVS, VO, VIO, SLAM</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/">Matplotlibè¾“å‡ºä¸­æ–‡</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Erkang Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-teaching-tailored-to-talent",title:"Teaching Tailored To Talent",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2026/Teaching-Tailored-to-Talent/"}},{id:"post-vins-and-esvo2",title:"Vins And Esvo2",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/VINS-and-ESVO2/"}},{id:"post-dino-\u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",title:"DINO \u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",description:"\u5982\u4f55\u4f7f\u7528DINO, DPT, RoPE",section:"Posts",handler:()=>{window.location.href="/blog/2025/dino/"}},{id:"post-3d-\u4efb\u52a1-sfm-mvs-nvs-vo-vio-slam",title:"3D \u4efb\u52a1\uff1aSFM, MVS, NVS, VO, VIO, SLAM",description:"\u591a\u89c6\u89d2\u51e0\u4f55\uff0c\u65b0\u89c6\u89d2\u751f\u6210",section:"Posts",handler:()=>{window.location.href="/blog/2025/3dtasks/"}},{id:"post-matplotlib\u8f93\u51fa\u4e2d\u6587",title:"Matplotlib\u8f93\u51fa\u4e2d\u6587",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"}},{id:"post-mamba",title:"Mamba",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/mamba/"}},{id:"post-flow",title:"Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/flow/"}},{id:"post-diffusion",title:"Diffusion",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-gaussiansplatting",title:"Gaussiansplatting",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussianSplatting/"}},{id:"post-colmap",title:"COLMAP",description:"SFM",section:"Posts",handler:()=>{window.location.href="/blog/2025/sfm-mvs-rendering/"}},{id:"post-\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",title:"\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/impulse/"}},{id:"post-\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",title:"\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/math/"}},{id:"post-fisheye",title:"Fisheye",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/fisheye/"}},{id:"post-montecarlo",title:"Montecarlo",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/MonteCarlo/"}},{id:"post-\u5c0f\u6ce2\u5206\u6790",title:"\u5c0f\u6ce2\u5206\u6790",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/"}},{id:"post-lifeifei",title:"Lifeifei",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Lifeifei/"}},{id:"post-orgmode",title:"Orgmode",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/orgmode/"}},{id:"post-python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",title:"Python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Python%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%88%96%E6%A8%A1%E5%9D%97/"}},{id:"post-proximal",title:"Proximal",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/proximal/"}},{id:"post-visualtransformer",title:"Visualtransformer",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/VisualTransformer/"}},{id:"post-pytorch-note",title:"Pytorch Note",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Pytorch-note/"}},{id:"post-android-opencv-ndk",title:"Android Opencv Ndk",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/android-opencv-ndk/"}},{id:"post-repvgg",title:"Repvgg",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/RepVGG/"}},{id:"post-popularlibrary",title:"Popularlibrary",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/popularLibrary/"}},{id:"post-\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",title:"\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%85%88%E8%BF%9B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}},{id:"post-one-stage-detection",title:"One Stage Detection",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/one-stage-detection/"}},{id:"post-polarization",title:"Polarization",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/polarization/"}},{id:"post-vae-vqvae-vagan",title:"Vae Vqvae Vagan",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/vae-vqvae-vagan/"}},{id:"post-multiple-object-tracking",title:"Multiple Object Tracking",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Multiple-Object-Tracking/"}},{id:"post-\u77e9\u9635",title:"\u77e9\u9635",description:"Matrix",section:"Posts",handler:()=>{window.location.href="/blog/2024/Matrix/"}},{id:"post-typora-and-jekyll",title:"Typora and Jekyll",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/images/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%65%6B%78%6D@%71%71.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hWo1RTsAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diffusion | 计算机视觉 </title> <meta name="author" content="Erkang Chen"> <meta name="description" content="简要介绍"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cekxm.github.io/blog/2025/diffusion/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> 计算机视觉 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusion</h1> <p class="post-meta"> Created in December 25, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] 2022 Understanding Diffusion Models A Unified Perspective.pdf</p> <p>[2] What are Diffusion Models Lilian Weng.pdf</p> <p>[3] <a href="https://drscotthawley.github.io/blog/posts/FlowModels.html" rel="external nofollow noopener" target="_blank">blog - Flow With What You Know</a></p> <h3 id="toy-example">toy example</h3> <ul> <li><a href="https://github.com/varun-ml/diffusion-models-tutorial?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">GitHub - varun-ml/diffusion-models-tutorial: Experiment with diffusion models that you can run on your local jupyter instances</a></li> <li>Classifier-Free-Guidance (CFG) https://github.com/dome272/Diffusion-Models-pytorch</li> <li>Classifier-Free-Guidance (CFG) forked from 上面的链接，增加了一些功能和一个博客 <ul> <li>https://github.com/tcapelle/Diffusion-Models-pytorch</li> <li>https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch–VmlldzoyNzIzNTQ1</li> </ul> </li> <li><a href="https://github.com/dome272/Diffusion-Models-pytorch" rel="external nofollow noopener" target="_blank">GitHub - dome272/Diffusion-Models-pytorch: Pytorch implementation of Diffusion Models (https://arxiv.org/pdf/2006.11239.pdf)</a></li> <li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">GitHub - lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch</a></li> </ul> <h2 id="variational-diffusion-models">Variational Diffusion Models</h2> <h3 id="markovian-hierarchical-variational-autoencoder">Markovian Hierarchical Variational Autoencoder</h3> <p>按照 [1] 的思路，首先介绍 Markovian Hierarchical Variational Autoencoder (MHVA)</p> <p><img src="/images/2025-12-25-diffusion/image-20250826145311374.png" alt="image-20250826145311374" class="img-fluid"></p> <p>从左往右是编码，从右往左是解码。MHVA 规定，解码时，满足马尔可夫性，也就是生成 \(z_t\) 只依赖于 \(z_{t+1}\)。</p> <p>编码也是马尔可夫的，这个应该是默认的。因此有 \(p(x,z_{1:T})=p(z_T)p_\theta (x\mid z_1 )\prod_{t=2}^Tp_\theta (z_{t-1}\mid z_t) \tag{1}\)</p> \[q_\phi (z_{1:T}\mid x)=q_\phi(z_1\mid x)\prod_{t=2}^T q_\phi (z_{t}\mid z_{t-1}) \tag{2}\] <h3 id="variational-diffusion-models-可以认为是mhva再满足三个约束">Variational Diffusion Models 可以认为是MHVA再满足三个约束</h3> <ol> <li>隐藏变量和数据 x 维度一样</li> <li>编码器的结构不是学习得到的，而是设定为高斯</li> <li>设置编码器的高斯参数随时间而变化，使得T时刻 \(z_T\) 为标准高斯 \(N(0,I)\)。</li> </ol> <p><img src="/images/2025-12-25-diffusion/image-20250826151937355.png" alt="image-20250826151937355" class="img-fluid"></p> <p>由于 z 和 x 维度一样，从这儿开始，如上图所示，统一用 x 表示。</p> <p>接下来，就是给出满足上述三个约束的 \(q(x_{t}\mid x_{t-1})\) 的参数了。然后推导得到约束3。这个推导参考文献[2]比较详细。这儿不再赘述。 \(q(x_{t}\mid x_{t-1})=N(x_t\mid \sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I )\) 在推导中，还有参数 \(\beta_t\), \(\bar{\alpha}_t\)。这几个参数可以相互转化。</p> <p>上式用<strong>参数化</strong>的技巧，就是 \(x_{t}=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} , \epsilon_{t-1}\sim N(0,I)\) 注意，参数 \(\alpha_t\) (or \(\beta_t\) or 其他变体) 的设置，可以按照预先规定的某种函数（scheduler），或者也可以学习得到。</p> <p>其次，经过推导，还有一个重要的性质 \(x_{t}=\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon , \epsilon\sim N(0,I)\) 也就是说，按编码的流程要采样得到 \(x_{t}\)，可以根据上式一步采样，不需要先采样 \(x_{1},x_{2}...x_{t-1}\)。这一点在训练中是有用的。</p> <h3 id="似然最大化elbo">似然最大化，ELBO</h3> <p>接下来，就是要通过最大似然来学习解码器 \(p_\theta (x_{t-1}\mid x_t)\) 了。具体来说，有满足某种分布的\(x\)数据集（这个 \(x\) 就是上面的 \(x_0\)），要最大化似然 \(\log p(x)=\log \int p(x_{0:T})dx_{1:T}\)</p> <blockquote> <p>这边有个疑惑，就是在后续推导中，有用到 \(q(x_{1:T}\mid x_0)\)。如果先进行编码，再进行解码，岂不是有两个不同的 \(x_1,x_2,...x_{T-1}\)。正确理解应该是，\(x_{1:T}\) 是被 marginalized out 的，所以考虑了所有可能，但每一种可能下，都只有一个值，不会有两个值。不管是p还是q，都是概率模型，所以任意值下都是可计算概率的。可以认为，先进行编码，得到了\(x_{1:T}\)，然后计算在当前模型参数下\(p_\theta (x_{t-1}\mid x_t)\)的似然，及其梯度。</p> </blockquote> <p>\(p_\theta (x_{t-1}\mid x_t)\)可以设为任意某种分布，只要知道\(z_{t-1},z_t\)（由编码生成）就可以计算概率\(p_\theta (x_{t-1}\mid x_t)\)。但是根据大段的推导，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</p> <p>推导参见参考文献。</p> <p>\(\log p(x)=\log \int p(x_{0:T})dx_{1:T}&gt;ELBO\)，ELBO由三项组成</p> <p><img src="/images/2025-12-25-diffusion/image-20250826210812707.png" alt="image-20250826210812707" class="img-fluid"></p> <p>其中，</p> <ol> <li>第一项类似于VAE中的ELBO，可以通过蒙特卡洛估计来近似和优化。但在我看的代码中，这一项并没有使用。</li> <li>第二项没有需训练的参数，也是等于0.</li> <li>第三项可以推导出，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</li> </ol> <p>关于第一项，在代码实现中没有被使用，AI是这样回答的。</p> <blockquote> <p><img src="/images/2025-12-25-diffusion/image-20250826211846887.png" alt="image-20250826211846887" class="img-fluid"></p> </blockquote> <p>现在进一步分析上述第三项。可以推导得到，\(q(x_{t-1}\mid x_t,x_0)\) 是一个高斯分布。因此 \(p_\theta (x_{t-1}\mid x_t)\) 也应该是高斯分布，方差和\(q(x_{t-1}\mid x_t,x_0)\)的方差相同（因为方差和\(x_0\)无关，所以可以直接设为相等），均值要尽量接近<img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826222640680.png" alt="image-20250826222640680" style="zoom:50%;"></p> <p>可以看出 \(\mu_q\) 和 \(x_0\) 有关，而\(p_\theta (x_{t-1}\mid x_t)\) 并没有\(x_0\)的信息，所以可以训练一个神经网络 \(\hat{x}_\theta (x_t,t)\)，从\(x_t\)来预测 \(x_0\)</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223028627.png" alt="image-20250826223028627" style="zoom:50%;"></p> <p>由于两个高斯的KL距离是可以直接计算，再经过推导，最大化ELBO就等价于，在每个timestep，神经网络的输出 \(\hat{x}_\theta (x_t,t)\)要和\(x_0\)越符合，越好。即最优解为</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223326616.png" alt="image-20250826223326616" style="zoom: 50%;"></p> <p>上式中的系数，也可以推导等于</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223502799.png" alt="image-20250826223502799" style="zoom:50%;"></p> <h3 id="训练过程">训练过程</h3> <p>训练采用sgd，对于每一个\(x_0\)，只随机取一个\(t\)，采样得到 \(x_t\)（不用迭代，直接采样），然后计算上式，上式即为 loss。</p> <h3 id="推理过程">推理过程</h3> <p>推理过程则需要执行完整的解码过程。从一个随机向量 \(x_T\)出发，通过神经网络，计算得到 \(\hat{x}_\theta (x_t,t)\)，然后由上面 ，计算得到均值 \(\mu_\theta(x_t,t)\)；计算后验方差（\(p_\theta\) 的方差和后验概率 \(q(x_{t-1}\mid x_t,x_0)\) 的方差相等）；采样得到 \(x_{T-1}\)，如此迭代，最后得到 \(x_0\)。</p> <h3 id="简化的loss">简化的Loss</h3> <p>见[2] 中的一些补充</p> <p>Empirically, Ho et al. (2020) 提出不带上式中系数的Loss</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250827094521508.png" alt="image-20250827094521508" style="zoom:50%;"></p> <p>其中，\(\epsilon_t\) 是编码过程采样得到 \(x_t\) 的标准高斯噪声，\(\epsilon_\theta\) 是学习的神经网络。大概理解是这样：目标是要让解码的 \(x_{t-1}\) 分布均值接近编码的 \(x_{t-1}\) 分布均值，编码的 \(x_{t-1}\) 分布均值和 \(x_{t}\) 之间有关系，除了乘性系数外，差值为 \(\epsilon_t\) 。解码的 \(x_{t-1}\) 分布均值经过推导式子和编码时大概一样，区别是需要预测一个\(\epsilon_\theta\)。见下图 Algorithm2 Sampling step 4.</p> <p><img src="/images/2025-12-25-diffusion/image-20250827095354468.png" alt="image-20250827095354468" class="img-fluid"></p> <h4 id="训练和推理">训练和推理</h4> <p><img src="/images/2025-12-25-diffusion/image-20250827143550828.png" alt="image-20250827143550828" class="img-fluid"></p> <h3 id="学习扩散噪声参数">学习扩散噪声参数</h3> <p>上述，扩散噪声参数是按照scheduler确定的，在实现中，也可以联合学习得到。</p> <h2 id="noise-conditioned-score-networks-ncsn">noise-conditioned score networks (NCSN)</h2> <p>注意：本小节引用了AI的回答，其中 \(p\) 泛指概率，而不是特制反向采样中的概率分布。</p> <h3 id="分数-s_thetax_t-t">分数 \(s_\theta(x_t, t)\)</h3> <p>所谓“分数”，指的是概率密度函数的对数关于数据的梯度，即： \(s(x) = \nabla_x \log p(x)\) 其中 \(p(x)\) 是数据的概率密度函数，分数 \(s(x)\) 描述了数据在概率密度上的局部变化方向和幅度。通过训练一个模型 \(s_\theta(x)\) 来逼近真实的分数 \(\nabla_x \log p(x)\)，可以间接学习数据的概率分布，而无需直接估计 \(p(x)\) 本身。</p> <p>在扩散模型中，分数可以有两种相关定义，具体取决于上下文：</p> <ul> <li> <strong>边缘分布的分数</strong>：\(\nabla_{x_t} \log p(x_t)\)，其中 \(p(x_t)\) 是时间步 \(t\) 的边缘分布，即： \(p(x_t) = \int p(x_t \mid x_0) p_{\text{data}}(x_0) \, dx_0\) 这个分数描述了 \(x_t\) 在整个数据分布上的概率密度梯度。然而，\(p(x_t)\) 通常难以直接计算，因为它涉及对所有可能的 \(x_0\) 积分。</li> <li> <strong>条件分布的分数</strong>：\(\nabla_{x_t} \log p(x_t \mid x_0)\)，其中 \(p(x_t \mid x_0)\) 是前向过程中给定原始数据 \(x_0\) 的条件分布。根据扩散模型的前向过程： \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\) 这个分数的计算更直接，因为它是高斯分布的梯度： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\)</li> </ul> <p>在实际的扩散模型训练中（如 DDPM），通常使用 <strong>去噪分数匹配（Denoising Score Matching, DSM）</strong>，训练分数模型 \(s_\theta(x_t, t)\) 来逼近 <strong>条件分数</strong> \(\nabla_{x_t} \log p(x_t \mid x_0)\)，而不是边缘分数 \(\nabla_{x_t} \log p(x_t)\)。这是因为条件分数的表达式已知且易于计算，而边缘分数需要复杂的积分。</p> <h3 id="分数如何用在采样中">分数如何用在采样中</h3> <p>大体思路如下：根据扩散模型的推导，在反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计（下面第2点），而\(x_0\) 的估计和分数\(s_\theta(x_t, t)\)有关系（下面第1点）。</p> <ol> <li> <strong>分数的可计算性</strong>：</li> </ol> <p>前向过程定义了 \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)，其分数可以通过高斯分布的性质直接计算： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 这个分数是已知的，因此可以作为训练目标。</p> <p>训练好的分数模型 \(s_\theta(x_t, t)\) 可以用来估计 \(x_0\)（原始数据）的值： \(s_\theta(x_t, t) \approx -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 重排后： \(x_0 \approx \frac{x_t + (1 - \bar{\alpha}_t) s_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\)</p> <ol> <li> <strong>与反向过程的联系</strong>：</li> </ol> <p>反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计。分数模型 \(s_\theta(x_t, t)\) 提供了从 \(x_t\) 估计 \(x_0\) 的方法。</p> <p>均值可以通过高斯条件分布公式得到： \(\mu_q(x_t, x_0) = \sqrt{\alpha_t} x_{t-1} + \frac{(1 - \alpha_t) \sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_t}\) 将 \(s_\theta(x_t, t)\) 代入反向过程的均值表达式，DDPM 推导出简化的均值形式： \(\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} s_\theta(x_t, t) \right)\) 这个表达式直接用 \(x_t\) 和 \(s_\theta(x_t, t)\) 表示均值。</p> <p>以上公示由AI生成，用作思路理解。</p> <h3 id="和上面简化的loss中-epsilon-的关系">和上面简化的Loss中 \(\epsilon\) 的关系</h3> <p>\(\epsilon\) 是 \(x_t\) 和它分布均值之间的差。即 \(\epsilon =x_t - \sqrt{\bar{\alpha}_t} x_0\) \(\epsilon_\theta(x_t,t)\) 是它的估计。</p> <p>因此有 \(s_\theta(x_t, t) \approx -\frac{\epsilon_\theta(x_t,t)}{1 - \bar{\alpha}_t}\)</p> <h2 id="classifier-guided-diffusion">Classifier Guided Diffusion</h2> <p>分类器引导扩散中，有一个条件 \(y\)，使用了条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\) 来调整无条件扩散模型的采样过程，符合分数模型的核心思想。</p> <p><strong>分类器 \(p(y \mid x_t)\) 和无条件扩散模型 \(s_\theta(x_t, t)\) 都是预训练的</strong>，然后通过调整反向过程的均值。分类器 \(p_\phi(y \mid x_t, t)\) 是预训练的，在带噪声的图像 \(x_t \sim q(x_t \mid x_0)\) 上训练，以预测类别 \(y\)。这确保分类器梯度在不同噪声水平下可靠，为条件分数提供准确的引导。</p> <p>具体来说，论文通过以下方式从分数模型角度解释分类器引导扩散：</p> <ul> <li> <p>它将无条件分数 \(\nabla_{x_t} \log p(x_t)\)（由<strong>预训练</strong>的扩散模型提供）与分类器的梯度 \(\nabla_{x_t} \log p(y \mid x_t)\) 结合，构造条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。这一步通过贝叶斯准则可以推导。 \(\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t)\)</p> </li> <li> <p>反向过程的均值通过条件分数调整，确保采样生成符合特定类别 \(y\) 的样本。</p> </li> </ul> <p>替换为条件分数后，得到：</p> <p>\(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \nabla_{x_t} \log p(x_t \mid y) \right)\) \(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \left( s_\theta(x_t, t) + s \cdot \nabla_{x_t} \log p_\phi(y \mid x_t) \right) \right)\) 这里的 \(s_\theta(x_t, t) \approx \nabla_{x_t} \log p(x_t)\)，而 \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) 是分类器梯度，整体形成条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。</p> <blockquote> <p>“To perform conditional sampling, we train a classifier \(p_\phi(y \mid x_t)\) on noisy images to predict the class label \(y\), and use its gradient \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) to guide the diffusion process. Specifically, we modify the reverse process mean as follows: \(\tilde{\mu}_t(x_t, y) = \mu_t(x_t) + s \cdot \sigma_t^2 \cdot \nabla_{x_t} \log p_\phi(y \mid x_t)\) where \(\mu_t(x_t)\) is the original mean of the reverse process, \(\sigma_t^2\) is the variance, and \(s\) is a scale factor controlling the strength of the classifier guidance.”</p> </blockquote> <p>这边分类器梯度前面有正负不一致。先不管。</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p><strong>论文中的符号和其他文献不同，\(\lambda\) 是时间 \(t\)，但是采样时 \(\lambda\) 从小到大。\(z_\lambda\) 对应 \(x_t\)。</strong></p> <p>CFG 的核心是联合训练一个支持条件和无条件的扩散模型，然后在采样时通过线性组合分数函数来实现引导。论文将扩散模型表述在连续时间框架下，\(\lambda\) 被定义为前向过程的噪声水平，\(\lambda=\log \alpha_\lambda^2/\sigma_\lambda^2\) 即 log SNR。其中前向过程为 \(q(z_\lambda \mid x) = \mathcal{N}(\alpha_\lambda x, \sigma_\lambda^2 I)\)，\(\alpha_\lambda^2 = 1 / (1 + e^{-\lambda})\)，\(\sigma_\lambda^2 = 1 - \alpha_\lambda^2\)。</p> <h3 id="训练过程algorithm-1-in-the-paper">训练过程（Algorithm 1 in the paper）：</h3> <ol> <li> <p>使用单个神经网络参数化分数估计器 \(\epsilon_\theta(z_\lambda, c)\)（条件模型）和 \(\epsilon_\theta(z_\lambda)\)（无条件模型）。</p> </li> <li> <p>在训练时，以概率 \(p_{\text{uncond}}\)（超参数，通常为 0.1-0.2）随机将条件信息 \(c\) 设置为无条件标识符 \(\emptyset\)，从而联合训练两个模型。 训练目标是去噪分数匹配（Denoising Score Matching）：\(\mathbb{E}_{\epsilon, \lambda} [ \\mid \epsilon_\theta(z_\lambda) - \epsilon \\mid _2^2 ]\)，其中 \(z_\lambda = \alpha_\lambda x + \sigma_\lambda \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)。</p> </li> <li> <p>论文描述：“We jointly train the unconditional and conditional models simply by randomly setting \(c\) to the unconditional class identifier \(\emptyset\) with some probability \(p_{\text{uncond}}\), set as a hyperparameter.”</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110741671.png" alt="image-20250930110741671" class="img-fluid"></p> </li> </ol> <h3 id="采样过程algorithm-2-in-the-paper">采样过程（Algorithm 2 in the paper）：</h3> <ol> <li>从纯噪声 \(z_{\lambda_T} \sim \mathcal{N}(0, I)\) 开始，逐步去噪。</li> <li>在每个时间步 \(t\)，计算引导分数：\(\tilde{\epsilon}_\theta(z_\lambda, c) = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\)，其中 \(w\) 是引导强度（guidance strength，通常 \(w &gt; 0\)）。</li> <li>使用这个引导分数来更新样本，例如在 DDPM 采样中替换原分数函数。 论文解释：“Form the classifier-free guided score at log SNR \(\lambda_t\): \(\tilde{\epsilon}_t = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\).”</li> </ol> <p>直观上，\(w = 0\) 时退化为标准条件采样；\(w &gt; 0\) 时，增强条件分数并减弱无条件分数，从而提高样本质量（更符合条件 \(c\)），但降低多样性。</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110925539.png" alt="image-20250930110925539" class="img-fluid"></p> <p>该算法中，step 3的解释如下，见[2]</p> <p><img src="/images/2025-12-25-diffusion/image-20250930111119169.png" alt="image-20250930111119169" class="img-fluid"></p> <p>step 4 中，\(\tilde{x}_t\) 是当前时间步的去噪数据估计，基于引导分数 \(\tilde{\epsilon}_t\)。它表示在条件 \(c\) 引导下，模型预测的原始数据 \(x\)。</p> <p>step 5中，计算均值 \(\tilde{\mu}_t\): \(\tilde{\mu}_t = \alpha_{\lambda_{t+1}} \tilde{x}_t + \sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\)</p> <p>意义：\(\tilde{\mu}_t\) 是条件反向分布 \(p(z_{\lambda_{t+1}} \mid z_{\lambda_t}, c) = \mathcal{N}(z_{\lambda_{t+1}}; \tilde{\mu}_t, \sigma_{\lambda_{t+1}}^2 I)\) 的均值，引导采样朝符合条件 \(c\) 的方向移动。 作为 \(z_t\) 和 \(\tilde{x}_t\) 的函数：公式明确显示 \(\tilde{\mu}_t\) 依赖于当前样本 \(z_t\) 和引导预测器 \(\tilde{x}_t\)。其中：</p> <p>\(\alpha_{\lambda_{t+1}} \tilde{x}_t\): 缩放后的去噪估计，代表信号部分。 \(\sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\): 噪声调整项，通过 \(\tilde{x}_t\) 和 \(z_t\) 的差值引入引导分数的影响。</p> <h3 id="简化采样">简化采样</h3> <p>实际编程中，没有按algorithm2这么复杂。还是按 DDPM 简化Loss的采样过程，不过把 \(\epsilon\) 替换成引导的 \(\tilde{\epsilon}\) 。</p> <h2 id="ddim">DDIM</h2> <p>应该可以肯定的是，DDIM和DDPM的训练过程是一样的，采样过程不一样。</p> <p>但在理论上，forward process（本论文称为 inference）不一样。DDIM引入了非马尔可夫forward process。</p> <h3 id="non-markovian-forward-processes">NON-MARKOVIAN FORWARD PROCESSES</h3> <p>前向过程是一个随机过程，用联合概率密度分布表示。由于在DDPM中，训练过程是在每一个 \(t\)，让神经网络根据 \(x_t\) 去预测分数\(\nabla_{x_t} \log p(x_t \mid x_0)\) （等价于预测 \(x_0\)，或 \(\epsilon(x_t,t)\)）。这个训练过程只和 \(q(x_t\mid x_0)\) 有关，和联合概率密度分布没有关系，实际上有无数联合概率密度分布，可以获得这个边际分布。DDPM定义了其中的具有马尔可夫性的一种。</p> <p>DDIM论文直接设计另一种联合概率分布。给定一个实数向量 \(\sigma \in R^T_{\ge 0}\)，定义联合概率密度分布：</p> <p><img src="/images/2025-12-25-diffusion/image-20251003144728914.png" alt="image-20251003144728914" class="img-fluid"></p> <p>论文证明，上面的这个联合概率分布，满足 \(q_\sigma(x_t\mid x_0)=N(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) 也就是和DDPM中相同。所以训练过程和 DDPM 一样。</p> <p>这个联合概率分布的设计中，直接给出了后向概率分布 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)，这样在采样时，就可以直接按照这个概率分布进行采样。</p> <blockquote> <p>The magnitude of \(\sigma\) controls the how stochastic the forward process is; when \(\sigma=0\), we reach an extreme case where as long as we observe x0 and xt for some t, then \(x_{t-1}\) become known and fixed.</p> </blockquote> <p>在上面的讨论中，并没有给出潜在的前向过程，也即是如何从 \(x_0\) 逐步推理得到 \(x_T\)。论文说这个过程可以由贝叶斯公式推导得到 \(q_\sigma(x_{t}\mid x_{t-1},x_0)\)，它是一个高斯分布。由于有依赖 \(x_0\)，它不是一个马尔可夫过程。熟悉DDPM的话，可以知道，在训练时，实际上并不需要去执行这个前向过程。</p> <h3 id="sampling-from-generalized-generative-processes">SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</h3> <p>前面说过，生成过程可以直接借助 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)</p> <blockquote> <p>Intuitively, given a noisy observation \(x_t\), we first make a prediction of the corresponding \(x_0\), and then use it to obtain a sample \(x_{t-1}\) through the reverse conditional distribution \(q_\sigma(x_{t-1}\mid x_t,x_0)\), which we have defined.</p> </blockquote> <p>如果我们采用simple loss预测 \(\epsilon\)，则</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153043856.png" alt="image-20251003153043856" class="img-fluid"></p> <p>上文中，\(f_\theta^t\) 就是对 \(x_0\) 的预测。把式（9）中\(f_\theta^t\)代入式(10)，可得</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153420286.png" alt="image-20251003153420286" class="img-fluid"></p> <p>上式就是采样过程。其中\(\epsilon_\theta^{(t)}\) 是神经网络的输出。某一个特定的 \(\alpha\)，则退回成DDPM。</p> <p>而 \(\alpha_t=0\)，即为 DDIM。</p> <h3 id="加速采样过程">加速采样过程</h3> <p>上述讨论已经定义了DDIM，但还没涉及到加速采样。加速采样，也有类似的理论，但是训练过程不变，因此也要满足 marginal 要和 DDPM 一样。</p> <p>假设我们已经选择了时间的子集 \(\tau\)，\(\tau_S=T\)；定义联合概率密度</p> <p><img src="/images/2025-12-25-diffusion/image-20251003154717877.png" alt="image-20251003154717877" class="img-fluid"></p> <p>也就是说， \(\tau\) 内的时间满足上面的非马尔可夫inference过程，其他时间是一个星形的概率图结构。由于marginal 要和 DDPM 一样，所以训练不变。</p> <p>在生成时，只用到了 \(\tau\) 内的时间，其他时间就不管。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/optical_flow/">Optical_Flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Teaching-Tailored-to-Talent/">Teaching Tailored To Talent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/VINS-and-ESVO2/">Vins And Esvo2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/dino/">DINO 如何用于密集预测</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/3dtasks/">3D 任务：SFM, MVS, NVS, VO, VIO, SLAM</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Erkang Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-optical-flow",title:"Optical_Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2026/optical_flow/"}},{id:"post-teaching-tailored-to-talent",title:"Teaching Tailored To Talent",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2026/Teaching-Tailored-to-Talent/"}},{id:"post-vins-and-esvo2",title:"Vins And Esvo2",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/VINS-and-ESVO2/"}},{id:"post-dino-\u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",title:"DINO \u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",description:"\u5982\u4f55\u4f7f\u7528DINO, DPT, RoPE",section:"Posts",handler:()=>{window.location.href="/blog/2025/dino/"}},{id:"post-3d-\u4efb\u52a1-sfm-mvs-nvs-vo-vio-slam",title:"3D \u4efb\u52a1\uff1aSFM, MVS, NVS, VO, VIO, SLAM",description:"\u591a\u89c6\u89d2\u51e0\u4f55\uff0c\u65b0\u89c6\u89d2\u751f\u6210",section:"Posts",handler:()=>{window.location.href="/blog/2025/3dtasks/"}},{id:"post-matplotlib\u8f93\u51fa\u4e2d\u6587",title:"Matplotlib\u8f93\u51fa\u4e2d\u6587",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"}},{id:"post-mamba",title:"Mamba",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/mamba/"}},{id:"post-flow",title:"Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/flow/"}},{id:"post-diffusion",title:"Diffusion",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-nerf-and-gaussian-splatting",title:"NeRF and Gaussian Splatting",description:"Gaussian Splatting, NeRF, Alpha-blending, Point-Based Rendering, Jacobian",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussianSplatting/"}},{id:"post-colmap",title:"COLMAP",description:"SFM",section:"Posts",handler:()=>{window.location.href="/blog/2025/sfm-mvs-rendering/"}},{id:"post-\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",title:"\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/impulse/"}},{id:"post-\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",title:"\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/math/"}},{id:"post-fisheye",title:"Fisheye",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/fisheye/"}},{id:"post-montecarlo",title:"Montecarlo",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/MonteCarlo/"}},{id:"post-\u5c0f\u6ce2\u5206\u6790",title:"\u5c0f\u6ce2\u5206\u6790",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/"}},{id:"post-lifeifei",title:"Lifeifei",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Lifeifei/"}},{id:"post-orgmode",title:"Orgmode",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/orgmode/"}},{id:"post-python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",title:"Python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Python%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%88%96%E6%A8%A1%E5%9D%97/"}},{id:"post-proximal",title:"Proximal",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/proximal/"}},{id:"post-visualtransformer",title:"Visualtransformer",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/VisualTransformer/"}},{id:"post-pytorch-note",title:"Pytorch Note",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Pytorch-note/"}},{id:"post-android-opencv-ndk",title:"Android Opencv Ndk",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/android-opencv-ndk/"}},{id:"post-repvgg",title:"Repvgg",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/RepVGG/"}},{id:"post-popularlibrary",title:"Popularlibrary",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/popularLibrary/"}},{id:"post-\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",title:"\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%85%88%E8%BF%9B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}},{id:"post-one-stage-detection",title:"One Stage Detection",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/one-stage-detection/"}},{id:"post-polarization",title:"Polarization",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/polarization/"}},{id:"post-vae-vqvae-vagan",title:"Vae Vqvae Vagan",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/vae-vqvae-vagan/"}},{id:"post-multiple-object-tracking",title:"Multiple Object Tracking",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Multiple-Object-Tracking/"}},{id:"post-\u77e9\u9635",title:"\u77e9\u9635",description:"Matrix",section:"Posts",handler:()=>{window.location.href="/blog/2024/Matrix/"}},{id:"post-typora-and-jekyll",title:"Typora and Jekyll",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/images/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%65%6B%78%6D@%71%71.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hWo1RTsAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
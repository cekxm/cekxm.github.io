<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vins And Esvo2 | 计算机视觉 </title> <meta name="author" content="Erkang Chen"> <meta name="description" content="简要介绍"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> 计算机视觉 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Vins And Esvo2</h1> <p class="post-meta"> Created in December 31, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="vins">VINS</h2> <h3 id="1-系统架构通用因子框架">1. 系统架构：通用因子框架</h3> <p>该方法的核心思想是将每种传感器都视为框架中的一个<strong>通用因子（Factor）</strong> 。</p> <ul> <li> <strong>多传感器支持</strong>：框架支持多种传感器组合，如单目相机+IMU、双目相机、以及双目相机+IMU。</li> <li> <strong>因子图构建</strong>：共享公共状态变量的因子会被累加在一起，构建一个位姿图（Pose Graph）优化问题 。</li> <li> <strong>鲁棒性</strong>：由于不依赖特定传感器，系统可以轻松处理传感器失效的情况，通过移除失效因子的方式快速切换传感器组合。</li> </ul> <p>该方法不是学习类的方法。</p> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251226153814897.png" alt="image-20251226153814897" class="img-fluid"></p> <p>系统通过一个滑动窗口（Sliding Window）来维护待优化的状态变量。以下是该方法中<strong>状态向量（State Vector）</strong>的详细构成：</p> <h4 id="完整的状态向量">完整的状态向量</h4> <p>在滑动窗口优化中，总的状态向量 \(\mathcal{X}\) 被定义为窗口内所有帧的状态、外参以及特征点深度的集合：</p> \[\mathcal{X} = [\mathbf{x}_m, \mathbf{x}_{m+1}, \dots, \mathbf{x}_n, \mathbf{x}_c^b, \lambda_1, \lambda_2, \dots, \lambda_l]\] <p>其中：</p> <ul> <li>\(m, n\) 是滑动窗口的起始帧和结束帧。</li> <li>\(\mathbf{x}_i\) 是第 \(i\) 帧对应的 <strong>IMU 核心状态</strong>。</li> <li>\(\mathbf{x}_c^b\) 是相机与IMU之间的 <strong>外参（Extrinsic Parameters）</strong>。</li> <li>\(\lambda_l\) 是第 \(l\) 个视觉特征点的 <strong>逆深度（Inverse Depth）</strong>。</li> </ul> <p><strong>和论文表示不太一样，但实际是一样的。这边body frame 的位姿状态放在IMU里面了。</strong></p> <hr> <h5 id="具体的-imu-状态-mathbfx_i">具体的 IMU 状态 (\(\mathbf{x}_i\))</h5> <p>对于窗口中的每一帧 \(i\)，其核心状态变量包含 15 个维度：</p> \[\mathbf{x}_i = [\mathbf{p}_{b_i}^w, \mathbf{v}_{b_i}^w, \mathbf{q}_{b_i}^w, \mathbf{b}_a, \mathbf{b}_g]\] <ul> <li> <strong>\(\mathbf{p}_{b_i}^w\) (3D Position)</strong>：IMU 坐标系在世界坐标系下的位置。</li> <li> <strong>\(\mathbf{v}_{b_i}^w\) (3D Velocity)</strong>：IMU 在世界坐标系下的速度。</li> <li> <strong>\(\mathbf{q}_{b_i}^w\) (Quaternion/Rotation)</strong>：从 IMU 坐标系到世界坐标系的旋转（通常用四元数表示 Hamilton 形式，或旋转矩阵）。</li> <li> <strong>\(\mathbf{b}_a\) (Accelerometer Bias)</strong>：加速度计的零偏。</li> <li> <strong>\(\mathbf{b}_g\) (Gyroscope Bias)</strong>：陀螺仪的零偏。</li> </ul> <hr> <h5 id="辅助状态变量">辅助状态变量</h5> <p>为了保证系统的通用性和精度，论文还包括了以下状态：</p> <ul> <li> <p>传感器外参 (\(\mathbf{x}_c^b\))：</p> <p>包含从相机到 IMU 坐标系的变换 \([\mathbf{p}_c^b, \mathbf{q}_c^b]\)。在系统在线运行时，这些外参也可以被放入优化器进行实时修正。</p> </li> <li> <p>视觉特征点状态 (\(\lambda_l\))：</p> <p>VINS 采用<strong>逆深度（Inverse Depth）</strong>作为特征点的参数化方式。相比于直接使用 3D 坐标 \((x, y, z)\)，逆深度可以更好地处理距离相机非常远的特征点，且其分布更接近高斯分布，有助于数值优化的收敛。</p> </li> </ul> <h4 id="因子">因子</h4> <p>每一个方框是一个 factor，也就是一次测量。更细致的话</p> <ul> <li>相机：一个测量对应一个特征点在该时刻的位置。特征点使用KLT进行跟踪，从之前 \(l\) 时刻到当前 \(t\) 时刻有对应关系，这两个时刻在图像上的位置有约束。</li> <li>如果双目相机，则特征点在左边和右边的位置上，也有约束。</li> <li>IMU: 基于预积分结果的相邻关键帧间的运动约束残差。</li> </ul> <h3 id="2-代价函数-cost-function">2. 代价函数 (Cost Function)</h3> <p>该系统将状态估计建模为一个<strong>最大似然估计（MLE）</strong>问题。在假设测量噪声符合高斯分布的条件下，MLE被转化为一个非线性最小二乘问题，即最小化所有传感器测量残差的加权平方和。</p> <p>在滑动窗口优化中，总代价函数（MAP 形式）定义如下：</p> \[\mathcal{X}_{m:n}^* = \arg \min_{\mathcal{X}_{m:n}} \left\{ \sum_{t=m}^{n} \sum_{k \in S} \\mid z_t^k - h_t^k(\mathcal{X}_{m:n}) \\mid _{\Omega_t^k}^2 + \\mid H_p \delta \mathcal{X}_{m:n} - b_p \\mid ^2 \right\}\] <p>该代价函数主要由以下三部分组成：</p> <ul> <li> <strong>视觉因子残差（Camera Factor）</strong>：特征点在不同帧之间的重投影误差 。</li> <li> <strong>IMU 因子残差（IMU Factor）</strong>：基于预积分结果的相邻关键帧间的运动约束残差。</li> <li> <strong>边缘化先验项（Prior Term）</strong>：由滑动窗口中被移除（边缘化）的旧状态所转换而来的约束信息，用于保留历史观测对当前状态的影响。</li> </ul> <h3 id="3-光束法平差-bundle-adjustment-ba">3. 光束法平差 (Bundle Adjustment, BA)</h3> <p>论文将状态估计过程称为<strong>基于滑动窗口的光束法平差</strong>。</p> <ul> <li> <strong>基本原理</strong>：BA通过同时优化相机位姿、IMU状态（速度、偏置）以及特征点的逆深度，使得所有观测残差最小化。</li> <li> <strong>求解方式</strong>： <ul> <li> <strong>线性化</strong>：在当前估计值附近对代价函数进行一阶泰勒展开，将其转化为线性最小二乘问题。</li> <li> <strong>迭代优化</strong>：采用 <strong>Gauss-Newton</strong> 或 <strong>Levenberg-Marquardt</strong> 方法进行多次迭代直至收敛。系统具体使用了 <strong>Ceres Solver</strong> 进行高效求解。</li> </ul> </li> <li> <strong>计算效率控制</strong>： <ul> <li> <strong>滑动窗口（Sliding Window）</strong>：为了维持实时性，优化只在固定大小的窗口（通常是10帧左右）内进行，而不是处理整个轨迹。</li> <li> <strong>边缘化（Marginalization）</strong>：当新帧加入导致窗口满时，利用<strong>舒尔补（Schur Complement）</strong>将最老的帧移除，并将其携带的信息转化为当前窗口内状态的先验分布。</li> </ul> </li> </ul> <p>这种基于优化的方法相比于传统的滤波器（如EKF）具有更高的精度，因为它可以在非线性空间内进行多次迭代更新，并能更好地处理传感器之间的时钟同步偏差。</p> <h2 id="esvo2">ESVO2</h2> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251227073033444.png" alt="image-20251227073033444" class="img-fluid"></p> <p><strong>ESVO2</strong> 是由湖南大学（Yi Zhou 教授团队）与香港科技大学（Shaojie Shen 教授团队）等合作开发的一个<strong>实时、紧耦合的双目事件相机视觉惯性里程计系统</strong> 。</p> <h3 id="概述">概述</h3> <p>相比前代系统，ESVO2 引入了以下四大创新 ：</p> <ul> <li> <strong>自适应累积 (Adaptive Accumulation, AA)</strong>：提出了一种新型的类图像事件表示方法，能够根据事件的局部动态自动调整累积时间。这使得系统能高效提取边缘轮廓点，而不受运动速度变化的影响。</li> <li> <strong>引入 IMU 预积分</strong>：通过将 IMU 测量值作为运动先验，解决了纯视觉追踪在特定旋转维度（如 Pitch 和 Yaw）上的简并（Degeneracy）问题 。</li> <li> <strong>紧耦合后端优化</strong>：设计了一个精简的后端，专门优化线性速度和 IMU 偏置（Bias），从而抑制轨迹漂移，确保全局一致性。</li> <li> <strong>增强的建图模块</strong>：结合了“时间同步双目（Temporal Stereo）”和“静态双目（Static Stereo）”配置，并引入快速块匹配方案，显著提升了深度图的完整性和局部平滑度。</li> </ul> <h4 id="系统架构">系统架构</h4> <p>ESVO2 采用并行设计，由四个独立线程组成：</p> <ol> <li> <strong>预处理线程</strong>：负责事件的自适应累积及时间表面（Time Surface）的更新。</li> <li> <strong>追踪线程（Localization）</strong>：执行 3D-2D 时空配准，利用无偏移平滑时间表面（OS-TS）进行位姿估计。</li> <li> <strong>建图线程（Mapping）</strong>：实时恢复半稠密深度图并维护局部 3D 地图。</li> <li> <strong>后端线程（Back-end）</strong>：进行滑动窗口优化，不断更新速度和 IMU 参数。</li> </ol> <h3 id="mapping">Mapping</h3> <h4 id="temporal-stereo时间双目"><strong>Temporal Stereo（时间双目）</strong></h4> <p>是一种核心的深度估计技术。它通过结合<strong>时间维度</strong>（单相机的运动）和<strong>空间维度</strong>（双相机的基线）来提高建图的鲁棒性。</p> <h5 id="1-定义与核心思想">1. 定义与核心思想</h5> <p><strong>Temporal Stereo</strong> 指的是利用<strong>单只相机在不同时刻（即随时间位移）</strong>观测到的视觉信息来估计物体深度的技术 。</p> <ul> <li> <p><strong>对比 Static Stereo（静态双目）</strong>：静态双目是利用左右两个相机在<strong>同一时刻</strong>的视图差异（视差）来计算深度。</p> </li> <li> <p><strong>结合的意义</strong>：在 ESVO2 中，系统并不只依赖左右相机的瞬时匹配，而是将相机的<strong>运动位移</strong>也作为一种“虚拟基线”。</p> </li> </ul> <h5 id="2-工作原理">2. 工作原理</h5> <p>在 ESVO2 的建图模块中，Temporal Stereo 的具体运作方式如下：</p> <ul> <li> <p><strong>时空观测一致性</strong>：当机器人移动时，同一个空间点会在不同时间点被相机捕获。系统会寻找当前事件流与该相机在不久前的历史观测之间的相关性 。</p> </li> <li> <p><strong>代价合并（Cost Fusion）</strong>：ESVO2 将 Temporal Stereo 和 Static Stereo 的匹配代价（Matching Cost）进行融合 。</p> </li> <li>如果运动方向有利于时间双目（例如向前运动，提供了纵向观测变化），Temporal Stereo 能提供很好的约束。</li> <li>如果左右相机之间的视差更明显，Static Stereo 则占据主导。</li> </ul> <h5 id="3-在-esvo2-中的作用">3. 在 ESVO2 中的作用</h5> <p>引入 Temporal Stereo 对事件相机 SLAM 具有至关重要的作用：</p> <ul> <li> <p><strong>解决观测退化</strong>：事件相机对垂直于其边缘运动的方向敏感。如果机器人仅进行某种特定方向的运动，静态双目可能会失效。Temporal Stereo 增加了额外的观测维度，保证了在 6-DoF（六自由度）复杂运动下的建图完整性 。</p> </li> <li> <p><strong>提高深度图质量</strong>：通过在时间轴上累积观测，系统能够过滤掉瞬时的噪声事件，生成的深度图更加平滑且空洞更少 。</p> </li> <li> <p><strong>快速块匹配</strong>：论文提到在执行 Temporal Stereo 匹配时采用了快速块匹配方案，这使得系统能够在普通 CPU 上实时处理 VGA 分辨率的高频数据 。</p> </li> </ul> <p>在 ESVO2 中，<strong>Temporal Stereo 实际上是将 VIO 系统变成了一个“多视图立体视觉（MVS）”系统</strong>。它不再局限于双目相机那段固定的物理基线，而是将相机的整个运动轨迹都变成了获取几何信息的源泉，从而在各种运动条件下都能输出高精度的半稠密深度图 。</p> <h4 id="fast-static-stereo快速静态双目"><strong>Fast Static Stereo（快速静态双目）</strong></h4> <p><strong>Fast Static Stereo（快速静态双目）</strong> 的目的确实是为了获得三维点的坐标，但它的实现方式和传统的“特征点提取 + 三角化”有所不同。</p> <p>以下是针对您问题的详细解答：</p> <h5 id="1-是为了获得稀疏三维点吗">1. 是为了获得稀疏三维点吗？</h5> <p><strong>是的，但更准确地说是“半稠密（Semi-dense）”的三维点。</strong></p> <ul> <li> <strong>非稀疏</strong>：传统的 VO（如 ORB-SLAM）只提取少量的特征点（如角点），产生的点云非常稀疏。</li> <li> <strong>非全稠密</strong>：由于事件相机只在亮度变化的地方产生数据（通常是物体的边缘），所以它无法像结构光或 LiDAR 那样获得物体的完整表面。</li> <li> <strong>半稠密</strong>：ESVO2 的 Fast Static Stereo 针对事件累积产生的<strong>边缘轮廓</strong>进行匹配，因此得到的是物体轮廓处的 3D 点云，这比传统的稀疏点云要密得多，足以辅助避障和局部建图。</li> </ul> <h5 id="2-只需要做三角化吗">2. 只需要做三角化吗？</h5> <p><strong>不完全是。</strong> 虽然双目外参（变换矩阵）已经离线标定好了，但“三角化”只是最后一步。最难、最核心的步骤是<strong>数据关联（Data Association / Correspondence）</strong>，即：<strong>左目里的这个事件点，对应右目里的哪个像素？</strong></p> <p>在 ESVO2 的 Fast Static Stereo 中，这个过程包含以下关键步骤：</p> <ul> <li> <strong>极线搜索 (Epipolar Search)</strong>：利用已标定的外参，在右目的极线上寻找匹配点。</li> <li> <strong>块匹配 (Block Matching)</strong>：由于事件数据不是灰度图，ESVO2 使用了 <strong>Time Surface（时间表面）</strong> 或像素块来进行相似度比较。</li> <li> <strong>时空一致性检查</strong>：系统会检查左右目事件在时间上的同步性（是否在极短的时间差内发生）以及空间上的几何一致性。</li> <li> <strong>三角化 (Triangulation)</strong>：只有当匹配的代价（Cost）足够低且置信度高时，系统才会根据已知的双目基线进行几何三角化，计算出深度 \(Z\)。</li> </ul> <h5 id="3-esvo2-fast-的体现">3. ESVO2 “Fast” 的体现</h5> <p>之所以称为 <strong>Fast</strong>，是因为它做了一些专门针对 CPU 实时性的优化：</p> <ol> <li> <strong>查找表 (Look-up Table)</strong>：预先计算极线方向，减少运行时的几何计算。</li> <li> <strong>跳过非边缘区域</strong>：只对有事件发生的像素点进行匹配，避开大量空白区域。</li> <li> <strong>并行化</strong>：利用多线程并行处理左右目的事件流块。</li> </ol> <p>你理解的“通过三角化获得坐标”是最终目的，但 <strong>Fast Static Stereo 的核心在于如何利用标定好的几何关系，在极高频率、高分辨率的事件流中快速找到左右目的匹配对。</strong></p> <p>值得注意的是，ESVO2 还会将 <strong>Static Stereo</strong>（左右目匹配）的结果与 <strong>Temporal Stereo</strong>（单目随时间运动产生的匹配）进行融合，这样即使在双目基线方向运动导致视差较小时，依然能获得准确的深度。</p> <h4 id="轮廓">轮廓</h4> <p>在 ESVO2 论文中，<strong>Fast Static Stereo（快速静态双目）不使用传统的稀疏特征点（如角点），而是直接使用物体的轮廓（Contour Points）进行匹配</strong> 。</p> <p>这种设计是由事件相机的物理特性决定的。以下是具体的实现逻辑：</p> <h5 id="1-为什么选择轮廓而不是特征点">1. 为什么选择轮廓而不是特征点？</h5> <ul> <li> <strong>物理机制</strong>：事件相机只在亮度变化时触发信号，因此其天然的输出就是物体的<strong>边缘和轮廓</strong>。</li> <li> <strong>鲁棒性</strong>：在高速运动中，传统的特征点（如 Harris 角点）可能因为观察不全而难以追踪 3。直接使用轮廓点能保留更完整的环境几何结构。</li> </ul> <h5 id="2-esvo2-的具体处理方式">2. ESVO2 的具体处理方式</h5> <p>ESVO2 并没有处理所有的事件点，而是通过以下步骤提取<strong>精准的轮廓点</strong>进行匹配：</p> <ul> <li> <strong>自适应累积 (Adaptive Accumulation, AA)</strong>：系统会根据事件的动态变化（速度快慢），自动决定累积多长时间的事件来生成一张类似图像的“AA图”。</li> <li> <strong>轮廓点采样 (Contour-point Sampling)</strong>：在 AA 图上，系统会采样那些代表瞬时边缘的像素点（即轮廓点） 。</li> <li> <strong>排除冗余</strong>：相比于直接使用原始事件流，这种方法筛选掉了大量噪声和冗余点，使得输入点更“精简且准确”。</li> </ul> <h5 id="3-fast-static-stereo-的快体现在哪">3. Fast Static Stereo 的“快”体现在哪？</h5> <p>由于处理的是轮廓点，ESVO2 采用了以下策略实现高效匹配：</p> <ul> <li> <strong>块匹配 (Block Matching)</strong>：在左右目相机的 AA 图或时间表面（Time Surface）上，沿着极线对这些<strong>采样后的轮廓像素块</strong>进行相似度搜索。</li> <li> <strong>取消非线性精化</strong>：论文提到，由于采样后的轮廓点已经非常精确，系统在 ESVO2 中<strong>取消了耗时的子像素级非线性精化步骤</strong>，从而显著提升了速度（在 VGA 分辨率下达到 20Hz 实时性），且精度几乎没有下降。</li> </ul> <p>在 ESVO2 的 <strong>Fast Static Stereo</strong> 模块中，获得 3D 坐标的过程是一个从“事件像素”到“空间点”的几何推导过程。虽然不使用复杂的特征描述子，但它利用了极其严格的<strong>时空约束</strong>。</p> <p>以下是具体的实现步骤：</p> <h5 id="1-极线约束搜索-epipolar-search">1. 极线约束搜索 (Epipolar Search)</h5> <p>假设左目相机在 \(u_L\) 像素处采样了一个轮廓点，由于双目相机的外参（基线 \(b\)、焦距 \(f\)）已预先标定且图像已做过极线校正（Rectification），那么这个点在右目图像中对应的匹配点一定位于<strong>水平的极线</strong>上。</p> <ul> <li> <strong>搜索范围</strong>：系统会在右目的极线上，根据设定的最小和最大深度范围，确定一个搜索区间。</li> </ul> <h5 id="2-基于时间表面的匹配-time-surface-matching">2. 基于“时间表面”的匹配 (Time Surface Matching)</h5> <p>这是最关键的一步。因为事件相机没有灰度值，它使用 <strong>Time Surface (TS)</strong> 或 <strong>AA 图</strong>来计算匹配相似度：</p> <ul> <li> <strong>提取 Patch</strong>：在左目 \(u_L\) 周围取一个小的像素块（例如 \(5 \times 5\)）。</li> <li> <strong>计算相似度</strong>：在右目极线的搜索区间内滑动，通过 <strong>零均值归一化互相关 (ZNCC)</strong> 或类似的度量函数，寻找与左目 Patch 最相似的区域。</li> <li> <strong>原理</strong>：虽然没有颜色，但物体轮廓在左右目形成的时间表面形状是非常相似的。</li> </ul> <h5 id="3-概率深度估计-probabilistic-depth-estimation">3. 概率深度估计 (Probabilistic Depth Estimation)</h5> <p>ESVO2 并不只是简单地计算一个视差，而是为每个点维护一个概率分布：</p> <ul> <li> <p><strong>逆深度建模</strong>：系统将深度表示为逆深度（Inverse Depth），并假设其服从高斯分布。</p> </li> <li> <p>代价聚合：当匹配代价最低的点被找到后，通过视差公式计算初始深度：</p> \[Z = \frac{f \cdot b}{d}\] <p>（其中 \(f\) 是焦距，\(b\) 是基线，\(d\) 是左右目像素的位移差/视差）。</p> </li> </ul> <h5 id="4-空间三角化-triangulation">4. 空间三角化 (Triangulation)</h5> <p>一旦确定了左右目的匹配对 \((u_L, v_L)\) 和 \((u_R, v_R)\)，就可以通过标准的<strong>三角化公式</strong>求出该轮廓点在左目相机坐标系下的 3D 坐标 \((X, Y, Z)\)：</p> <ul> <li> \[Z = \frac{f \cdot b}{u_L - u_R}\] </li> <li> \[X = \frac{(u_L - c_x) \cdot Z}{f}\] </li> <li> \[Y = \frac{(v_L - c_y) \cdot Z}{f}\] </li> </ul> <h5 id="5-esvo2-的特殊优化取消非线性精化">5. ESVO2 的特殊优化：取消非线性精化</h5> <p>在早期的 ESVO 版本中，三角化后通常还需要一个耗时的“非线性最小二乘精化”过程来微调 3D 位置。</p> <p>ESVO2 的改进：由于采用了自适应累积 (AA) 提取的轮廓点非常精准，论文提到他们直接跳过了非线性精化步骤。只要匹配代价足够低，就直接输出三角化的结果。这种“直接法”极大地提升了处理 VGA 分辨率数据的速度。</p> <h5 id="总结">总结</h5> <p>简单来说，获得 3D 坐标的流程是：</p> <p>左目采样轮廓点 \(\rightarrow\) 右目极线搜索匹配块 \(\rightarrow\) 计算视差 \(\rightarrow\) 几何三角化 \(\rightarrow\) 得到 3D 坐标。</p> <p>这一过程生成的点云是<strong>半稠密</strong>的，因为它只存在于有光强变化的边缘位置，但对于机器人感知环境（如避障、路径规划）已经足够丰富。</p> <h3 id="localization">Localization</h3> <h4 id="1-空间上的-3d-2d-配准当前时刻-vs-历史时刻">1. 空间上的 3D-2D 配准（当前时刻 vs 历史时刻）</h4> <p>追踪线程的核心逻辑是：<strong>“用过去建立的地图，来对齐现在的观测。”</strong></p> <ul> <li> <strong>3D 信息（来自过去/不同时刻）：</strong> 追踪线程使用的 3D 点云是由“建图线程（Mapping）”提供的。这些 3D 点是根据<strong>之前的一系列时刻</strong>（滑动窗口内的历史帧）计算并累积出来的局部地图。</li> <li> <strong>2D 信息（来自当前时刻）：</strong> 追踪线程使用的是<strong>当前最新时刻</strong>产生的事件流（并转化成了 OS-TS 时间表面）。</li> </ul> <p><strong>结论：</strong> 它是将<strong>历史时刻积累的 3D 结构</strong>投影到<strong>当前时刻的 2D 平面</strong>上。如果投影的位置和当前看到的位置重合，就说明位姿估计是准确的。</p> <hr> <h4 id="2-时间上的时空一致性时空配准">2. 时间上的“时空一致性”（时空配准）</h4> <p>ESVO2 之所以强调“时空（Spatio-temporal）”，是因为它不仅仅看空间位置，还考虑了<strong>事件发生的时间戳</strong>。</p> <ul> <li> <strong>Time Surface (TS) 的本质：</strong> 时间表面本身就存储了时间信息（像素值代表该位置最近一次事件发生的时间戳）。</li> <li>配准逻辑： 1. 假设当前时间是 \(t_{now}\)。 <ol> <li>如果 3D 点投影到 \(u\) 位置，而 OS-TS 在 \(u\) 位置记录的事件时间非常接近 \(t_{now}\)，说明这个 3D 点在当前时刻是“活跃”的，配准残差就小。</li> <li>反之，如果该位置没有新事件，或者事件发生的时间很久远，残差就会很大。</li> </ol> </li> </ul> <hr> <h4 id="3-imu-的作用连接不同时刻的纽带">3. IMU 的作用：连接不同时刻的纽带</h4> <p>IMU 预积分在这里扮演了极其重要的角色：</p> <ul> <li>它利用<strong>两个时刻之间</strong>的高频惯性数据，预测出从上一时刻到当前时刻的相对位姿变化。</li> <li>这个预测值作为初值，告诉追踪线程：“根据 IMU 估计，那些 3D 点现在应该出现在这个位置。” 然后视觉配准再在这个基础上进行微调。</li> </ul> <h3 id="跨时刻的配准">“跨时刻”的配准</h3> <p>“跨时刻”的配准（通常指 SLAM 或视觉里程计中的位姿估计）确实是通过<strong>雅可比矩阵（Jacobian）</strong>来建立误差与位姿增量之间的线性关系，进而通过迭代优化的方式更新位姿的。</p> <p>由于位姿（旋转 + 平移）所在的空间（如 \(SE(3)\)）并不是一个欧几里得空间（你不能简单地把两个旋转矩阵相加），数学上通常会引入<strong>李群（Lie Group）</strong>和<strong>李代数（Lie Algebra）</strong>来处理求导问题。</p> <p>以下是这一过程的数学推导核心步骤：</p> <hr> <h4 id="1-定义误差函数residual">1. 定义误差函数（Residual）</h4> <p>假设在 \(t\) 时刻，我们观察到一个空间点 \(P\)。在 \(t+1\) 时刻，相机的位姿变为 \(T\)（包含旋转 \(R\) 和平移 \(t\)）。该点在当前相机坐标系下的投影预测值为：</p> \[\hat{z} = h(T, P)\] <p>其中 \(h\) 是投影函数。如果实际观测到的特征点坐标是 \(z\)，那么<strong>误差（残差）</strong>定义为：</p> \[e(T) = z - h(T, P)\] <h4 id="2-引入扰动模型perturbation-model">2. 引入扰动模型（Perturbation Model）</h4> <p>因为直接对旋转矩阵 \(R\) 求导非常复杂且必须保持正交性约束，我们通常给当前的位姿 \(T\) 左乘一个微小的扰动 \(\Delta T\)。</p> <p>在李代数上，这个扰动可以用一个 6 维向量 \(\xi = [\rho, \phi]^T\) 表示（前三维为平移扰动，后三维为旋转扰动）：</p> \[T_{new} = \exp(\xi^{\wedge}) \cdot T_{old}\] <h4 id="3-利用雅可比矩阵进行线性化">3. 利用雅可比矩阵进行线性化</h4> <p>我们要计算的是：当位姿发生微小变化 \(\xi\) 时，误差 \(e\) 发生了多少变化？</p> <p>利用泰勒展开：</p> \[e(T_{new}) = e(\exp(\xi^{\wedge})T_{old}) \approx e(T_{old}) + \frac{\partial e}{\partial \xi} \xi\] <p>这里的 \(J = \frac{\partial e}{\partial \xi}\) 就是你所说的雅可比矩阵。</p> <h4 id="4-雅可比矩阵的具体分解">4. 雅可比矩阵的具体分解</h4> <p>根据链式法则，这个雅可比矩阵通常可以拆解为两部分：</p> \[J = \frac{\partial e}{\partial P'} \cdot \frac{\partial P'}{\partial \xi}\] <ul> <li> <p><strong>第一部分 \(\frac{\partial e}{\partial P'}\)</strong>：像素误差对空间点坐标（在相机坐标系下）的导数。这取决于相机的内参模型（如针孔模型）。</p> </li> <li> <p>第二部分 \(\frac{\partial P'}{\partial \xi}\)：变换后的空间点坐标对位姿扰动的导数。在 \(SE(3)\) 下，对于点 \(P' = [X, Y, Z]^T\)，其推导结果通常是一个 \(3 \times 6\) 的矩阵：</p> \[\frac{\partial P'}{\partial \xi} = \begin{bmatrix} I &amp; -P'^{\wedge} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; Z &amp; -Y \\ 0 &amp; 1 &amp; 0 &amp; -Z &amp; 0 &amp; X \\ 0 &amp; 0 &amp; 1 &amp; Y &amp; -X &amp; 0 \end{bmatrix}\] </li> </ul> <h4 id="5-求解与位姿更新">5. 求解与位姿更新</h4> <p>为了使误差最小化（最小二乘问题），我们构建高斯-牛顿（Gauss-Newton）方程：</p> \[(J^T J) \Delta \xi = -J^T e\] <ul> <li> <strong>\(H = J^T J\)</strong>：近似海森矩阵（Hessian）。</li> <li> <strong>\(\Delta \xi\)</strong>：计算出的最优位姿增量。</li> </ul> <p>更新步：</p> <p>得到 \(\Delta \xi\) 后，我们将其映射回李群，更新当前的位姿估计：</p> \[T \leftarrow \exp(\Delta \xi^{\wedge}) \cdot T\] <p>通过不断重复上述过程（线性化 -&gt; 求解 -&gt; 更新），位姿会逐渐收敛到能够使跨时刻特征点对齐的最优值。</p> <h3 id="localization-与-mapping并行且循环的工作流">Localization 与 Mapping并行且循环的工作流</h3> <p>在 ESVO2 的追踪线程（Localization）中，<strong>它同时利用了“跨时刻”的 3D 信息和“当前时刻”的 2D 信息。</strong></p> <h4 id="1-追踪线程localization利用过去引导现在">1. 追踪线程（Localization）：利用“过去”引导“现在”</h4> <ul> <li> <strong>正确性确认</strong>：是的。追踪线程是第一步。</li> <li> <strong>逻辑</strong>： <ul> <li> <strong>输入</strong>：当前的 2D 信息（OS-TS 时间表面）+ 之前的 Local 3D Map。</li> <li> <strong>过程</strong>：它执行 3D-2D 的时空配准。简单说，就是把已经建好的 3D 地图点投影到当前的 2D 画面上，看对不对得上。</li> <li> <strong>结果</strong>：计算出当前最准确的 <strong>Camera Pose</strong>（相机位姿）。IMU 在这里提供了一个非常关键的初始预测位姿，使得追踪在剧烈运动时不会丢。</li> </ul> </li> </ul> <h4 id="2-建图线程mapping利用现在更新未来">2. 建图线程（Mapping）：利用“现在”更新“未来”</h4> <ul> <li> <strong>正确性确认</strong>：是的。获得当前位姿后，紧接着（或并行）执行 Mapping。</li> <li> <strong>逻辑</strong>： <ul> <li> <strong>输入</strong>：当前的 Camera Pose + 当前的 2D 信息（左右目事件流）。</li> <li> <strong>Temporal Stereo 的作用</strong>：正如你所说，Temporal Stereo 需要知道相机的运动轨迹。它利用刚算出来的 <strong>Current Pose</strong> 结合历史位姿，计算出相机在移动过程中产生的“时间视差”。</li> <li> <strong>结果</strong>：结合 <strong>Static Stereo</strong>（左右目即时视差）和 <strong>Temporal Stereo</strong>，生成新的 3D 点，并更新 <strong>Local 3D Map</strong>。</li> </ul> </li> </ul> <h4 id="3-为什么这个顺序很重要">3. 为什么这个顺序很重要？</h4> <p>这个循环构成了一个典型的自洽系统：</p> <ol> <li> <strong>没有 Pose，无法建图</strong>：尤其是 Temporal Stereo，必须精确知道相机从 A 点挪到了 B 点，才能根据像素的移动反推深度。</li> <li> <strong>没有 Map，无法追踪</strong>：追踪线程必须有一个“参照物”（3D Map），才能知道当前看到的 2D 图像代表自己在空间中的什么位置。</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/optical_flow/">Optical_Flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Teaching-Tailored-to-Talent/">Teaching Tailored To Talent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/dino/">DINO 如何用于密集预测</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/3dtasks/">3D 任务：SFM, MVS, NVS, VO, VIO, SLAM</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/">Matplotlib输出中文</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Erkang Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-optical-flow",title:"Optical_Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2026/optical_flow/"}},{id:"post-teaching-tailored-to-talent",title:"Teaching Tailored To Talent",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2026/Teaching-Tailored-to-Talent/"}},{id:"post-vins-and-esvo2",title:"Vins And Esvo2",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/VINS-and-ESVO2/"}},{id:"post-dino-\u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",title:"DINO \u5982\u4f55\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b",description:"\u5982\u4f55\u4f7f\u7528DINO, DPT, RoPE",section:"Posts",handler:()=>{window.location.href="/blog/2025/dino/"}},{id:"post-3d-\u4efb\u52a1-sfm-mvs-nvs-vo-vio-slam",title:"3D \u4efb\u52a1\uff1aSFM, MVS, NVS, VO, VIO, SLAM",description:"\u591a\u89c6\u89d2\u51e0\u4f55\uff0c\u65b0\u89c6\u89d2\u751f\u6210",section:"Posts",handler:()=>{window.location.href="/blog/2025/3dtasks/"}},{id:"post-matplotlib\u8f93\u51fa\u4e2d\u6587",title:"Matplotlib\u8f93\u51fa\u4e2d\u6587",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"}},{id:"post-mamba",title:"Mamba",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/mamba/"}},{id:"post-flow",title:"Flow",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/flow/"}},{id:"post-diffusion",title:"Diffusion",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-nerf-and-gaussian-splatting",title:"NeRF and Gaussian Splatting",description:"Gaussian Splatting, NeRF, Alpha-blending, Point-Based Rendering, Jacobian",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussianSplatting/"}},{id:"post-colmap",title:"COLMAP",description:"SFM",section:"Posts",handler:()=>{window.location.href="/blog/2025/sfm-mvs-rendering/"}},{id:"post-\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",title:"\u51b2\u6fc0\u4fe1\u53f7\u7684\u5c3a\u5ea6\u53d8\u6362",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/impulse/"}},{id:"post-\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",title:"\u4fe1\u53f7\u4e0e\u7cfb\u7edf\u8bfe\u7a0b\u7684\u6570\u5b66\u77e5\u8bc6",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2025/math/"}},{id:"post-fisheye",title:"Fisheye",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/fisheye/"}},{id:"post-montecarlo",title:"Montecarlo",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/MonteCarlo/"}},{id:"post-\u5c0f\u6ce2\u5206\u6790",title:"\u5c0f\u6ce2\u5206\u6790",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/"}},{id:"post-lifeifei",title:"Lifeifei",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Lifeifei/"}},{id:"post-orgmode",title:"Orgmode",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/orgmode/"}},{id:"post-python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",title:"Python\u8fd0\u884c\u811a\u672c\u6216\u6a21\u5757",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Python%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%88%96%E6%A8%A1%E5%9D%97/"}},{id:"post-proximal",title:"Proximal",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/proximal/"}},{id:"post-visualtransformer",title:"Visualtransformer",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/VisualTransformer/"}},{id:"post-pytorch-note",title:"Pytorch Note",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Pytorch-note/"}},{id:"post-android-opencv-ndk",title:"Android Opencv Ndk",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/android-opencv-ndk/"}},{id:"post-repvgg",title:"Repvgg",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/RepVGG/"}},{id:"post-popularlibrary",title:"Popularlibrary",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/popularLibrary/"}},{id:"post-\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",title:"\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/%E5%85%88%E8%BF%9B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}},{id:"post-one-stage-detection",title:"One Stage Detection",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/one-stage-detection/"}},{id:"post-polarization",title:"Polarization",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/polarization/"}},{id:"post-vae-vqvae-vagan",title:"Vae Vqvae Vagan",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/vae-vqvae-vagan/"}},{id:"post-multiple-object-tracking",title:"Multiple Object Tracking",description:"\u7b80\u8981\u4ecb\u7ecd",section:"Posts",handler:()=>{window.location.href="/blog/2024/Multiple-Object-Tracking/"}},{id:"post-\u77e9\u9635",title:"\u77e9\u9635",description:"Matrix",section:"Posts",handler:()=>{window.location.href="/blog/2024/Matrix/"}},{id:"post-typora-and-jekyll",title:"Typora and Jekyll",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/images/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%65%6B%78%6D@%71%71.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hWo1RTsAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
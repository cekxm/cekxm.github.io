<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cekxm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cekxm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-01T08:50:34+00:00</updated><id>https://cekxm.github.io/feed.xml</id><title type="html">计算机视觉</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Teaching Tailored To Talent</title><link href="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/" rel="alternate" type="text/html" title="Teaching Tailored To Talent"/><published>2026-01-01T01:20:30+00:00</published><updated>2026-01-01T01:20:30+00:00</updated><id>https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/"><![CDATA[<p><img src="/images/2026-01-01-Teaching-Tailored-to-Talent/image-20260101091803120.png" alt="image-20260101091803120" class="img-fluid" data-zoomable=""/></p> <h2 id="介绍">介绍</h2> <p>这篇论文介绍了一种名为 <strong>\(T^3\)-DiffWeather</strong> 的新型图像恢复框架，专门用于解决复杂和多变的恶劣天气条件（如雨、雪、雾等）下的图像修复问题。</p> <p>其核心理念是<strong>“因材施教”（Teaching Tailored to Talent）</strong>，通过结合 <strong>Prompt Learning（提示学习）</strong> 和 <strong>Diffusion Model（扩散模型）</strong>，使网络能够根据不同的天气退化情况动态地调整修复策略。</p> <p>以下是该方法的主要组成部分：</p> <h3 id="1-核心管线预测退化残差-degradation-residual">1. 核心管线：预测退化残差 (Degradation Residual)</h3> <p>不同于传统的扩散模型直接从噪声中恢复清晰图像，该论文将扩散模型的目标转向<strong>预测退化残差 \(r_d\)</strong>（即退化图像与清晰图像之间的差值） 3333。这种设计能够更清晰地表示退化特征，从而引导扩散过程更精准地重建背景。</p> <h3 id="2-提示词池-prompt-pool--针对天气退化">2. 提示词池 (Prompt Pool) —— 针对天气退化</h3> <p>为了应对现实世界中不可预测的天气组合，作者设计了一个<strong>提示词池 (Prompt Pool)</strong> ：</p> <ul> <li><strong>自主构建：</strong> 网络能根据输入图像的退化残差，从提示词池中自动选择并组合最相关的“子提示词”（Sub-prompts），构建出针对特定样本的“天气提示词”（Weather-prompts）。</li> <li><strong>灵活性：</strong> 这种方式通过共享子提示词来捕捉天气的相似性（如雾气和低对比度），同时利用独立的子提示词来区分不同天气的独特性。</li> </ul> <h3 id="3-depth-anything-约束的通用提示词--针对场景建模">3. Depth-Anything 约束的通用提示词 —— 针对场景建模</h3> <p>作者观察到，尽管天气千变万化，但被遮挡的图像背景场景往往具有共同特征。</p> <ul> <li><strong>通用提示词 (General Prompts)：</strong> 专门用于捕捉背景场景的共同属性，为扩散过程提供场景级约束。</li> <li><strong>Depth-Anything 约束：</strong> 论文首次提出利用预训练的 <strong>Depth-Anything</strong> 模型提取的鲁棒特征来引导这些通用提示词 10101010。由于 Depth-Anything 模型在极端天气下仍能保持极高的背景鲁棒性，它能提供准确的场景先验，使修复过程不受天气干扰。</li> </ul> <h3 id="4-对比提示词损失-contrastive-prompt-loss">4. 对比提示词损失 (Contrastive Prompt Loss)</h3> <p>为了确保上述两类提示词（天气提示词和场景通用提示词）能够各司其职，作者引入了<strong>对比提示词损失</strong>：</p> <ul> <li><strong>相互推开：</strong> 将针对天气的提示词和针对场景的提示词视为负样本对，通过对比学习增强各自的表征能力。</li> <li><strong>特征拉近：</strong> 引导提示词与其对应的特征嵌入（如 Depth-Anything 特征）在潜层空间中更接近。</li> </ul> <h3 id="5-高效的推理性能">5. 高效的推理性能</h3> <p>由于采用了精准的提示词条件引导和残差预测策略，\(T^3\)-DiffWeather 的效率极高：</p> <ul> <li><strong>采样步数：</strong> 仅需 <strong>2 步</strong> 采样即可达到优秀性能（相比之下，之前的 Weather Diffusion 模型通常需要更多步骤）。</li> <li><strong>计算开销：</strong> 推理时的计算复杂度仅为目前最先进（SOTA）方法的几十分之一。</li> </ul> <p><strong>总结：</strong> 该论文通过“天气提示词池”和“深度约束的背景提示词”实现了对复杂天气退化的解耦处理，在显著提升修复质量的同时，大幅降低了扩散模型的推理成本。</p> <h2 id="promt-的嵌入">Promt 的嵌入</h2> <p>根据论文中的描述，\(\hat{\mathcal{F}}_e\) 的生成和使用方式如下：</p> <h3 id="1-hatmathcalf_e-的生成过程">1. \(\hat{\mathcal{F}}_e\) 的生成过程</h3> <p>\(\hat{\mathcal{F}}_e\) 是通过<strong>交叉注意力（Cross-Attention）</strong>机制将两种不同类型的 Prompt 逐步嵌入到扩散网络（Diffusion Network）的潜层（Latent Layer）中得到的 111111：</p> <ul> <li><strong>第一步：</strong> 将潜层的特征嵌入 \(\mathcal{F}_e\) 作为 Query，与由<strong>天气提示词（Weather-Prompts）</strong> \(\mathcal{P}_w\) 生成的 Key 和 Value 进行注意力计算，得到中间特征 \(\mathcal{F}_e^{\prime}\) 。</li> <li><strong>第二步：</strong> 将 \(\mathcal{F}_e^{\prime}\) 作为 Query，与由<strong>通用提示词（General Prompts）</strong> \(\mathcal{P}_{gd}\) 生成的 Key 和 Value 再次进行注意力计算，最终输出 \(\hat{\mathcal{F}}_e\) 。</li> </ul> <h3 id="2-hatmathcalf_e-的最终用途">2. \(\hat{\mathcal{F}}_e\) 的最终用途</h3> <p>\(\hat{\mathcal{F}}_e\) 最终被用作<strong>扩散模型去噪过程中的核心条件（Condition）</strong>，具体体现在以下几个方面：</p> <ul> <li><strong>作为信息丰富的引导条件：</strong> 论文指出，\(\hat{\mathcal{F}}_e\) 承载了来自提示词池（Prompt Pool）的退化特征信息以及来自 Depth-Anything 约束的场景背景信息。这些信息共同构成了公式（1）中的条件 \(c\)，用于引导扩散模型从噪声中准确恢复出图像。</li> <li><strong>指导退化残差（Degradation Residual）的重建：</strong> 该论文将扩散模型的目标从直接生成清晰图像转变为生成“退化残差” \(r_d\)（即退化图像与清晰图像之差）。\(\hat{\mathcal{F}}_e\) 作为潜层特征，直接参与到这个残差的去噪重建过程中。</li> <li><strong>实现“因材施教”的恢复：</strong> 由于 \(\hat{\mathcal{F}}_e\) 包含了针对具体样本自适应选择的天气属性和稳健的场景先验，它使得扩散模型能够根据输入图像的具体退化类型（如雨、雪、雾的组合）进行针对性的修复。</li> </ul> <p>在公式（2）中，<strong>\(F_e\) 的原始信息确实通过残差连接（或类似的加和操作）被保留了下来</strong>，\(\hat{\mathcal{F}}_e\) 并不是简单地“扔掉”了 \(F_e\)，而是在 \(F_e\) 的基础上进行了<strong>信息增强</strong> 。</p> <p>以下是基于论文内容及引用 [67]（Stable Diffusion / LDM 架构）的详细确认：</p> <h3 id="f_e-的信息依然存在">\(F_e\) 的信息依然存在</h3> <p>在该论文中，作者明确指出其交叉注意力机制<strong>类似于 Stable Diffusion [67] 中的文本嵌入方式</strong> 22。在 SD 的标准实现中，潜层特征 \(F_e\) 的使用遵循以下逻辑：</p> <ul> <li><strong>计算逻辑：</strong> \(F_e\) 作为 <strong>Query (Q)</strong> 输入到 Attention 模块中。</li> <li><strong>残差更新：</strong> 最终传给下一层的特征通常遵循公式：\(\text{Output} = F_e + \text{Attention}(F_e, \text{Prompt})\)。</li> <li><strong>物理意义：</strong> 这意味着 \(\hat{\mathcal{F}}_e\) 实际上是“<strong>原始图像特征 \(F_e\) + 提示词引导的修正/补充信息</strong>”。\(F_e\) 提供了场景的基础结构和纹理，而 Prompt 信息负责告诉网络哪些部分需要针对天气退化进行调整。</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Vins And Esvo2</title><link href="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/" rel="alternate" type="text/html" title="Vins And Esvo2"/><published>2025-12-31T00:19:46+00:00</published><updated>2025-12-31T00:19:46+00:00</updated><id>https://cekxm.github.io/blog/2025/VINS-and-ESVO2</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/"><![CDATA[<h2 id="vins">VINS</h2> <h3 id="1-系统架构通用因子框架">1. 系统架构：通用因子框架</h3> <p>该方法的核心思想是将每种传感器都视为框架中的一个<strong>通用因子（Factor）</strong> 。</p> <ul> <li><strong>多传感器支持</strong>：框架支持多种传感器组合，如单目相机+IMU、双目相机、以及双目相机+IMU。</li> <li><strong>因子图构建</strong>：共享公共状态变量的因子会被累加在一起，构建一个位姿图（Pose Graph）优化问题 。</li> <li><strong>鲁棒性</strong>：由于不依赖特定传感器，系统可以轻松处理传感器失效的情况，通过移除失效因子的方式快速切换传感器组合。</li> </ul> <p>该方法不是学习类的方法。</p> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251226153814897.png" alt="image-20251226153814897" class="img-fluid"/></p> <p>系统通过一个滑动窗口（Sliding Window）来维护待优化的状态变量。以下是该方法中<strong>状态向量（State Vector）</strong>的详细构成：</p> <h4 id="完整的状态向量">完整的状态向量</h4> <p>在滑动窗口优化中，总的状态向量 \(\mathcal{X}\) 被定义为窗口内所有帧的状态、外参以及特征点深度的集合：</p> \[\mathcal{X} = [\mathbf{x}_m, \mathbf{x}_{m+1}, \dots, \mathbf{x}_n, \mathbf{x}_c^b, \lambda_1, \lambda_2, \dots, \lambda_l]\] <p>其中：</p> <ul> <li>\(m, n\) 是滑动窗口的起始帧和结束帧。</li> <li>\(\mathbf{x}_i\) 是第 \(i\) 帧对应的 <strong>IMU 核心状态</strong>。</li> <li>\(\mathbf{x}_c^b\) 是相机与IMU之间的 <strong>外参（Extrinsic Parameters）</strong>。</li> <li>\(\lambda_l\) 是第 \(l\) 个视觉特征点的 <strong>逆深度（Inverse Depth）</strong>。</li> </ul> <p><strong>和论文表示不太一样，但实际是一样的。这边body frame 的位姿状态放在IMU里面了。</strong></p> <hr/> <h5 id="具体的-imu-状态-mathbfx_i">具体的 IMU 状态 (\(\mathbf{x}_i\))</h5> <p>对于窗口中的每一帧 \(i\)，其核心状态变量包含 15 个维度：</p> \[\mathbf{x}_i = [\mathbf{p}_{b_i}^w, \mathbf{v}_{b_i}^w, \mathbf{q}_{b_i}^w, \mathbf{b}_a, \mathbf{b}_g]\] <ul> <li><strong>\(\mathbf{p}_{b_i}^w\) (3D Position)</strong>：IMU 坐标系在世界坐标系下的位置。</li> <li><strong>\(\mathbf{v}_{b_i}^w\) (3D Velocity)</strong>：IMU 在世界坐标系下的速度。</li> <li><strong>\(\mathbf{q}_{b_i}^w\) (Quaternion/Rotation)</strong>：从 IMU 坐标系到世界坐标系的旋转（通常用四元数表示 Hamilton 形式，或旋转矩阵）。</li> <li><strong>\(\mathbf{b}_a\) (Accelerometer Bias)</strong>：加速度计的零偏。</li> <li><strong>\(\mathbf{b}_g\) (Gyroscope Bias)</strong>：陀螺仪的零偏。</li> </ul> <hr/> <h5 id="辅助状态变量">辅助状态变量</h5> <p>为了保证系统的通用性和精度，论文还包括了以下状态：</p> <ul> <li> <p>传感器外参 (\(\mathbf{x}_c^b\))：</p> <p>包含从相机到 IMU 坐标系的变换 \([\mathbf{p}_c^b, \mathbf{q}_c^b]\)。在系统在线运行时，这些外参也可以被放入优化器进行实时修正。</p> </li> <li> <p>视觉特征点状态 (\(\lambda_l\))：</p> <p>VINS 采用<strong>逆深度（Inverse Depth）</strong>作为特征点的参数化方式。相比于直接使用 3D 坐标 \((x, y, z)\)，逆深度可以更好地处理距离相机非常远的特征点，且其分布更接近高斯分布，有助于数值优化的收敛。</p> </li> </ul> <h4 id="因子">因子</h4> <p>每一个方框是一个 factor，也就是一次测量。更细致的话</p> <ul> <li>相机：一个测量对应一个特征点在该时刻的位置。特征点使用KLT进行跟踪，从之前 \(l\) 时刻到当前 \(t\) 时刻有对应关系，这两个时刻在图像上的位置有约束。</li> <li>如果双目相机，则特征点在左边和右边的位置上，也有约束。</li> <li>IMU: 基于预积分结果的相邻关键帧间的运动约束残差。</li> </ul> <h3 id="2-代价函数-cost-function">2. 代价函数 (Cost Function)</h3> <p>该系统将状态估计建模为一个<strong>最大似然估计（MLE）</strong>问题。在假设测量噪声符合高斯分布的条件下，MLE被转化为一个非线性最小二乘问题，即最小化所有传感器测量残差的加权平方和。</p> <p>在滑动窗口优化中，总代价函数（MAP 形式）定义如下：</p> \[\mathcal{X}_{m:n}^* = \arg \min_{\mathcal{X}_{m:n}} \left\{ \sum_{t=m}^{n} \sum_{k \in S} \\mid z_t^k - h_t^k(\mathcal{X}_{m:n}) \\mid _{\Omega_t^k}^2 + \\mid H_p \delta \mathcal{X}_{m:n} - b_p \\mid ^2 \right\}\] <p>该代价函数主要由以下三部分组成：</p> <ul> <li><strong>视觉因子残差（Camera Factor）</strong>：特征点在不同帧之间的重投影误差 。</li> <li><strong>IMU 因子残差（IMU Factor）</strong>：基于预积分结果的相邻关键帧间的运动约束残差。</li> <li><strong>边缘化先验项（Prior Term）</strong>：由滑动窗口中被移除（边缘化）的旧状态所转换而来的约束信息，用于保留历史观测对当前状态的影响。</li> </ul> <h3 id="3-光束法平差-bundle-adjustment-ba">3. 光束法平差 (Bundle Adjustment, BA)</h3> <p>论文将状态估计过程称为<strong>基于滑动窗口的光束法平差</strong>。</p> <ul> <li><strong>基本原理</strong>：BA通过同时优化相机位姿、IMU状态（速度、偏置）以及特征点的逆深度，使得所有观测残差最小化。</li> <li><strong>求解方式</strong>： <ul> <li><strong>线性化</strong>：在当前估计值附近对代价函数进行一阶泰勒展开，将其转化为线性最小二乘问题。</li> <li><strong>迭代优化</strong>：采用 <strong>Gauss-Newton</strong> 或 <strong>Levenberg-Marquardt</strong> 方法进行多次迭代直至收敛。系统具体使用了 <strong>Ceres Solver</strong> 进行高效求解。</li> </ul> </li> <li><strong>计算效率控制</strong>： <ul> <li><strong>滑动窗口（Sliding Window）</strong>：为了维持实时性，优化只在固定大小的窗口（通常是10帧左右）内进行，而不是处理整个轨迹。</li> <li><strong>边缘化（Marginalization）</strong>：当新帧加入导致窗口满时，利用<strong>舒尔补（Schur Complement）</strong>将最老的帧移除，并将其携带的信息转化为当前窗口内状态的先验分布。</li> </ul> </li> </ul> <p>这种基于优化的方法相比于传统的滤波器（如EKF）具有更高的精度，因为它可以在非线性空间内进行多次迭代更新，并能更好地处理传感器之间的时钟同步偏差。</p> <h2 id="esvo2">ESVO2</h2> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251227073033444.png" alt="image-20251227073033444" class="img-fluid"/></p> <p><strong>ESVO2</strong> 是由湖南大学（Yi Zhou 教授团队）与香港科技大学（Shaojie Shen 教授团队）等合作开发的一个<strong>实时、紧耦合的双目事件相机视觉惯性里程计系统</strong> 。</p> <h3 id="概述">概述</h3> <p>相比前代系统，ESVO2 引入了以下四大创新 ：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：提出了一种新型的类图像事件表示方法，能够根据事件的局部动态自动调整累积时间。这使得系统能高效提取边缘轮廓点，而不受运动速度变化的影响。</li> <li><strong>引入 IMU 预积分</strong>：通过将 IMU 测量值作为运动先验，解决了纯视觉追踪在特定旋转维度（如 Pitch 和 Yaw）上的简并（Degeneracy）问题 。</li> <li><strong>紧耦合后端优化</strong>：设计了一个精简的后端，专门优化线性速度和 IMU 偏置（Bias），从而抑制轨迹漂移，确保全局一致性。</li> <li><strong>增强的建图模块</strong>：结合了“时间同步双目（Temporal Stereo）”和“静态双目（Static Stereo）”配置，并引入快速块匹配方案，显著提升了深度图的完整性和局部平滑度。</li> </ul> <h4 id="系统架构">系统架构</h4> <p>ESVO2 采用并行设计，由四个独立线程组成：</p> <ol> <li><strong>预处理线程</strong>：负责事件的自适应累积及时间表面（Time Surface）的更新。</li> <li><strong>追踪线程（Localization）</strong>：执行 3D-2D 时空配准，利用无偏移平滑时间表面（OS-TS）进行位姿估计。</li> <li><strong>建图线程（Mapping）</strong>：实时恢复半稠密深度图并维护局部 3D 地图。</li> <li><strong>后端线程（Back-end）</strong>：进行滑动窗口优化，不断更新速度和 IMU 参数。</li> </ol> <h3 id="mapping">Mapping</h3> <h4 id="temporal-stereo时间双目"><strong>Temporal Stereo（时间双目）</strong></h4> <p>是一种核心的深度估计技术。它通过结合<strong>时间维度</strong>（单相机的运动）和<strong>空间维度</strong>（双相机的基线）来提高建图的鲁棒性。</p> <h5 id="1-定义与核心思想">1. 定义与核心思想</h5> <p><strong>Temporal Stereo</strong> 指的是利用<strong>单只相机在不同时刻（即随时间位移）</strong>观测到的视觉信息来估计物体深度的技术 。</p> <ul> <li> <p><strong>对比 Static Stereo（静态双目）</strong>：静态双目是利用左右两个相机在<strong>同一时刻</strong>的视图差异（视差）来计算深度。</p> </li> <li> <p><strong>结合的意义</strong>：在 ESVO2 中，系统并不只依赖左右相机的瞬时匹配，而是将相机的<strong>运动位移</strong>也作为一种“虚拟基线”。</p> </li> </ul> <h5 id="2-工作原理">2. 工作原理</h5> <p>在 ESVO2 的建图模块中，Temporal Stereo 的具体运作方式如下：</p> <ul> <li> <p><strong>时空观测一致性</strong>：当机器人移动时，同一个空间点会在不同时间点被相机捕获。系统会寻找当前事件流与该相机在不久前的历史观测之间的相关性 。</p> </li> <li> <p><strong>代价合并（Cost Fusion）</strong>：ESVO2 将 Temporal Stereo 和 Static Stereo 的匹配代价（Matching Cost）进行融合 。</p> </li> <li>如果运动方向有利于时间双目（例如向前运动，提供了纵向观测变化），Temporal Stereo 能提供很好的约束。</li> <li>如果左右相机之间的视差更明显，Static Stereo 则占据主导。</li> </ul> <h5 id="3-在-esvo2-中的作用">3. 在 ESVO2 中的作用</h5> <p>引入 Temporal Stereo 对事件相机 SLAM 具有至关重要的作用：</p> <ul> <li> <p><strong>解决观测退化</strong>：事件相机对垂直于其边缘运动的方向敏感。如果机器人仅进行某种特定方向的运动，静态双目可能会失效。Temporal Stereo 增加了额外的观测维度，保证了在 6-DoF（六自由度）复杂运动下的建图完整性 。</p> </li> <li> <p><strong>提高深度图质量</strong>：通过在时间轴上累积观测，系统能够过滤掉瞬时的噪声事件，生成的深度图更加平滑且空洞更少 。</p> </li> <li> <p><strong>快速块匹配</strong>：论文提到在执行 Temporal Stereo 匹配时采用了快速块匹配方案，这使得系统能够在普通 CPU 上实时处理 VGA 分辨率的高频数据 。</p> </li> </ul> <p>在 ESVO2 中，<strong>Temporal Stereo 实际上是将 VIO 系统变成了一个“多视图立体视觉（MVS）”系统</strong>。它不再局限于双目相机那段固定的物理基线，而是将相机的整个运动轨迹都变成了获取几何信息的源泉，从而在各种运动条件下都能输出高精度的半稠密深度图 。</p> <h4 id="fast-static-stereo快速静态双目"><strong>Fast Static Stereo（快速静态双目）</strong></h4> <p><strong>Fast Static Stereo（快速静态双目）</strong> 的目的确实是为了获得三维点的坐标，但它的实现方式和传统的“特征点提取 + 三角化”有所不同。</p> <p>以下是针对您问题的详细解答：</p> <h5 id="1-是为了获得稀疏三维点吗">1. 是为了获得稀疏三维点吗？</h5> <p><strong>是的，但更准确地说是“半稠密（Semi-dense）”的三维点。</strong></p> <ul> <li><strong>非稀疏</strong>：传统的 VO（如 ORB-SLAM）只提取少量的特征点（如角点），产生的点云非常稀疏。</li> <li><strong>非全稠密</strong>：由于事件相机只在亮度变化的地方产生数据（通常是物体的边缘），所以它无法像结构光或 LiDAR 那样获得物体的完整表面。</li> <li><strong>半稠密</strong>：ESVO2 的 Fast Static Stereo 针对事件累积产生的<strong>边缘轮廓</strong>进行匹配，因此得到的是物体轮廓处的 3D 点云，这比传统的稀疏点云要密得多，足以辅助避障和局部建图。</li> </ul> <h5 id="2-只需要做三角化吗">2. 只需要做三角化吗？</h5> <p><strong>不完全是。</strong> 虽然双目外参（变换矩阵）已经离线标定好了，但“三角化”只是最后一步。最难、最核心的步骤是<strong>数据关联（Data Association / Correspondence）</strong>，即：<strong>左目里的这个事件点，对应右目里的哪个像素？</strong></p> <p>在 ESVO2 的 Fast Static Stereo 中，这个过程包含以下关键步骤：</p> <ul> <li><strong>极线搜索 (Epipolar Search)</strong>：利用已标定的外参，在右目的极线上寻找匹配点。</li> <li><strong>块匹配 (Block Matching)</strong>：由于事件数据不是灰度图，ESVO2 使用了 <strong>Time Surface（时间表面）</strong> 或像素块来进行相似度比较。</li> <li><strong>时空一致性检查</strong>：系统会检查左右目事件在时间上的同步性（是否在极短的时间差内发生）以及空间上的几何一致性。</li> <li><strong>三角化 (Triangulation)</strong>：只有当匹配的代价（Cost）足够低且置信度高时，系统才会根据已知的双目基线进行几何三角化，计算出深度 \(Z\)。</li> </ul> <h5 id="3-esvo2-fast-的体现">3. ESVO2 “Fast” 的体现</h5> <p>之所以称为 <strong>Fast</strong>，是因为它做了一些专门针对 CPU 实时性的优化：</p> <ol> <li><strong>查找表 (Look-up Table)</strong>：预先计算极线方向，减少运行时的几何计算。</li> <li><strong>跳过非边缘区域</strong>：只对有事件发生的像素点进行匹配，避开大量空白区域。</li> <li><strong>并行化</strong>：利用多线程并行处理左右目的事件流块。</li> </ol> <p>你理解的“通过三角化获得坐标”是最终目的，但 <strong>Fast Static Stereo 的核心在于如何利用标定好的几何关系，在极高频率、高分辨率的事件流中快速找到左右目的匹配对。</strong></p> <p>值得注意的是，ESVO2 还会将 <strong>Static Stereo</strong>（左右目匹配）的结果与 <strong>Temporal Stereo</strong>（单目随时间运动产生的匹配）进行融合，这样即使在双目基线方向运动导致视差较小时，依然能获得准确的深度。</p> <h4 id="轮廓">轮廓</h4> <p>在 ESVO2 论文中，<strong>Fast Static Stereo（快速静态双目）不使用传统的稀疏特征点（如角点），而是直接使用物体的轮廓（Contour Points）进行匹配</strong> 。</p> <p>这种设计是由事件相机的物理特性决定的。以下是具体的实现逻辑：</p> <h5 id="1-为什么选择轮廓而不是特征点">1. 为什么选择轮廓而不是特征点？</h5> <ul> <li><strong>物理机制</strong>：事件相机只在亮度变化时触发信号，因此其天然的输出就是物体的<strong>边缘和轮廓</strong>。</li> <li><strong>鲁棒性</strong>：在高速运动中，传统的特征点（如 Harris 角点）可能因为观察不全而难以追踪 3。直接使用轮廓点能保留更完整的环境几何结构。</li> </ul> <h5 id="2-esvo2-的具体处理方式">2. ESVO2 的具体处理方式</h5> <p>ESVO2 并没有处理所有的事件点，而是通过以下步骤提取<strong>精准的轮廓点</strong>进行匹配：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：系统会根据事件的动态变化（速度快慢），自动决定累积多长时间的事件来生成一张类似图像的“AA图”。</li> <li><strong>轮廓点采样 (Contour-point Sampling)</strong>：在 AA 图上，系统会采样那些代表瞬时边缘的像素点（即轮廓点） 。</li> <li><strong>排除冗余</strong>：相比于直接使用原始事件流，这种方法筛选掉了大量噪声和冗余点，使得输入点更“精简且准确”。</li> </ul> <h5 id="3-fast-static-stereo-的快体现在哪">3. Fast Static Stereo 的“快”体现在哪？</h5> <p>由于处理的是轮廓点，ESVO2 采用了以下策略实现高效匹配：</p> <ul> <li><strong>块匹配 (Block Matching)</strong>：在左右目相机的 AA 图或时间表面（Time Surface）上，沿着极线对这些<strong>采样后的轮廓像素块</strong>进行相似度搜索。</li> <li><strong>取消非线性精化</strong>：论文提到，由于采样后的轮廓点已经非常精确，系统在 ESVO2 中<strong>取消了耗时的子像素级非线性精化步骤</strong>，从而显著提升了速度（在 VGA 分辨率下达到 20Hz 实时性），且精度几乎没有下降。</li> </ul> <p>在 ESVO2 的 <strong>Fast Static Stereo</strong> 模块中，获得 3D 坐标的过程是一个从“事件像素”到“空间点”的几何推导过程。虽然不使用复杂的特征描述子，但它利用了极其严格的<strong>时空约束</strong>。</p> <p>以下是具体的实现步骤：</p> <h5 id="1-极线约束搜索-epipolar-search">1. 极线约束搜索 (Epipolar Search)</h5> <p>假设左目相机在 \(u_L\) 像素处采样了一个轮廓点，由于双目相机的外参（基线 \(b\)、焦距 \(f\)）已预先标定且图像已做过极线校正（Rectification），那么这个点在右目图像中对应的匹配点一定位于<strong>水平的极线</strong>上。</p> <ul> <li><strong>搜索范围</strong>：系统会在右目的极线上，根据设定的最小和最大深度范围，确定一个搜索区间。</li> </ul> <h5 id="2-基于时间表面的匹配-time-surface-matching">2. 基于“时间表面”的匹配 (Time Surface Matching)</h5> <p>这是最关键的一步。因为事件相机没有灰度值，它使用 <strong>Time Surface (TS)</strong> 或 <strong>AA 图</strong>来计算匹配相似度：</p> <ul> <li><strong>提取 Patch</strong>：在左目 \(u_L\) 周围取一个小的像素块（例如 \(5 \times 5\)）。</li> <li><strong>计算相似度</strong>：在右目极线的搜索区间内滑动，通过 <strong>零均值归一化互相关 (ZNCC)</strong> 或类似的度量函数，寻找与左目 Patch 最相似的区域。</li> <li><strong>原理</strong>：虽然没有颜色，但物体轮廓在左右目形成的时间表面形状是非常相似的。</li> </ul> <h5 id="3-概率深度估计-probabilistic-depth-estimation">3. 概率深度估计 (Probabilistic Depth Estimation)</h5> <p>ESVO2 并不只是简单地计算一个视差，而是为每个点维护一个概率分布：</p> <ul> <li> <p><strong>逆深度建模</strong>：系统将深度表示为逆深度（Inverse Depth），并假设其服从高斯分布。</p> </li> <li> <p>代价聚合：当匹配代价最低的点被找到后，通过视差公式计算初始深度：</p> \[Z = \frac{f \cdot b}{d}\] <p>（其中 \(f\) 是焦距，\(b\) 是基线，\(d\) 是左右目像素的位移差/视差）。</p> </li> </ul> <h5 id="4-空间三角化-triangulation">4. 空间三角化 (Triangulation)</h5> <p>一旦确定了左右目的匹配对 \((u_L, v_L)\) 和 \((u_R, v_R)\)，就可以通过标准的<strong>三角化公式</strong>求出该轮廓点在左目相机坐标系下的 3D 坐标 \((X, Y, Z)\)：</p> <ul> <li> \[Z = \frac{f \cdot b}{u_L - u_R}\] </li> <li> \[X = \frac{(u_L - c_x) \cdot Z}{f}\] </li> <li> \[Y = \frac{(v_L - c_y) \cdot Z}{f}\] </li> </ul> <h5 id="5-esvo2-的特殊优化取消非线性精化">5. ESVO2 的特殊优化：取消非线性精化</h5> <p>在早期的 ESVO 版本中，三角化后通常还需要一个耗时的“非线性最小二乘精化”过程来微调 3D 位置。</p> <p>ESVO2 的改进：由于采用了自适应累积 (AA) 提取的轮廓点非常精准，论文提到他们直接跳过了非线性精化步骤。只要匹配代价足够低，就直接输出三角化的结果。这种“直接法”极大地提升了处理 VGA 分辨率数据的速度。</p> <h5 id="总结">总结</h5> <p>简单来说，获得 3D 坐标的流程是：</p> <p>左目采样轮廓点 \(\rightarrow\) 右目极线搜索匹配块 \(\rightarrow\) 计算视差 \(\rightarrow\) 几何三角化 \(\rightarrow\) 得到 3D 坐标。</p> <p>这一过程生成的点云是<strong>半稠密</strong>的，因为它只存在于有光强变化的边缘位置，但对于机器人感知环境（如避障、路径规划）已经足够丰富。</p> <h3 id="localization">Localization</h3> <h4 id="1-空间上的-3d-2d-配准当前时刻-vs-历史时刻">1. 空间上的 3D-2D 配准（当前时刻 vs 历史时刻）</h4> <p>追踪线程的核心逻辑是：<strong>“用过去建立的地图，来对齐现在的观测。”</strong></p> <ul> <li><strong>3D 信息（来自过去/不同时刻）：</strong> 追踪线程使用的 3D 点云是由“建图线程（Mapping）”提供的。这些 3D 点是根据<strong>之前的一系列时刻</strong>（滑动窗口内的历史帧）计算并累积出来的局部地图。</li> <li><strong>2D 信息（来自当前时刻）：</strong> 追踪线程使用的是<strong>当前最新时刻</strong>产生的事件流（并转化成了 OS-TS 时间表面）。</li> </ul> <p><strong>结论：</strong> 它是将<strong>历史时刻积累的 3D 结构</strong>投影到<strong>当前时刻的 2D 平面</strong>上。如果投影的位置和当前看到的位置重合，就说明位姿估计是准确的。</p> <hr/> <h4 id="2-时间上的时空一致性时空配准">2. 时间上的“时空一致性”（时空配准）</h4> <p>ESVO2 之所以强调“时空（Spatio-temporal）”，是因为它不仅仅看空间位置，还考虑了<strong>事件发生的时间戳</strong>。</p> <ul> <li><strong>Time Surface (TS) 的本质：</strong> 时间表面本身就存储了时间信息（像素值代表该位置最近一次事件发生的时间戳）。</li> <li>配准逻辑： 1. 假设当前时间是 \(t_{now}\)。 <ol> <li>如果 3D 点投影到 \(u\) 位置，而 OS-TS 在 \(u\) 位置记录的事件时间非常接近 \(t_{now}\)，说明这个 3D 点在当前时刻是“活跃”的，配准残差就小。</li> <li>反之，如果该位置没有新事件，或者事件发生的时间很久远，残差就会很大。</li> </ol> </li> </ul> <hr/> <h4 id="3-imu-的作用连接不同时刻的纽带">3. IMU 的作用：连接不同时刻的纽带</h4> <p>IMU 预积分在这里扮演了极其重要的角色：</p> <ul> <li>它利用<strong>两个时刻之间</strong>的高频惯性数据，预测出从上一时刻到当前时刻的相对位姿变化。</li> <li>这个预测值作为初值，告诉追踪线程：“根据 IMU 估计，那些 3D 点现在应该出现在这个位置。” 然后视觉配准再在这个基础上进行微调。</li> </ul> <h3 id="跨时刻的配准">“跨时刻”的配准</h3> <p>“跨时刻”的配准（通常指 SLAM 或视觉里程计中的位姿估计）确实是通过<strong>雅可比矩阵（Jacobian）</strong>来建立误差与位姿增量之间的线性关系，进而通过迭代优化的方式更新位姿的。</p> <p>由于位姿（旋转 + 平移）所在的空间（如 \(SE(3)\)）并不是一个欧几里得空间（你不能简单地把两个旋转矩阵相加），数学上通常会引入<strong>李群（Lie Group）</strong>和<strong>李代数（Lie Algebra）</strong>来处理求导问题。</p> <p>以下是这一过程的数学推导核心步骤：</p> <hr/> <h4 id="1-定义误差函数residual">1. 定义误差函数（Residual）</h4> <p>假设在 \(t\) 时刻，我们观察到一个空间点 \(P\)。在 \(t+1\) 时刻，相机的位姿变为 \(T\)（包含旋转 \(R\) 和平移 \(t\)）。该点在当前相机坐标系下的投影预测值为：</p> \[\hat{z} = h(T, P)\] <p>其中 \(h\) 是投影函数。如果实际观测到的特征点坐标是 \(z\)，那么<strong>误差（残差）</strong>定义为：</p> \[e(T) = z - h(T, P)\] <h4 id="2-引入扰动模型perturbation-model">2. 引入扰动模型（Perturbation Model）</h4> <p>因为直接对旋转矩阵 \(R\) 求导非常复杂且必须保持正交性约束，我们通常给当前的位姿 \(T\) 左乘一个微小的扰动 \(\Delta T\)。</p> <p>在李代数上，这个扰动可以用一个 6 维向量 \(\xi = [\rho, \phi]^T\) 表示（前三维为平移扰动，后三维为旋转扰动）：</p> \[T_{new} = \exp(\xi^{\wedge}) \cdot T_{old}\] <h4 id="3-利用雅可比矩阵进行线性化">3. 利用雅可比矩阵进行线性化</h4> <p>我们要计算的是：当位姿发生微小变化 \(\xi\) 时，误差 \(e\) 发生了多少变化？</p> <p>利用泰勒展开：</p> \[e(T_{new}) = e(\exp(\xi^{\wedge})T_{old}) \approx e(T_{old}) + \frac{\partial e}{\partial \xi} \xi\] <p>这里的 \(J = \frac{\partial e}{\partial \xi}\) 就是你所说的雅可比矩阵。</p> <h4 id="4-雅可比矩阵的具体分解">4. 雅可比矩阵的具体分解</h4> <p>根据链式法则，这个雅可比矩阵通常可以拆解为两部分：</p> \[J = \frac{\partial e}{\partial P'} \cdot \frac{\partial P'}{\partial \xi}\] <ul> <li> <p><strong>第一部分 \(\frac{\partial e}{\partial P'}\)</strong>：像素误差对空间点坐标（在相机坐标系下）的导数。这取决于相机的内参模型（如针孔模型）。</p> </li> <li> <p>第二部分 \(\frac{\partial P'}{\partial \xi}\)：变换后的空间点坐标对位姿扰动的导数。在 \(SE(3)\) 下，对于点 \(P' = [X, Y, Z]^T\)，其推导结果通常是一个 \(3 \times 6\) 的矩阵：</p> \[\frac{\partial P'}{\partial \xi} = \begin{bmatrix} I &amp; -P'^{\wedge} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; Z &amp; -Y \\ 0 &amp; 1 &amp; 0 &amp; -Z &amp; 0 &amp; X \\ 0 &amp; 0 &amp; 1 &amp; Y &amp; -X &amp; 0 \end{bmatrix}\] </li> </ul> <h4 id="5-求解与位姿更新">5. 求解与位姿更新</h4> <p>为了使误差最小化（最小二乘问题），我们构建高斯-牛顿（Gauss-Newton）方程：</p> \[(J^T J) \Delta \xi = -J^T e\] <ul> <li><strong>\(H = J^T J\)</strong>：近似海森矩阵（Hessian）。</li> <li><strong>\(\Delta \xi\)</strong>：计算出的最优位姿增量。</li> </ul> <p>更新步：</p> <p>得到 \(\Delta \xi\) 后，我们将其映射回李群，更新当前的位姿估计：</p> \[T \leftarrow \exp(\Delta \xi^{\wedge}) \cdot T\] <p>通过不断重复上述过程（线性化 -&gt; 求解 -&gt; 更新），位姿会逐渐收敛到能够使跨时刻特征点对齐的最优值。</p> <h3 id="localization-与-mapping并行且循环的工作流">Localization 与 Mapping并行且循环的工作流</h3> <p>在 ESVO2 的追踪线程（Localization）中，<strong>它同时利用了“跨时刻”的 3D 信息和“当前时刻”的 2D 信息。</strong></p> <h4 id="1-追踪线程localization利用过去引导现在">1. 追踪线程（Localization）：利用“过去”引导“现在”</h4> <ul> <li><strong>正确性确认</strong>：是的。追踪线程是第一步。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 2D 信息（OS-TS 时间表面）+ 之前的 Local 3D Map。</li> <li><strong>过程</strong>：它执行 3D-2D 的时空配准。简单说，就是把已经建好的 3D 地图点投影到当前的 2D 画面上，看对不对得上。</li> <li><strong>结果</strong>：计算出当前最准确的 <strong>Camera Pose</strong>（相机位姿）。IMU 在这里提供了一个非常关键的初始预测位姿，使得追踪在剧烈运动时不会丢。</li> </ul> </li> </ul> <h4 id="2-建图线程mapping利用现在更新未来">2. 建图线程（Mapping）：利用“现在”更新“未来”</h4> <ul> <li><strong>正确性确认</strong>：是的。获得当前位姿后，紧接着（或并行）执行 Mapping。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 Camera Pose + 当前的 2D 信息（左右目事件流）。</li> <li><strong>Temporal Stereo 的作用</strong>：正如你所说，Temporal Stereo 需要知道相机的运动轨迹。它利用刚算出来的 <strong>Current Pose</strong> 结合历史位姿，计算出相机在移动过程中产生的“时间视差”。</li> <li><strong>结果</strong>：结合 <strong>Static Stereo</strong>（左右目即时视差）和 <strong>Temporal Stereo</strong>，生成新的 3D 点，并更新 <strong>Local 3D Map</strong>。</li> </ul> </li> </ul> <h4 id="3-为什么这个顺序很重要">3. 为什么这个顺序很重要？</h4> <p>这个循环构成了一个典型的自洽系统：</p> <ol> <li><strong>没有 Pose，无法建图</strong>：尤其是 Temporal Stereo，必须精确知道相机从 A 点挪到了 B 点，才能根据像素的移动反推深度。</li> <li><strong>没有 Map，无法追踪</strong>：追踪线程必须有一个“参照物”（3D Map），才能知道当前看到的 2D 图像代表自己在空间中的什么位置。</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">DINO 如何用于密集预测</title><link href="https://cekxm.github.io/blog/2025/dino/" rel="alternate" type="text/html" title="DINO 如何用于密集预测"/><published>2025-12-26T05:54:16+00:00</published><updated>2025-12-26T05:54:16+00:00</updated><id>https://cekxm.github.io/blog/2025/dino</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/dino/"><![CDATA[<h2 id="资料">资料</h2> <ul> <li><a href="https://zhuanlan.zhihu.com/p/1933583851923439816">(3 封私信 / 80 条消息) 万字长文超详解读之DINO全系列—视觉表征对比学习的高峰 - 知乎</a></li> <li><a href="https://zhuanlan.zhihu.com/p/1940400858836742367">(3 封私信 / 80 条消息) 万字长文超详解之DINO-V3（DINO全系列之补充篇） - 知乎</a></li> <li><a href="https://www.youtube.com/watch?v=j2_42Yx_1_w">Inside DINOv2: Architecture Analysis + CIFAR-10 Experiment - YouTube</a></li> <li><a href="https://mashaan14.github.io/YouTube-channel/self_supervised_learning/2025_05_19_SSL">Self-Supervised Learning Review: from SimCLR to DINOv2 | Mashaan blog</a></li> </ul> <blockquote> <p>DINOv3 的发布，标志着计算机视觉进入了类似 NLP 的“GPT-3 时刻”<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p> <p><strong>对于学术界：</strong></p> <ul> <li><strong>Bad News</strong>：传统的“设计一个新 Backbone”、“魔改 Transformer 模块”刷点数的路子越来越窄了。在 7B 模型面前，微小的架构创新几乎没有意义。</li> <li><strong>Good News</strong>：新的研究方向被打开了。 <ul> <li><strong>Post-training</strong>：如何更高效地利用这些冻结特征？</li> <li><strong>多模态对齐</strong>：DINOv3 展示了初步的文本对齐能力，但这方面远未饱和。</li> <li><strong>视频理解</strong>：利用 DINOv3 强大的时序一致性做原生的视频大模型。</li> </ul> </li> </ul> <p><strong>对于工业界/工程师：</strong></p> <ul> <li><strong>这是巨大的利好</strong>。你不再需要收集几十万张标注数据去训练一个分割模型。直接下载 DINOv3 的权重，冻结它，用几百张图训练一个轻量级 Head，你就能得到工业级可用的效果。</li> <li><strong>Deployment</strong>：Meta 提供的蒸馏版小模型（特别是 ViT-Small 和 Base）将是边缘端部署的神器。</li> </ul> <p><strong>DINOv3 并没有让天塌下来，它只是铺平了地基。</strong> 它把提取“好特征”这件最脏最累最费算力的事做完了。现在的我们，可以站在 70 亿参数的肩膀上，去探索视觉智能更上层的逻辑——这何尝不是一种幸运？</p> </blockquote> <h2 id="如何使用-dino">如何使用 DINO</h2> <p>对于 dense prediction，一般要使用多个层的特征，比如 VGGT，以及 dinov3 中的应用部分有提及。</p> <ol> <li><strong>多层特征提取：</strong> 在 VGGT 的实现细节中，为了生成高分辨率的密集输出（如深度图和点图），模型将来自 DINOv2 骨干网络不同阶段的特征提供给 <strong>DPT（密集预测 Transformer）头</strong>进行处理。具体而言，VGGT 会提取 DINOv2（ViT-L/14）中<strong>第 4、11、17 和 23 块（blocks，实际上就是layer）</strong>的令牌（tokens），并将这些中间层的特征输入 DPT 进行上采样。</li> <li><strong>与 DINOv3 结合时的用法：</strong> 在 DINOv3 的后续实验中，研究人员将 VGGT 的图像特征提取器更换为 DINOv3 ViT-L。在这种配置下，他们同样使用了 <strong>4 个中间层特征的拼接（concatenation）</strong> 作为下游模块的输入，而不是仅使用最后一层。实验发现，这种使用多个中间层的方法对 DINOv3 带来了性能提升。</li> </ol> <h2 id="dpt">DPT</h2> <p>R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense prediction,” in <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 12173-12183.</p> <p><img src="/images/2025-12-26-dino/image-20251226103424453.png" alt="image-20251226103424453" class="img-fluid"/></p> <h3 id="encoder">Encoder</h3> <p>DPT共需要4层的特征，如果用ViT/dino，那这四层的尺寸是一样的。</p> <h3 id="reassemble">Reassemble</h3> <p>Read-&gt;Concatenate-&gt;Resample</p> <p>为了更清楚地理解，可以看它在完整流程中的作用：</p> <ol> <li><strong>Read：</strong> 处理 CLS 令牌（这里可能涉及拼接投影等线性运算）。</li> <li><strong>Concatenate：</strong> 将令牌<strong>摆放</strong>回网格位置。</li> <li><strong>Resamples：</strong> 使用 \(1 \times 1\) 和 \(3 \times 3\) 卷积进行通道投影及空间缩放（这里才进行真正的<strong>卷积运算</strong>）。</li> </ol> <p><strong>总结：</strong> Concatenate 阶段就像是<strong>拼图</strong>。Read 操作决定了每一块拼图（token）上的内容，而 Concatenate 只是<strong>按照位置把拼图摆好</strong>。真正的“修图”和“放大”工作是由接下来的 Resamples 卷积层完成的。</p> <h4 id="read">Read</h4> <p>在 DPT（Dense Prediction Transformer）架构中，<strong>Read 操作</strong>是其核心组件“重组操作”（Reassemble）的<strong>第一阶段</strong>。它的主要任务是处理 Vision Transformer (ViT) 输出的特殊令牌，并将令牌序列转换为可以进行空间排列的形式。</p> <p>以下是 Read 操作的详细介绍：</p> <h5 id="1-核心定义与目的">1. 核心定义与目的</h5> <p>在 ViT 中，输出包含 \(N_p\) 个图像块令牌（patch tokens）和 <strong>1 个特殊的“读取令牌”（readout token，通常指 CLS token）</strong>。</p> <ul> <li><strong>输入：</strong> \(N_p + 1\) 个令牌。</li> <li><strong>目的：</strong> 将这 \(N_p + 1\) 个令牌映射回 \(N_p\) 个令牌，以便后续的“拼接操作”（Concatenate）能将它们按照原始图像位置还原成特征图。</li> <li><strong>数学表达式：</strong> \(Read: \mathbb{R}^{(N_p+1) \times D} \to \mathbb{R}^{N_p \times D}\)。</li> </ul> <h5 id="2-三种实现方案">2. 三种实现方案</h5> <p>DPT 论文评估了处理读取令牌（\(t_0\)）与图像块令牌（\(t_1, \dots, t_{N_p}\)）之间关系的三种不同方式：</p> <ul> <li><strong>Readignore（忽略）：</strong> 直接<strong>丢弃读取令牌</strong>，只保留 \(N_p\) 个图像块令牌。这是最简单的方法，即 \(Read_{ignore}(t) = {t_1, \dots, t_{N_p}}\)。</li> <li><strong>Readadd（相加）：</strong> 将读取令牌的信息<strong>加到所有其他令牌上</strong>。即 \(Read_{add}(t) = {t_1 + t_0, \dots, t_{N_p} + t_0}\)。</li> <li><strong>Readproj（投影/默认方案）：</strong> 通过<strong>拼接后投影</strong>的方式融合信息。将读取令牌与每个图像块令牌拼接，然后通过一个线性层（MLP）将维度投影回原始大小 \(D\)。其公式为：\(Read_{proj}(t) = {mlp(cat(t_1, t_0)), \dots, mlp(cat(t_{N_p}, t_0))}\)。</li> </ul> <h5 id="3-性能表现与结论">3. 性能表现与结论</h5> <p>根据来源中的消融实验结果：</p> <ul> <li><strong>Readproj 是默认的最优方案</strong>，在单目深度估计等任务中表现略优于其他方案，因为它能更有效地捕获并分配全局信息。</li> <li>相比之下，<code class="language-plaintext highlighter-rouge">Readadd</code> 的效果甚至差于完全忽略令牌的 <code class="language-plaintext highlighter-rouge">Readignore</code>。</li> </ul> <p><strong>总结：</strong> <strong>Read 操作</strong>就像是一个“令牌筛选与融合器”，它决定了如何将 Transformer 学习到的<strong>全局图像表示（读取令牌）*<em>回馈给各个*</em>局部特征（图像块令牌）</strong>，从而确保在进入后续的卷积解码阶段前，每个像素级别的特征都已融入了全局上下文信息。</p> <h4 id="concatenate">Concatenate</h4> <p>在 DPT（Dense Prediction Transformer）的 <strong>Reassemble</strong> 操作中，<strong>Concatenate（拼接）</strong> 阶段本身<strong>不涉及任何复杂的数学运算或学习参数</strong>，它的本质是一个<strong>形状变换（Reshape/Rearrange）</strong>过程。</p> <h4 id="resample">Resample</h4> <p>这一步才真正涉及空间缩放。</p> <ul> <li><strong>输入：</strong> 空间排列好的特征图，尺寸为 \(\frac{H}{p} \times \frac{W}{p}\)，通道数为 \(D\)。</li> <li><strong>输出：</strong> 缩放后的特征图，尺寸为 \(\frac{H}{s} \times \frac{W}{s}\)，通道数为 \(\hat{D}\)（DPT 默认 \(\hat{D} = 256\)）。</li> </ul> <p>Resample 通过两步卷积运算来完成变换：</p> <ol> <li> <p><strong>通道投影：</strong> 首先使用 <strong>1x1 卷积</strong>。这一步负责将来自不同 Transformer 层的令牌维度（如 ViT-Large 的 1024 维）投影到解码器所需的统一维度 \(\hat{D}\)。</p> </li> <li> <p>空间缩放：</p> <p>随后根据目标缩放比例 \(s\) 与初始图像块大小 \(p\) 的关系，使用</p> <p>3x3 卷积</p> <p>进行调整：</p> <ul> <li><strong>下采样（\(s \ge p\)）：</strong> 使用<strong>步长（strided）为 3x3 的卷积</strong>来降低分辨率。</li> <li><strong>上采样（\(s &lt; p\)）：</strong> 使用<strong>步长为 3x3 的转置卷积（transpose convolution）</strong>来提升分辨率。</li> </ul> </li> </ol> <h3 id="fusion">Fusion</h3> <p>每级上采样2倍。</p> <h2 id="multi-task-image-restoration-guided-by-robust-dino-features">Multi-task image restoration guided by robust DINO features</h2> <p>X. Lin, C. Ren, K. C. Chan, L. Qi, J. Pan, and M. H. Yang, “Multi-task image restoration guided by robust DINO features,” <em>arXiv preprint arXiv:2312.01677</em>, 2023 (v3 revised 2024).</p> <p><img src="/images/2025-12-26-dino/image-20251226133529550.png" alt="image-20251226133529550" class="img-fluid"/></p> <p>其核心思路是：传统的图像恢复模型在任务数量增加时性能会下降，而 DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节，因此可以作为一种<strong>退化无关的表示</strong>来引导恢复过程。</p> <p>以下是 DINO-IR 的具体实现方法和核心组件：</p> <h3 id="1-核心架构与模块">1. 核心架构与模块</h3> <p>DINO-IR 基于 <strong>Restormer</strong> 架构，并集成了以下三个关键组件：</p> <ul> <li>像素-语义融合模块 (PSF, Pixel-Semantic Fusion)： <ul> <li><strong>目的：</strong> 动态融合 DINOv2 不同层级的特征。由于浅层包含像素信息，深层包含语义信息，该模块负责提取并加权这些特征。</li> <li><strong>实现：</strong> 采用<strong>门控网络（Gating Network）*<em>和多个*</em>专家网络（Expert Networks）</strong>。门控网络会根据输入图像自适应地学习浅层、中层和深层特征的权重，将对恢复任务最有益的特征赋予更高的权重进行融合。</li> </ul> </li> <li>DINO-Restore (D-R) 适配与融合模块： <ul> <li><strong>目的：</strong> 将 DINOv2 的特征集成到图像恢复主模型中。</li> <li><strong>实现：</strong> 首先通过适配层调整 PSF 融合特征的通道数和尺度，使其与恢复模型对齐。然后采用<strong>基于自注意力的融合方式</strong>：将适配后的 DINO 特征作为 <strong>Query (Q)</strong>，而将恢复模型的中间特征作为 <strong>Key (K)</strong> 和 <strong>Value (V)</strong>，通过交叉注意力机制实现特征融合。</li> </ul> </li> </ul> <h3 id="2-dino-感知对比损失-dpc-loss">2. DINO 感知对比损失 (DPC Loss)</h3> <p>为了约束模型训练，DINO-IR 提出了一种基于 DINOv2 特征空间的<strong>对比学习损失</strong>：</p> <ul> <li><strong>原理：</strong> 提取恢复后的输出图像、原始清晰图像（正样本）和退化输入图像（负样本）在 DINOv2 隐藏层中的特征。</li> <li><strong>目标：</strong> 强制要求输出图像的 DINO 特征在空间中尽可能<strong>靠近清晰目标图像</strong>，并尽可能<strong>远离低质量输入图像</strong>。这种损失利用了 DINOv2 特征区分图像质量的能力来提升视觉效果。</li> </ul> <h3 id="3-方法优势">3. 方法优势</h3> <ul> <li><strong>退化鲁棒性：</strong> DINOv2 特征在不同噪声水平和退化类型下表现出极高的稳定性（方差远低于图像像素特征），这使得模型在处理冲突任务（如去噪需要滤除高频，而去模糊需要增强高频）时更加稳定。</li> <li><strong>泛化能力：</strong> 实验证明 DINO-IR 在<strong>未见过的退化级别</strong>（如更高强度的噪声）和<strong>未见过的测试数据集</strong>上具有更好的泛化效果。</li> <li><strong>性能提升：</strong> 在 deraining, denoising, deblurring, dehazing 四项任务的平均 PSNR 表现上，DINO-IR 优于 AirNet 和 PromptIR 等现有先进的多任务恢复方法。</li> </ul> <h3 id="note">Note</h3> <p>作者有做额外实验，DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节。</p> <p>It is known that the features extracted from shallow layersof DINOv2 (M, T, and T 2023) can discern low- and high-quality images。</p> <p>根据 DINO-IR（基于鲁棒 DINO 特征引导的多任务图像恢复）的研究资料，其提出的 <strong>DINO 感知对比损失（DINO Perception Contrastive Loss，简称 \(L_{DINO}\) 或 DPC Loss）</strong> 的公式及相关总损失公式如下：</p> <p>该损失函数旨在通过对比学习，使恢复后的图像在 DINOv2 的特征空间中靠近清晰图像，并远离退化的输入图像。公式表达为：</p> \[L_{DINO} = L(v, v^+, v^-) = \sum_{i=1}^n w_i \frac{D(\Psi_i(v), \Psi_i(v^+))}{D(\Psi_i(v), \Psi_i(v^-))}\] <p><strong>参数含义：</strong></p> <ul> <li><strong>\(v\)</strong>：恢复模型生成的输出图像。</li> <li><strong>\(v^+\)</strong>：正样本，即对应的<strong>清晰目标图像（Ground Truth）</strong>。</li> <li><strong>\(v^-\)</strong>：负样本，即<strong>低质量的退化输入图像</strong>。</li> <li><strong>\(\Psi_i\)</strong>：表示从固定的预训练 DINOv2 模型中提取的第 \(i\) 个隐藏层特征。</li> <li><strong>\(D(x, y)\)</strong>：表示 \(x\) 与 \(y\) 之间的 <strong>\(L1\) 距离</strong>。</li> <li><strong>\(w_i\)</strong>：对应层级的权重系数。</li> </ul> <p>作者并没有给出是第几层。但太深的层应该没用。</p> <h2 id="处理任意输入大小图片">处理任意输入大小图片</h2> <p>DINO（包括 DINOv2 和 DINOv3）处理任意大小图片的核心机制在于其 <strong>Transformer 架构的灵活性</strong>、<strong>分块（Patchification）策略</strong>以及<strong>位置编码的动态插值或旋转机制</strong>。</p> <p>以下是具体的实现方式：</p> <h3 id="1-灵活的序列长度处理set-to-set-架构">1. 灵活的序列长度处理（Set-to-set 架构）</h3> <p>DINO 系列模型基于 Vision Transformer (ViT)。与传统的卷积神经网络不同，Transformer 是一种<strong>“集合到集合”（set-to-set）的架构</strong>，它将图像视为一系列令牌（tokens）。</p> <ul> <li><strong>分块机制：</strong> 图像被切分为固定大小的 patch（例如 DINOv2 使用 \(14 \times 14\)，DINOv3 使用 \(16 \times 16\)）。</li> <li><strong>令牌数量随分辨率变化：</strong> 当输入图像变大时，模型只会产生更多的令牌，而 Transformer 的自注意力机制（Self-attention）天然可以处理任意长度的输入序列。</li> </ul> <h3 id="2-位置编码的适配核心技术">2. 位置编码的适配（核心技术）</h3> <p>由于 Transformer 本身无法感知令牌的空间顺序，必须加入位置编码。处理不同分辨率图像的关键在于如何让固定长度的位置编码适应变动的令牌数量：</p> <ul> <li><strong>线性插值（DINOv2/DPT 方案）：</strong> 在 DINOv2 和 DPT 中，如果输入图像的分辨率与训练时的分辨率不同，模型会对预训练的<strong>位置嵌入（Position Embeddings）进行线性插值</strong>。这使得模型能够动态适配到新的令牌网格尺寸，确保每个令牌都能获得其在图像中相对位置的信息。</li> <li><strong>旋转位置编码（DINOv3 的 RoPE 机制）：</strong> <strong>DINOv3</strong> 引入了更先进的 <strong>RoPE（Rotary Positional Embeddings）</strong> 机制。它将每个 patch 的坐标分配在一个归一化的 \([-1, 1]\) 框内，并在多头注意力操作中根据 patch 间的相对位置应用偏差。</li> <li><strong>无缝缩放：</strong> 依靠 RoPE 和坐标框抖动（box jittering）技术，DINOv3 能够<strong>在不进行任何适配的情况下无缝处理不同分辨率的图像</strong>。实验显示，即使在远超训练分辨率（如 4k 分辨率）的情况下，DINOv3 仍能保持稳定的特征表现。</li> </ul> <h3 id="3-全局感受野的维持">3. 全局感受野的维持</h3> <p>在卷积网络中，感受野随层数增加而受限，但在 DINO 中，由于使用了全局自注意力机制，<strong>每一个阶段（stage）都拥有全局感受野</strong>。</p> <ul> <li>这意味着无论图像多大，每个令牌都能与图像中的所有其他令牌进行交互。</li> <li>这种特性确保了模型在处理任意大小图片时，都能产生<strong>全局连贯（globally coherent）</strong>且精细的预测结果。</li> </ul> <h3 id="总结">总结</h3> <p>DINO 处理任意大小图片的逻辑可以类比为<strong>“拼图”</strong>：</p> <ul> <li><strong>分块</strong>是把图片切成小拼块，图越大拼块越多。</li> <li><strong>Transformer</strong> 是拼图者，他能处理任意数量的拼块。</li> <li><strong>RoPE 或插值编码</strong> 就像是在每一块拼图背面标注坐标的记号笔，通过动态缩放记号的刻度（坐标），拼图者总能知道每一块在大图中的精确位置。</li> </ul> <h3 id="dinov2-处理任意大小图像的代码">DINOv2 处理任意大小图像的代码</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">skimage.color</span> <span class="kn">import</span> <span class="n">hsv2rgb</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="c1"># --- 1. 配置参数 ---
# 图像文件路径 (已设置为您的文件)
</span><span class="n">IMAGE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fruits.jpg</span><span class="sh">'</span> 
<span class="c1"># DINOv2 模型 ID (Base 版本，公开且无需权限)
</span><span class="n">MODEL_ID</span> <span class="o">=</span> <span class="sh">"</span><span class="s">facebook/dinov2-base</span><span class="sh">"</span> 
<span class="n">DEVICE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="c1"># DINOv2-base 默认 Patch size 为 14x14
</span><span class="n">PATCH_SIZE</span> <span class="o">=</span> <span class="mi">14</span> 
<span class="c1"># 设置一个最小的安全尺寸，防止原图太小
</span><span class="n">MIN_SIZE</span> <span class="o">=</span> <span class="mi">224</span> 

<span class="k">def</span> <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    加载 DINOv2 模型，提取 Patch 特征，使用 PCA 降维并可视化。
    图像尺寸会调整到最接近原始尺寸且是 Patch Size 的整数倍。
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">错误: 图像文件未找到于 </span><span class="sh">'</span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="sh">'</span><span class="s">。请检查路径并重试。</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 2. 初始化模型和处理器 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">正在加载 DINOv2 模型: </span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s"> 到 </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># 评估模式
</span>    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型加载失败。请检查模型 ID 或网络连接。</span><span class="se">\n</span><span class="s">错误信息: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 3. 图像处理与特征提取 (重点修改部分) ---
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">原始图像尺寸 (W x H): </span><span class="si">{</span><span class="n">W_orig</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_orig</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># a. 计算目标输入尺寸 (必须是 PATCH_SIZE 的整数倍)
</span>    <span class="c1"># 取最接近原始尺寸且小于等于原始尺寸的 PATCH_SIZE 倍数
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">H_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    
    <span class="c1"># 确保尺寸不小于最小安全尺寸
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">W_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型目标输入尺寸 (W x H): </span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># b. 预处理
</span>    <span class="c1"># 显式传递 size 和 crop_size 参数，控制预处理器的缩放行为
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">processor</span><span class="p">(</span>
        <span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> 
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 缩放或调整到目标尺寸
</span>        <span class="n">crop_size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 确保不进行中心裁剪
</span>        <span class="n">do_center_crop</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 明确禁用中心裁剪
</span>    <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 实际输入模型张量的尺寸
</span>    <span class="n">h_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">w_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    
    <span class="c1"># 重新计算 Patch 网格尺寸 (H, W)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">h_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    
    <span class="c1"># c. 提取特征
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># **inputs 解包字典作为命名参数
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> 
        <span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="c1"># d. 移除 CLS Token
</span>    <span class="k">if</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">已移除 CLS Token。剩余 Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- 4. PCA 降维 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">正在进行 PCA 降维...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="c1"># 归一化到 [0, 1] 范围
</span>    <span class="n">pca_min</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pca_max</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">pca_max</span> <span class="o">-</span> <span class="n">pca_min</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-8</span> 
    <span class="n">pca_features_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pca_features</span> <span class="o">-</span> <span class="n">pca_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># --- 5. 可视化映射 ---
</span>
    <span class="c1"># a. 重塑为网格形状 (H, W, 3)
</span>    <span class="n">pca_grid</span> <span class="o">=</span> <span class="n">pca_features_norm</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># b. 映射到 HSV 颜色空间 
</span>    <span class="n">hsv_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pca_grid</span><span class="p">)</span>
    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Hue (色调)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>              <span class="c1"># Saturation (饱和度)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Value (亮度)
</span>
    <span class="c1"># 转换为 RGB 颜色
</span>    <span class="n">rgb_vis</span> <span class="o">=</span> <span class="nf">hsv2rgb</span><span class="p">(</span><span class="n">hsv_image</span><span class="p">)</span>
    
    <span class="c1"># c. 缩放可视化结果到原始图像大小
</span>    <span class="n">rgb_vis_upscaled</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span>
        <span class="n">rgb_vis</span><span class="p">,</span> 
        <span class="p">(</span><span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span><span class="p">),</span> <span class="c1"># 使用原图尺寸进行缩放
</span>        <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">INTER_NEAREST</span> <span class="c1"># 最近邻插值保持 Patch 块状效果
</span>    <span class="p">)</span>

    <span class="c1"># --- 6. 显示和保存结果 ---
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">原始图像 (Original Image: fruits.jpg)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rgb_vis_upscaled</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">DINOv2 特征 PCA 可视化 (</span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s">x</span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="s"> 输入)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">output_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dinov2_fruits_pca_visualization.png</span><span class="sh">"</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">output_filename</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">可视化结果已保存为 </span><span class="si">{</span><span class="n">output_filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">IMAGE_PATH</span><span class="p">,</span> <span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">PATCH_SIZE</span><span class="p">,</span> <span class="n">MIN_SIZE</span><span class="p">)</span>
</code></pre></div></div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="https://zhuanlan.zhihu.com/p/1986387271688139358">(3 封私信 / 80 条消息) DINOv3 is All You Need? 为什么 DINOv3 发布后，CV 圈感觉“天塌了”？ - 知乎</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[如何使用DINO, DPT, RoPE]]></summary></entry><entry><title type="html">3D 任务：SFM, MVS, NVS, VO, VIO, SLAM</title><link href="https://cekxm.github.io/blog/2025/3dtasks/" rel="alternate" type="text/html" title="3D 任务：SFM, MVS, NVS, VO, VIO, SLAM"/><published>2025-12-26T00:00:00+00:00</published><updated>2025-12-26T00:00:00+00:00</updated><id>https://cekxm.github.io/blog/2025/3dtasks</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/3dtasks/"><![CDATA[<h2 id="多视角几何mvs新视角生成nvs对比">多视角几何（MVS），新视角生成（NVS）对比</h2> <p><img src="/images/2025-12-26-3dtasks/d3dcd8d3-35cb-403b-998d-64256b21ba06.png" alt="SfM_NeRF" class="img-fluid"/></p> <h3 id="mvssfm-vs-vggt-vs-nvs-nerfgs-综合对比表">(MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>(MVS) SfM (如 COLMAP)</strong></th> <th><strong>VGGT (Visual Geometry Grounded Transformer)</strong></th> <th><strong>NVS (NeRF / 3D GS)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>核心任务目标</strong></td> <td><strong>三维重建</strong>：求解精确的相机姿态和场景几何（点云/深度）。</td> <td><strong>统一几何推断</strong>：一站式、秒级预测相机、点图、深度图和追踪。</td> <td><strong>新视角合成</strong>：在未拍摄过的角度生成照片级逼真的图像。</td> </tr> <tr> <td><strong>底层表示</strong></td> <td><strong>离散几何</strong>：稀疏或稠密的 3D 点云、深度图。</td> <td><strong>稠密几何图</strong>：Point Maps (\(H \times W \times 3\)) 和 Depth Maps。</td> <td><strong>光场表示</strong>：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。</td> </tr> <tr> <td><strong>运行机制</strong></td> <td><strong>优化驱动</strong>：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。</td> <td><strong>前馈推理 (Feed-forward)</strong>：一次性通过 Transformer 模型直接“看”出几何。</td> <td><strong>视图对齐优化</strong>：通过渲染结果与原图的颜色误差来反向训练。</td> </tr> <tr> <td><strong>对相机的需求</strong></td> <td><strong>未知或已知</strong>：通常用于解算未知相机参数。</td> <td><strong>无需预设</strong>：直接预测相机的 9 维参数（内外参）。</td> <td><strong>必须已知</strong>：通常依赖 SfM (COLMAP) 提供位姿初始化。</td> </tr> <tr> <td><strong>几何精确度</strong></td> <td><strong>高（度量级）</strong>：数学推导严谨，但易受弱纹理、模糊影响。</td> <td><strong>高且鲁棒</strong>：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。</td> <td><strong>中/低</strong>：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。</td> </tr> <tr> <td><strong>渲染视觉效果</strong></td> <td><strong>差</strong>：只有离散点，无法生成连续、真实的图像。</td> <td><strong>中</strong>：提供稠密几何，但主要用于几何任务而非美学渲染。</td> <td><strong>极高</strong>：支持照片级渲染、阴影、反射和透明效果。</td> </tr> <tr> <td><strong>处理速度</strong></td> <td><strong>极慢</strong>：通常需要数分钟到数小时。</td> <td><strong>极快</strong>：全流程通常在 <strong>1 秒以内</strong>。</td> <td><strong>训练慢/渲染快</strong>：NeRF 训练慢，GS 渲染极快，但都需要初始化。</td> </tr> </tbody> </table> <p>简而言之：<strong>(MVS)SfM</strong> 是传统的“几何尺子”；<strong>VGGT</strong> 是现代的“几何大模型”；而 <strong>NVS</strong> 是“虚拟照相机”。</p> <p>根据您上传的论文内容及相关技术背景，<strong>ESVO2 是一个 VIO（视觉惯性里程计）系统</strong>，它是基于纯视觉算法 ESVO 的增强版，引入了惯性测量单元（IMU）来提高鲁棒性。</p> <p>以下是关于 ESVO2 的定位及其与 SLAM 区别的详细介绍：</p> <h2 id="vio-与-slam-的区别">VIO 与 SLAM 的区别</h2> <p>虽然两者都旨在解决“我在哪”的问题，但它们在功能覆盖和目标重点上有所不同：</p> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>视觉惯性里程计 (VIO)</strong></th> <th><strong>同步定位与建图 (SLAM)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要组成</strong></td> <td>视觉处理 + IMU 预积分 + 后端优化</td> <td>VIO (前端) + <strong>回环检测</strong> + 全局优化</td> </tr> <tr> <td><strong>误差累计</strong></td> <td>会随着位移增加而产生<strong>累计漂移（Drift）</strong></td> <td>通过回环检测修正累计误差，具有<strong>全局一致性</strong></td> </tr> <tr> <td><strong>地图规模</strong></td> <td>通常只维护一个局部地图（滑动窗口内）</td> <td>维护全局地图，允许机器人回到已知点时重定位</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>相对较低，适合实时性要求极高的场景</td> <td>较高，需要存储和检索大量历史关键帧数据</td> </tr> <tr> <td><strong>典型代表</strong></td> <td>VINS-Mono (VIO模式), ESVO2, OKVIS</td> <td>ORB-SLAM3, VINS-Fusion (含回环)</td> </tr> </tbody> </table> <h2 id="slam">SLAM</h2> <p>在 SLAM（同步定位与建图）框架中，<strong>回环检测（Loop Closure Detection）</strong>和<strong>全局优化（Global Optimization）</strong>是消除累计误差、保证地图全局一致性的核心机制。</p> <p>如果没有这两个部分，系统仅仅是一个<strong>里程计（Odometry）</strong>，位姿误差会随着时间的推移不断增加，轨迹最终会“漂散”。</p> <hr/> <h3 id="回环检测-loop-closure-detection">回环检测 (Loop Closure Detection)</h3> <p>回环检测的任务是：<strong>识别机器人是否回到了曾经到过的地方。</strong></p> <ul> <li> <p>为什么要检测？</p> <p>里程计每一步都会引入微小的误差，经过长距离运行后，系统估计的当前位置与真实位置可能相差巨大。如果能识别出“旧地重游”，就能通过这个历史“锚点”来纠正累积误差。</p> </li> <li> <p><strong>如何实现？（主流方法：词袋模型 BoW）</strong></p> <ol> <li><strong>特征提取</strong>：提取当前图像的特征点（如 ORB）。</li> <li><strong>词袋描述</strong>：将特征点转化为一个数值向量（类似一篇文章的关键词提取）。</li> <li><strong>相似度检索</strong>：在历史图像库中寻找与当前图像最相似的帧。</li> <li><strong>几何验证</strong>：通过对极几何等方法，确认两张图不仅长得像，而且在空间逻辑上也是匹配的，从而防止“感知歧义”（例如两间长得一模一样的办公室）。</li> </ol> </li> </ul> <hr/> <h3 id="全局优化-global-optimization">全局优化 (Global Optimization)</h3> <p>一旦回环检测发现当前帧 $i$ 与历史帧 $j$ 匹配成功，系统就获得了一个闭环约束。接下来需要通过全局优化来调整整个轨迹。</p> <h4 id="位姿图优化-pose-graph-optimization"><strong>位姿图优化 (Pose Graph Optimization)</strong></h4> <p>在回环发生后，为了保持实时性，通常不再优化复杂的 3D 空间点，而仅仅优化<strong>相机的位姿节点</strong>。</p> <ul> <li><strong>节点（Nodes）</strong>：机器人每一时刻的位姿。</li> <li><strong>边（Edges）</strong>： <ul> <li><strong>里程计边</strong>：相邻帧之间的约束（由 VIO 提供）。</li> <li><strong>回环边</strong>：当前帧与历史帧之间的约束（由回环检测提供）。</li> </ul> </li> <li><strong>优化目标</strong>：调整所有节点，使得所有“边”的残差总和最小。这就像是一个弹簧网，回环边强行把漂移的轨迹拉回到历史位置，而其他轨迹点则像弹簧一样跟随调整。</li> </ul> <hr/> <h3 id="区别总结vio-vs-slam">区别总结：VIO vs SLAM</h3> <table> <thead> <tr> <th><strong>模块</strong></th> <th><strong>VIO (如 ESVO2, VINS 前端)</strong></th> <th><strong>SLAM (如 VINS 后端, ORB-SLAM3)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>漂移</strong></td> <td>随距离/时间线性增长</td> <td>遇到回环时清零</td> </tr> <tr> <td><strong>一致性</strong></td> <td>局部一致</td> <td>全局一致</td> </tr> <tr> <td><strong>地图</strong></td> <td>瞬时、局部</td> <td>持久、可重用</td> </tr> </tbody> </table> <p><strong>COLMAP</strong> 通常不被认为是一个 <strong>VO（视觉里程计）</strong> 方法，它是一个标准的 <strong>SfM（运动恢复结构）</strong> 框架。</p> <p>虽然两者底层都使用了相似的技术（如特征提取、对极几何、Bundle Adjustment），但它们在设计目标、运行方式和应用场景上有显著区别。</p> <hr/> <h2 id="sfm-与-vo-的本质区别">SfM 与 VO 的本质区别</h2> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>COLMAP (SfM)</strong></th> <th><strong>Visual Odometry (VO)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>处理时效</strong></td> <td><strong>离线 (Offline)</strong>：处理一组预先采集好的图片或视频序列。</td> <td><strong>实时 (Real-time)</strong>：随着相机的移动即时计算位姿。</td> </tr> <tr> <td><strong>图像顺序</strong></td> <td><strong>无序/有序</strong>：可以处理乱序的照片（如不同人的街拍），通过特征匹配寻找联系。</td> <td><strong>必须有序</strong>：依赖视频流的连续性进行帧间追踪。</td> </tr> <tr> <td><strong>优化范围</strong></td> <td><strong>全局 (Global)</strong>：通常对所有相机位姿和所有点进行全局优化（Global BA）。</td> <td><strong>局部 (Local)</strong>：通常只在滑动窗口内进行优化以维持实时性。</td> </tr> <tr> <td><strong>应用目标</strong></td> <td>追求<strong>极致精度</strong>和高质量的 3D 重建（如制作模型、地图）。</td> <td>追求<strong>低延迟</strong>和机器人的实时定位（如无人机飞行）。</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>极高：可能需要数小时或数天处理大型场景。</td> <td>较低：需要在嵌入式设备或普通 CPU/GPU 上实时运行。</td> </tr> </tbody> </table> <hr/> <h3 id="它们之间的联系">它们之间的联系</h3> <p>虽然它们定位不同，但有很深的血缘关系：</p> <ul> <li><strong>技术基础一致</strong>：它们都遵循“特征提取 $\rightarrow$ 特征匹配 $\rightarrow$ 位姿估计 $\rightarrow$ 三角化建图 $\rightarrow$ 后端优化”的流程。</li> <li><strong>增量式 SfM</strong>：COLMAP 属于“增量式（Incremental）”SfM，它的工作方式是一张一张地把新照片注册到现有地图中。这种“一张张增加”的逻辑在形式上非常接近 VO，只是 COLMAP 每增加一张都会做大量的全局检查以保证精度，而 VO 只看局部。</li> <li><strong>VO 是 SfM 的子集</strong>：你可以把 VO 看作是一种追求实时性、牺牲全局一致性、且只能处理有序序列的特殊 SfM。</li> </ul> <hr/> <h3 id="为什么不把-colmap-当作-vo-使用">为什么不把 COLMAP 当作 VO 使用？</h3> <p>如果你尝试用 COLMAP 跑实时定位，会遇到以下问题：</p> <ol> <li><strong>速度太慢</strong>：COLMAP 的每一步（尤其是匹配和全局优化）都非常耗时，无法做到每秒处理 30 帧。</li> <li><strong>内存消耗</strong>：COLMAP 试图建立和维护完整的场景模型，随着照片增加，内存开销会爆炸。</li> <li><strong>缺乏状态估计</strong>：VO 通常会融合 <strong>IMU（惯性测量单元）</strong> 来处理快速运动（如 VINS-Fusion），而 COLMAP 主要是纯视觉处理。</li> </ol> <h3 id="总结">总结</h3> <p><strong>COLMAP 是用来“建图”的利器，而 VO 是用来“带路”的工具。</strong></p> <ul> <li>如果你有 1000 张从各个角度拍的景区照片，想做一个 3D 模型，用 <strong>COLMAP</strong>。</li> <li>如果你有一个机器人正在屋里跑，需要知道它现在在哪，用 <strong>VINS-Mono</strong> 或 <strong>ORB-SLAM3</strong>。</li> </ul> <h2 id="研究方向">研究方向</h2> <p>在 2025 年的时间节点上，<strong>Novel View Synthesis (NVS)</strong> 在学术热度和资本市场显然更“火”，但 <strong>Multi-View Stereo (MVS)</strong> 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。</p> <p>这两者并非孤立竞争，而是呈现出一种<strong>深度融合</strong>的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：</p> <hr/> <h3 id="1-热门程度novel-view-synthesis-nvs-占据-c-位">1. 热门程度：Novel View Synthesis (NVS) 占据 C 位</h3> <p><strong>核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D</strong></p> <ul> <li><strong>学术热度：</strong> 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 <strong>3DGS (3D 高斯溅射)</strong> 和 <strong>生成式新视角合成</strong> 的论文数量呈爆炸式增长。</li> <li><strong>AIGC 助力：</strong> 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 <strong>Generative NVS</strong>）成了最热门的方向。</li> <li><strong>用户感知度：</strong> NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。</li> </ul> <h3 id="2-发展前景mvs-正在向几何大模型进化">2. 发展前景：MVS 正在向“几何大模型”进化</h3> <p><strong>核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry</strong></p> <ul> <li><strong>从“工具”到“大脑”：</strong> 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 <strong>VGGT</strong> 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个<strong>前馈网络（Feed-forward）</strong>。</li> <li><strong>解决“不可能任务”：</strong> 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。</li> <li><strong>工业刚需：</strong> 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的<strong>精确绝对坐标（Metric Geometry）</strong>。</li> </ul> <hr/> <h3 id="3-2025-年的关键趋势两者边界的模糊融合">3. 2025 年的关键趋势：两者边界的模糊（融合）</h3> <p>如果你在考虑职业发展或研究方向，<strong>“几何感知的 NVS” (Geometry-aware NVS)</strong> 是 2025 年最具前景的方向。</p> <ol> <li><strong>MVS 为前，NVS 为后：</strong> 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。</li> <li><strong>可推广性 (Generalizability)：</strong> 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 <strong>LRM (Large Reconstruction Models)</strong>，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。</li> <li><strong>动态场景：</strong> 静态场景的重建基本解决，2025 年的蓝海是<strong>动态 4D 重建</strong>（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。</li> </ol> <hr/> <h3 id="总结建议">总结建议</h3> <ul> <li><strong>如果你追求“视觉震撼”和“快速产出”：</strong> 选择 <strong>Novel View Synthesis (尤其是 3DGS)</strong>。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。</li> <li><strong>如果你追求“底层技术”和“稳健性”：</strong> 选择 <strong>MVS/几何大模型</strong>。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。</li> <li><strong>最具潜力的路径：</strong> 研究<strong>如何将 MVS 的几何约束引入 NVS</strong>（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。</li> </ul> <p><strong>结论：</strong> <strong>NVS 更“热门”（Hotter）</strong>，但 <strong>MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）</strong>。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[多视角几何，新视角生成]]></summary></entry><entry><title type="html">Matplotlib输出中文</title><link href="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/" rel="alternate" type="text/html" title="Matplotlib输出中文"/><published>2025-12-25T14:06:17+00:00</published><updated>2025-12-25T14:06:17+00:00</updated><id>https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"><![CDATA[<h4 id="步骤一在-ubuntu-系统上安装中文字体并清理缓存">步骤一：在 Ubuntu 系统上安装中文字体并清理缓存</h4> <p>您需要在 Ubuntu 24 上安装一个包含中文支持的字体包。</p> <ol> <li> <p><strong>安装中文字体（推荐：文泉驿）：</strong> 打开您的终端，执行以下命令安装常用的开源中文字体包：</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt update
sudo apt install fonts-wqy-zenhei
</code></pre></div> </div> </li> <li> <p><strong>清理 Matplotlib 缓存：</strong> Matplotlib 会缓存系统中的字体信息。如果您不清理缓存，即使安装了新字体，它也可能无法立即识别。</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 找到缓存目录的位置
python -c "import matplotlib; print(matplotlib.get_cachedir())"
   
# 通常该目录是 ~/.cache/matplotlib，清理它
rm -rf ~/.cache/matplotlib
   
# （重要）清理后，请确保重新启动您的 Cursor 终端或整个 IDE，以使新的环境变量和字体生效。
</code></pre></div> </div> </li> </ol> <h4 id="步骤二修改-python-代码配置-matplotlib-字体">步骤二：修改 Python 代码配置 Matplotlib 字体</h4> <p>在您的 Python 脚本开头，导入 Matplotlib 之后，添加字体配置代码，指定 Matplotlib 优先使用支持中文的字体。</p> <p><strong>请用下面这段代码替换您脚本开头的所有 <code class="language-plaintext highlighter-rouge">import</code> 语句以及它们之后的代码（在 <code class="language-plaintext highlighter-rouge">visualize_dinov2_features_variable_res</code> 函数定义之前）。</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># =========================================================
# Matplotlib 中文字体配置（解决 UserWarning 问题）
# =========================================================
</span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># 尝试设置 Matplotlib 的字体
</span>    <span class="c1"># 使用一个字体列表，优先尝试安装的文泉驿字体，确保兼容性
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">sans-serif</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.sans-serif</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">WenQuanYi Zen Hei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Microsoft YaHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DejaVu Sans</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 解决负号显示问题 (可选，但推荐)
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">axes.unicode_minus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matplotlib 已配置中文字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">配置中文字体失败: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">。将使用默认字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># =========================================================
</span></code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Mamba</title><link href="https://cekxm.github.io/blog/2025/mamba/" rel="alternate" type="text/html" title="Mamba"/><published>2025-12-25T14:03:54+00:00</published><updated>2025-12-25T14:03:54+00:00</updated><id>https://cekxm.github.io/blog/2025/mamba</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/mamba/"><![CDATA[<h2 id="mamba">Mamba</h2> <p>Mamba 处理的是序列的建模问题，由输入 \(x(t)\) 得到输出 \(y(t)\)，所以它更像是一个特征提取。它的优势是对于序列长度 \(L\) 来说，复杂度线性。</p> <p>S4 是 structured state space sequence model. 结构化指的是A 是 HiPPO 矩阵（是这样吗？）。但是它的参数依然是时不变的。</p> <p>Mamba 是S6，加了 selective scan，使得参数时变，依赖于输入 \(x\)。</p> <p>如果有一定的基础，可以直接看这个对比。其中 \(B,L,D,N\) 分别是 batch，序列长度，输入（输出）的特征维度，内部特征的维度。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111607190.png" alt="image-20250904111607190" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904112651698.png" alt="image-20250904112651698" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904113042955.png" alt="image-20250904113042955" class="img-fluid"/></p> <p>它会独立的把 \(x\) 的每一通道到输出 \(y\) 的每一维，中间通过一个更维的隐藏状态 \(h\)。如上图，\(x\) 的每一通道是独立计算的，因此 \(\bar{A}\)，\(\bar{B}\) 的尺寸为 \((B,L,D,N)\)。但是\(x\) 的每一通道的参数和整个 \(x\) 有关系。 \(\bar{A}\)，\(\bar{B}\) 的离散化见下图公式（4），结合上面的表格，在计算中，尺寸有变化，因此在代码中，有大量的 einsum 操作。</p> <p>由 \(A\)，\(B\) 求 \(\bar{A}\)，\(\bar{B}\) 的过程是离散化。这个和信号与系统的知识有关。它内在的过程还是连续的，但是取值的时间是离散的，所以是去算一个微分方程+初值在 \(\Delta\) 时间后的状态。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111925419.png" alt="image-20250904111925419" class="img-fluid"/></p> <h3 id="矩阵-a">矩阵 \(A\)</h3> <p>在 Mamba 模型中，矩阵 A 是对角阵（diagonal matrix），而非 HiPPO 阵。HiPPO 是一种用于初始化矩阵 A 的策略，主要出现在早期的 S4 模型中，但 Mamba 采用了对角结构并使用不同的初始化方式，以实现更高效的计算。</p> <p>在 Mamba 中，虽然矩阵 A A A 被简化为对角矩阵，但其对角元素的初始化方式仍然受到 HiPPO 矩阵的启发，具体体现在以下几个方面：</p> <ol> <li>特征值的分布 <ul> <li>HiPPO 矩阵的特征值通常被设计为负实部，以确保系统的稳定性。在 Mamba 中，对角矩阵 A A A 的对角元素（即其特征值）被初始化为负值，模仿 HiPPO 矩阵的稳定性特性。这确保了 Mamba 在处理长序列时不会出现数值不稳定的问题。</li> <li>具体来说，Mamba 的对角元素通常被初始化为负的、对数分布的值（如 −1,−2,−4,…-1, -2, -4, \ldots−1,−2,−4,… 或类似的分布），这与 HiPPO 矩阵的特征值分布有相似的动机，即通过控制特征值的范围来平衡短期和长期记忆。</li> </ul> </li> <li>动态生成对角元素 <ul> <li>Mamba 的对角矩阵 A A A 的对角元素是由网络参数动态生成的，而不是固定的。这些参数在训练开始时会根据 HiPPO 的思想进行初始化。例如，Mamba 可能通过对数尺度（log-scale）初始化对角元素，以模拟 HiPPO 矩阵在不同时间尺度上的记忆能力。</li> <li>这种初始化方式使得 Mamba 能够在训练初期就具备捕捉长程依赖的能力，而无需像 S4 那样依赖稠密的 HiPPO 矩阵。</li> </ul> </li> <li>高效性与简化 <ul> <li>HiPPO 矩阵通常是稠密的，计算成本较高。Mamba 通过将 A A A 限制为对角矩阵，极大地降低了计算复杂度（从 O(N2) O(N^2) O(N2) 降到 O(N) O(N) O(N)，其中 N N N 是状态维度）。</li> <li>尽管结构上简化了，Mamba 仍然通过借鉴 HiPPO 的初始化策略，保留了其在序列建模中的核心优势，如对长序列的记忆能力和稳定性。</li> </ul> </li> </ol> <h3 id="gpu-sram--gpu-hbm">GPU SRAM + GPU HBM</h3> <p>这一点也是 mamba 效率的关键。</p> <p>参见1 <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a> Hardware-aware Algorithm</p> <p>以及原论文 sec3.3.2</p> <h2 id="参考文献">参考文献</h2> <h3 id="博客">博客</h3> <ol> <li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a></li> </ol> <h3 id="代码">代码</h3> <h4 id="自然语言处理">自然语言处理</h4> <ol> <li> <p><a href="https://github.com/johnma2006/mamba-minimal/tree/master">GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.</a></p> <p>这个代码给出了 mamba 的简化结构代码，没有训练，但是有调用预训练模型（<a href="https://huggingface.co/state-spaces/mamba-370m/tree/main">需要从 huggingface 上下载，大小1.5G</a>）。</p> <p>可以理解它的底层代码。</p> </li> <li> <p><a href="https://readmedium.com/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049">Building Mamba from Scratch: A Comprehensive Code Walkthrough</a></p> </li> </ol> <p>这个代码是整套的，包含底层结构和训练，使用 enwiki8 数据。可以运行，我把它放到了 colab 上。</p> <p><img src="/images/2025-12-25-mamba/image-20250904095945491.png" alt="image-20250904095945491" class="img-fluid"/></p> <p>刚开始的 loss 和初始化比较有关系。（昨天因为这个原因，在 M4 上暂停了，回头再跑一次）。</p> <h4 id="图像">图像</h4> <p><a href="https://github.com/pprp/Vision-Mamba-CIFAR10">GitHub - pprp/Vision-Mamba-CIFAR10</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Flow</title><link href="https://cekxm.github.io/blog/2025/flow/" rel="alternate" type="text/html" title="Flow"/><published>2025-12-25T13:44:12+00:00</published><updated>2025-12-25T13:44:12+00:00</updated><id>https://cekxm.github.io/blog/2025/flow</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/flow/"><![CDATA[<h1 id="flow-matching">Flow matching</h1> <h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] <a href="https://medium.com/@nikolaus.correll/flow-matching-for-generative-models-from-scratch-8264bad4e0ba">Flow Matching For Generative Models From Scratch | by Nikolaus Correll | Toward Humanoids | Medium</a></p> <p>[2] <a href="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/">A Visual Dive into Conditional Flow Matching | ICLR Blogposts 2025</a></p> <h3 id="toy-example">toy example</h3> <p><a href="https://github.com/Whalefishin/Latent_Flow_Matching_MNIST">GitHub - Whalefishin/Latent_Flow_Matching_MNIST: A minimal example for training a flow matching model in a pretrained VAE’s latent space to generate MNIST digits.</a></p> <p><a href="https://github.com/lebellig/discrete-fm">GitHub - lebellig/discrete-fm: Educational implementation of the Discrete Flow Matching paper</a></p> <h2 id="理论">理论</h2> <h3 id="continuous-normalizing-flows">Continuous Normalizing Flows</h3> <p>以下主要参考 [2]</p> <p><img src="/images/2025-12-25-flow/image-20250911165058417.png" alt="image-20250911165058417" class="img-fluid"/></p> <p>首先要理解这个图。</p> <ul> <li> <p>概率路径 \(p_t\) 指 \(t\) 时刻 \(x\) 的概率分布 \(p_t(x)\)，一般令 \(p_0(t)\) 是一个标准的高斯分布，\(p_1(t)\) 是我们要建模的、未知的分布。概率路径就是从0时刻到1时刻，概率分布的变化过程。</p> </li> <li> <p>速度场 \(u(x,t)\) 比较好理解</p> </li> <li> <p>flow 就是流，类似水流，水流中 \(x\) 位置的一个水滴（假设它有独特的标记），经过 \(t\) 时候，会跟随水流移动到 \(f(x,t)\) 位置。 \(f(x,t)\) 可以看成一个映射，\(f^u: \mathbb{R}^d \times[0, 1] \to \mathbb{R}^d\)</p> </li> <li> <p>和 flow 有关系的另一个变量是 \(x(t)\) ,也是表示原处于 \(x_0\) 位置的点随时间t的变化，见下面这个常微分方程（ODE），它很好理解，速度是位置的导数。</p> </li> </ul> \[\begin{cases} x(0) = x_0 \\ \partial_t x(t) = u(x(t), t) \quad \forall t \in [0, 1] \end{cases}\] <p>在文献[2]中，\(\partial_t\) 就是指 \(\frac{\partial}{\partial t}\)。上式也叫 initial value problem（初值问题）。flow \(f^u(x,t)\) defined as the solution at time \(t\) to the initial value problem driven by \(u\) with initial condition \(x(0)=x\).</p> <p>上面这个图，三个要素形成三角关系。</p> <ul> <li>flow 和速度场之间是 ODE 的关系，知道速度场，可以求解 flow。\(f^u(x,t)\) 是它的解，\(x\) 实际上是初值。</li> <li>概率路径和速度场之间满足连续性方程</li> </ul> \[\begin{equation}\label{eq:continuity_eq} \partial_t p_t + \nabla\cdot u_t p_t = 0 \end{equation}\] <p>其中 \(\nabla\cdot\) 表示散度。这个方程保证了概率质量的守恒：概率密度在空间中的变化（通过时间导数 \(\frac{\partial p_t(x)}{\partial t}\)）由概率流量的散度决定。</p> <ul> <li>概率路径和flow之间满足 <em>change-of-variable formula</em>，也称为 pushforward \(p_t = f^u(\cdot, t)\# p_0\)。就是说通过这个流，把 \(p_0\) 经时间t，变为 \(p_t\)。</li> </ul> <p>以下是AI关于change-of-variable formula的介绍。</p> <p><strong>Change-of-variable formula</strong> 描述了概率密度如何随这种映射变化。对于一个可逆的映射 \(\phi_t: x_0 \mapsto x_t\) （可逆映射在这边就是flow，它是一个映射，并且是可逆的），概率密度 \(p_t(x_t)\) 与初始密度 \(p_0(x_0)\) 的关系为： \(p_t(x_t) = p_0(x_0) \cdot \left| \det\left( \frac{\partial \phi_t^{-1}}{\partial x_t} \right) \right| = p_0(\phi_t^{-1}(x_t)) \cdot \left| \det\left( \frac{\partial x_0}{\partial x_t} \right) \right|\) 其中：</p> <ul> <li>\(\phi_t^{-1}\) 是逆映射，从 \(x_t\) 映射回 \(x_0\)。</li> <li>\(\det\left( \frac{\partial x_0}{\partial x_t} \right)\) 是逆映射的雅可比行列式的绝对值。</li> </ul> <p>在流匹配中，雅可比行列式反映了流映射如何缩放空间，从而影响概率密度。</p> <blockquote> <p>我之前有疑问，从 \(x_0\) 到 \(x_t\) 可能经过了很长的距离变化，怎么这两个位置之间的密度能建立联系？答案可能是因为流是连续的，\(x_0\) 附近的质量都会流到 \(x_t\) 附近，这个变化由雅可比行列式来决定。</p> </blockquote> <h3 id="conditional-flow-matching">Conditional Flow Matching</h3> <p>可能，flow matching 就是指 conditional flow matching.</p> <p>CFM 核心思想是选择一个条件变量 \(z\)，以及一个条件概率路径 \(p(x\mid t,z)\)，满足两点</p> <ol> <li>由 \(p(x\mid t,z)\)​ 推导出的全局概率路径 \(p(x\mid t)\)​ 可以把 \(p_0\)​ 转化为 \(p_{data}\)​。即要求 \(p(x\mid t,z)\) 在 t=0,t=1 的边际概率 \(p(x\mid t=0)\)，\(p(x\mid t=1)\)符合</li> </ol> \[\begin{align*} \forall x \space E_z [ p(x \vert z, t=0) ] = p_0(x) , \\ \forall x \space E_z [ p(x \vert z, t=1) ] = p_{data}(x). \end{align*}\] <ol> <li>\(p(x\mid t,z)\) 对应的条件速度场 \(u^{cond}(x,t,z)\) （回忆一下，二者的关系是连续性方程）具有一个解析的形式，这是因为要使用一个神经网络来回归条件速度场。</li> </ol> <p>文中用下图进行概括：</p> <p><img src="/images/2025-12-25-flow/image-20250911233204539.png" alt="image-20250911233204539" class="img-fluid"/></p> <ul> <li> <p>首先，要做出 choice 1</p> </li> <li> <p>然后做出 choice 2（必须满足要求1）</p> </li> <li> <p>\(p(x\mid t,z)\) 能够确定 \(u^{cond}(x,t,z)\) ，它们之间满足连续性方程</p> </li> <li> <p>\(p(x\mid t,z)\) 边际化 \(z\) 可以获得 \(p(x\mid t)\)</p> </li> <li> <p>\(p(x\mid t)\) 和速度场 \(u(x,t)\) 之间满足连续性方程</p> </li> <li> <p>\(u(x,t)\)可由 \(u^{cond}(x,t,z)\)​ 显式的表达，这个关系就是文中的Theorem 1:</p> </li> </ul> \[\begin{align} \forall t, x, \, \, u(x,t) &amp;= E_{z\mid x, t} {u^{cond} }(x,t,z) \end{align}\] <ul> <li>实际计算中，并不是通过这个表达式去获得 \(u(x,t)\)。而是使用一个神经网络来回归条件速度场。那回归条件速度场有什么用呢？这个解释是文中的 Theorem 2. 即使用下面这个Loss来回归条件速度场，</li> </ul> \[\begin{aligned} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \overset{\mathrm{def}}{=} E_{ \substack{t \sim \mathcal{U}([0, 1]) \\ z \sim p_z \\ x \sim p( \cdot \mid t, z) } }{\lVert u_\theta^{CFM}(x,t) - \underbrace{u^{cond}(x,t,z)}_{\substack{ \text{chosen to be} \\ \text{explictly defined}, \\ \text{cheap to compute}, \\ \text{e.g., } x_1 - x_0}} \rVert^2} \enspace, \end{aligned}\] <p>等价于回归不可知的速度场</p> \[\begin{align*} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \underset{(\text{proof below})}{=} E_{\substack{ t \sim \mathcal{U}([0, 1]) \\ x \sim p_t} } \Vert{u_\theta^{CFM}(x,t) - \underbrace{u(x,t)}_{\substack{\text{implicitly defined,} \\ \text{hard/expensive} \\ \text{to compute}}}}\Vert^2 + \underbrace{C}_{\text{indep. of } \theta} \end{align*}\] <p>Theorem 2 的证明使用了Theorem 1.</p> <p>因此，我们用神经网络去逼近条件速度 \(u^{cond}(x,t,z)\)，最终学习得到的是经过点 \((x,t)\) 的所有轨迹的平均速度，这个平均速度也就是 \(u(x,t)\)。</p> <h1 id="mean-flow">Mean flow</h1> <h2 id="mean-flow-论文中的-flow-matching-定义">mean flow 论文中的 flow matching 定义</h2> <p>flow 的 0 时刻为 \(p_{data}(x)\)，1 时刻为 \(p_{prior}(\epsilon)\)。flow 是把数据映射为先验（一版是高斯噪声），和之前定义的映射反过来了。给定\(x\sim p_{prior}(\epsilon)\)， \(x\sim p_{data}(x)\)，定义 flow path: \(z_t=a_t x+b_t \epsilon\)，其中 \(a_t\) , \(b_t\) 是 predifined schedules.</p> <blockquote> <p>可能，只要满足 \(z_0=\epsilon\), \(z_1=x\) 就行。</p> </blockquote> <p>比较常用的是，\(a_t=1-t\), \(b_t=t\)。由于速度 \(v_t = z'_t=a'_t x+b'_t \epsilon\)，因此 \(v_t = \epsilon - x\)。</p> <p>给定速度场 \(v(z_t,t)\)，通过求解 ODE 来进行数据采样 \(z_t\): \(\frac{d}{dt} z_t=v(z_t,t)\) starting from \(z_1=\epsilon\)。注意，这个初值问题的初值是 \(z_1\)。所以是从 \(t=1\) 倒推的。解可以写成 \(z_r=z_t - \int_r^t v(z_r,r)dr\) 实现时，是用数值解，比如欧拉法 \(z_{t_{i+1}}=z_{t_{i}}+(t_{i+1}-t_i)v(z_{t_{i}},t_i)\) 注意，这两个式子没有矛盾。倒推时，\(t_{i+1}-t_i&lt;0\)，速度取的时间是 \(t_i\)。</p> <h2 id="mean-flows">mean flows</h2> <p>定义平均速度 \(u\)： \(u(z_t,r,t) \triangleq \frac{1}{t-r}\int_r^tv(z_r,r)dr\) 用一个神经网络 \(u_\theta(z_t,r,t)\) 来预测平均速度 \(u\)。在训练时，就需要它的真值。经过推导可得 \(u(z_t,r,t) = v(z_t,t)-(t-r)\frac{d}{dt}u(z_t,r,t)\)</p> \[\frac{d}{dt}u(z_t,r,t) =v(z_t,t)\partial_z u +\partial_t u\] <p>可以看出，要知道真值，需要真值对时间的微分，这没法获得。所以实际使用的真值是 \(u_{tgt} = v(z_t,t)-(t-r)(v(z_t,t)\partial_z u_\theta +\partial_t u_\theta)\) 即在计算微分时，用参数化的 \(\partial u_\theta\) 来代替 \(\partial u\)。并且回顾一下神经网络的训练，真值用于计算 loss，它本身一般是一个固定值。而在这边真值和 \(u_\theta\) 有关系，所以需要额外设定它不参与微分计算，否则就会 “double backpropagation”。 \(\mathcal{L}(\theta)=E\parallel u_\theta(z_t,r,t) - sg(u_{tgt})\parallel_2^2\) <img src="/images/2025-12-25-flow/image-20251012141549566.png" alt="image-20251012141549566" class="img-fluid"/></p> <h2 id="mean-flows-with-guidance">Mean Flows with Guidance</h2> <p>classifier-free guidance</p> <p>推导看得不是很懂，但是训练过程，如下</p> <p><img src="/images/2025-12-25-flow/image-20251013095354856.png" alt="image-20251013095354856" class="img-fluid"/></p> <p>有区别的地方在于式（19），\(v_t\) 是 \(\epsilon -x\)，\(u_\theta^{cfg}(z_t,t,t)\)注意后面的两个时间都是 t，它是一个速度。</p> <p>上面 \(\omega\) 表示引导的强度。对照原版的CFG，原版的CFG还在训练时是没有 \(\omega\) 的，只在采样时使用，在训练时，有一步骤是以一定概率不给class，即 \(c=\empty\)。此处，同样有这步。</p> <h1 id="2025-cvpr-reversing-flow-for-image-restoration">2025 CVPR Reversing Flow for Image Restoration</h1> <p>论文强调了<strong>不确定范围（uncertainty scope）</strong>的概念（如图1所示）：从HQ到LQ的退化过程遵循数据处理不等式（Data Processing Inequality, DPI），即HQ与中间图像之间的互信息（mutual information）逐渐减少。随着退化加深，LQ图像的“不确定范围”扩大——多个HQ图像可能退化到相似的LQ图像（例如，不同清晰图像添加雾霾后变得相似）。图1直观描绘了这一过程：灰色区域表示从中间状态的不确定范围，随着从LQ向HQ逆转，不确定范围缩小，互信息增加。</p> <p>现有生成模型（如扩散模型或分数匹配模型）通常将退化过程建模为<strong>随机变换（stochastic transformation）</strong>，从高斯噪声开始逆向生成HQ图像。这引入了不必要的复杂性和计算开销（如数百步采样），因为<strong>LQ图像已提供结构信息，无需从纯噪声重构</strong>。论文指出，这种随机性导致训练和推理效率低下，且忽略了退化过程的确定性本质。</p> <p>ResFlow的核心动机：将退化过程重新定义为<strong>确定性路径（deterministic path）</strong>，使用连续归一化流（Continuous Normalizing Flows）实现可逆映射，从而高效逆转退化，仅需少于4步采样。</p> <blockquote> <p>这是论文的第一个评论，扩散模型属于随机前向过程，论文认为是不好的。flow matching 是确定性前向过程。但这个评论和从高斯噪声开始采样不是同一个问题。</p> <p>论文认为，从高斯噪声开始采样没有必要，因为LQ图像已提供结构信息。</p> <p>有一些扩散模型的方法从LQ图像开始恢复，比如论文第二页罗列的一些。但是论文说这些方法依然是随机前向过程。However, these approaches still treat the degradation process as a progressively diffusing stochastic forward process, which seems unnecessary and introduces additional complexity and inefficiency. Given that the degraded image is already known, the degradation process could be redefined as a deterministic forward process.</p> </blockquote> <table> <thead> <tr> <th>方法</th> <th>t=0 分布（起始/目标分布）</th> <th>t=T 分布（结束/噪声分布）</th> <th>关键特点</th> </tr> </thead> <tbody> <tr> <td><strong>DDRM [38]</strong></td> <td>清晰图像分布（HQ/clean image distribution）。恢复过程的最终输出 x_0 是估计的 HQ 图像。</td> <td>近似噪声分布（approximated noise distribution，通常高斯噪声 N(0, I)）。x_T 是 Markov 链的起始，条件于退化观测 y = H x + z（H 是退化算子，z 是已知噪声）。</td> <td>基于预训练 DDPM，解决线性逆问题（如去模糊、超分辨率）。前向从 HQ 扩散到噪声；逆向从噪声恢复 HQ，条件于 LQ（degraded image）。</td> </tr> <tr> <td><strong>IR-SDE [64]</strong></td> <td>清晰图像分布（HQ/high-quality image x(0)）。</td> <td>退化图像的噪声版本（LQ + fixed Gaussian noise, μ + ε，其中 μ 是 LQ，ε ~ N(0, σ²I)）。</td> <td>使用 mean-reverting SDE 建模退化过程，从 HQ 扩散到 noisy LQ。逆向从 noisy LQ 开始恢复 HQ。非 Markov 链，而是连续 SDE。</td> </tr> <tr> <td><strong>I2SB [55]</strong></td> <td>一个给定分布（通常清晰图像分布 HQ/clean data distribution）。</td> <td>另一个给定分布（退化图像分布 LQ/degraded data distribution）。</td> <td>构建 Schrödinger bridge（扩散桥），直接连接两个分布（clean 和 degraded）。非线性扩散过程，从 LQ 桥接到 HQ。边界对：t=0 为 clean，t=T 为 degraded（或反之）。</td> </tr> <tr> <td><strong>ResShift [112]</strong></td> <td>高分辨率图像分布（HR/high-resolution image distribution）。初始状态近似 HR。</td> <td>低分辨率图像分布（LR/low-resolution image distribution）。最终状态近似 LR。</td> <td>通过 shifting residual 在 HR 和 LR 之间构建 Markov 链。针对超分辨率（SR），减少步数；t=0 为 HR，t=T 为 LR。</td> </tr> <tr> <td><strong>RDDM [57]</strong></td> <td>目标图像分布（HQ/target image distribution）。</td> <td>纯噪声分布（pure noise, N(0, I)）用于生成；或噪声携带的输入图像（noise-carrying input, LQ + noise）用于恢复。</td> <td>双重扩散：residual diffusion（从 HQ 到 LQ 的定向扩散）和 noise diffusion（随机扰动）。统一生成和恢复；t=T 根据任务调整（纯噪声或 noisy LQ）。</td> </tr> <tr> <td><strong>Resfusion [89]</strong></td> <td>原图像分布（ground truth/original image distribution）。</td> <td>noisy degraded 图像分布（noisy degraded images, LQ + weighted residual noise）。</td> <td>将 residual term 引入前向过程，从 noisy LQ 开始逆过程。预测 resnoise（weighted residual + noise）；t=0 为 HQ，t=T 为 noisy LQ。统一训练/推理。</td> </tr> </tbody> </table> <p>因此论文，要 reverses the deterministic paths between HQ and LQ images for image restoration。即要使用flow matching 方法，同时从 LQ开始进行恢复。这样的问题是前面谈到的不确定范围。</p> <p>但直接用flow来建模HQ到LQ的退化是不可以的。这是因为退化导致互信息下降（信息处理不等式），而flow对应的ODE却会保持互信息不变。见命题1</p> <h2 id="命题1">命题1</h2> <p>flow 的 OED 保持互信息不变。 \(MI(z_{t_1}, r) = MI(z_{t_2}, r),\) 其中：</p> <p>\(z_t\) 是随时间 \(t\) 变化的随机过程（random process），由普通微分方程（ODE）定义：\(\frac{\partial z_t}{\partial t} = v(z_t, t)\)。 \(r\) 是任意参考随机变量（reference random variable），可以是 \(z_t\) 的任意状态（如 \(z_0\) 或其他 \(z_{t'}\)）。 \(MI(\cdot, \cdot)\) 表示互信息，定义为两个随机变量之间的共同信息量，数学上等价于： \(MI(X, Y) = H(X) + H(Y) - H(X, Y),\) 其中 \(H(X)\) 和 \(H(Y)\) 分别是 \(X\) 和 \(Y\) 的熵，\(H(X, Y)\) 是联合熵。</p> <h2 id="逆转流的基本形式reversing-flow-for-image-restoration">逆转流的基本形式（Reversing Flow for Image Restoration）</h2> <p>退化过程定义为随机过程{zt | 0 ≤ t ≤ 1}上的ODE： \(\frac{\partial z_t}{\partial t} = v(z_t, t); \quad 0 \leq t \leq 1,\) 其中v是速度场（velocity field），z_0对应HQ图像x_HQ，z_1对应LQ图像x_LQ。</p> <table> <tbody> <tr> <td>为使过程可逆，引入辅助过程{yt</td> <td>0 ≤ t ≤ 1}，增强状态：</td> </tr> </tbody> </table> \[z_t^T = [x_t^T; y_t^T], \quad z_0^T = [x_{HQ}^T; y_0^T], \quad z_1^T = [x_{LQ}^T; y_1^T].\] <p>yt编码“信息丢失”，与不确定范围耦合：当x_t接近x_LQ时，y_t与x_0的互信息增加，以保持整体MI(z_t, z_0)恒定</p> <p>。图2框架：zt由数据组件x_t（HQ到LQ）和辅助组件y_t（不确定范围缩小）组成。前向过程通过插值定义，逆向过程通过匹配速度场学习。神经网络v_θ估计速度： \(\frac{\partial [x_t^T; y_t^T]^T}{\partial t} = v_\theta(x_t, y_t, t).\)</p> <p>在实际实现中，\(y_0=0\), \(y_1\sim N(0,I)\).从 \(y_0\) 到 \(y_1\) 熵是增加的。</p> <h2 id="resflow-训练过程算法步骤">ResFlow 训练过程算法步骤</h2> <p>论文中的训练过程基于速度场匹配（velocity field matching），通过最小化Eq. (9)的损失函数实现。采用U-Net架构作为v_θ，Adam优化器，训练在256分辨率图像crops上进行。以下是算法步骤伪代码： text算法: ResFlow 训练过程</p> <p>输入: HQ-LQ图像对数据集 {(x0, x1)}，超参数 β=10, γ=1.75, 学习率 (详见Appendix C) 输出: 训练好的速度场网络 v_θ</p> <ol> <li> <p>初始化神经网络 v_θ (采用DDPM的U-Net架构，timestep t 通过adaptive layer normalization嵌入)</p> </li> <li> <p>对于每个训练epoch: a. 从数据集采样一个batch的HQ-LQ对 (x0, x1) b. 对于每个样本: i. 设置 y0 = 0 (零向量) ii. 采样 y1 ~ N(0, I) (标准高斯分布) iii. 定义退化调度: α^x_t = 1 - t, σ^x_t = t (对于数据组件 x) α^y_t = 1 - σ^y_t, σ^y_t = β / (1 - t + β) (对于辅助组件 y, 熵保持) iv. 计算路径点 (geodesics): x_t = α^x_t * x0 + σ^x_t * x1 y_t = α^y_t * y0 + σ^y_t * y1 z_t = [x_t; y_t] (增强状态) ˙z_t = [˙α^x_t * x0 + ˙σ^x_t * x1; ˙α^y_t * y0 + ˙σ^y_t * y1] (真实速度) v. 通过网络预测速度: v_θ(x_t, y_t, t) vi. 计算时间权重: λ(t) = [cos(π/2 * (t - 2)) + 1]^γ (强调t接近1) vii. 计算损失: L = ∫_0^1 λ(t) * ||v_θ(x_t, y_t, t) - ˙z_t||^2_2 dt (积分近似或蒙特卡罗采样t) c. 平均batch损失 d. 使用Adam优化器更新 θ (反向传播)</p> </li> <li> <p>重复步骤2直到收敛 (详见Appendix C的超参数，如学习率、batch size) 注意：损失优化确保凸传输成本非增，且无需模拟ODE（与传统流方法不同）。训练强调t接近1的困难样本，以平衡梯度。 ResFlow 采样过程算法步骤 论文中的采样（推理）过程基于逆向求解ODE Eq. (6)，从LQ图像开始，仅需4步采样（uniform time schedule）。以下是算法步骤伪代码： text算法: ResFlow 采样过程 (图像恢复)</p> </li> </ol> <p>输入: 低质量图像 x1 (LQ), 训练好的 v_θ, 步数 N=4 (默认) 输出: 恢复的高质量图像 ˆx0 (HQ)</p> <ol> <li> <p>采样辅助变量: y1 ~ N(0, I) (标准高斯分布)</p> </li> <li> <p>初始化增强状态: z1 = [x1; y1]</p> </li> <li> <p>设置时间步: t 从1到0，分N=4步 (uniform schedule, e.g., Δt = 1/N)</p> </li> <li> <p>对于每个时间步 i 从1到N: a. 当前 t = 1 - (i-1)/N b. 预测速度: v = v_θ(z_t 的 x 组件, z_t 的 y 组件, t) c. 更新状态: z_{t-Δt} = z_t - v * Δt (Euler方法或更高阶如Heun求解ODE dz/dt = v(z, t)) d. (可选) 替换中间 ˆy_t 为 ground-truth y_t = α^y_t * 0 + σ^y_t * y1 (基于Eq.(5)，但概念上丢弃 ˆy_t)</p> </li> <li> <p>从最终 z0 提取 ˆx0 (丢弃 ˆy0)</p> </li> <li> <p>输出 ˆx0 作为恢复图像 (全分辨率测试) 注意：采样是确定性的（无随机噪声注入），通过辅助y消除不确定性。论文实验显示此过程在&lt;4步内完成，适用于实时应用。</p> </li> </ol> <h1 id="pnp-flow-plug-and-play-image-restoration-with-flow-matching">PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING</h1> <p>pnp 方法的洞见是 <em>the proximal</em> step on the regularization term is effectively a denoising operation. 因此近端算子可以用BM3D或神经网络。</p> <p>本文的出发点是，近来生成模型提供了智能的框架来从数据直接学习 priors，可以超越人工设计或神经网络去噪器。</p> <h2 id="图像恢复的数学问题">图像恢复的数学问题</h2> <p>在论文的引言部分，图像恢复（image restoration）问题被表述为从退化观测（degraded observation）\(y\) 恢复未知图像 \(x\) 的逆问题（inverse problem），其中 \(y = Hx + \xi\) 这里，\(H\) 是一个（线性）退化算子（degradation operator），\(\xi\) 表示加性噪声（additive noise）模型。由于该问题是病态的（ill-posed）和高维的，求解具有挑战性。 论文假设图像 \(x\) 来自具有密度 \(p_X\) 的随机变量 \(X\)，观测 \(y\) 来自具有密度 \(p_Y\) 的随机变量 \(Y\)。然后，使用最大后验（maximum a posteriori, MAP）估计器求解具有最高后验概率的值： \(\arg\max_{x \in \mathbb{R}^d} \left[ \log p_{X\mid Y=y}(x) \right] = \arg\max_{x \in \mathbb{R}^d} \left[ \log p_{Y\mid X=x}(y) + \log p_X(x) \right],\) 其中右侧第一项是数据保真（fidelity to the data），第二项是图像的先验分布（prior distribution）。 由于 \(p_X\) 通常未知，且缺乏训练数据，论文转而考虑一个正则化优化问题（regularized optimization problem）： \(\arg\min_{x \in \mathbb{R}^d} \left\{ F(x) + R(x) \right\},\) 其中 \(F(x) := -\log p_{Y\mid X=x}(y)\) 表示数据保真项（data-fidelity term），\(R : \mathbb{R}^d \to \mathbb{R}\) 通常强制对解的一些假设（enforces some assumptions on the solution），以确保（唯一）最小化器的存在。例如，对于高斯噪声 \(\mathcal{N}(0, \sigma^2 I_d)\)，数据保真项对应 \(F(x) = \frac{1}{2\sigma^2} \\mid Hx - y\\mid ^2.\) 该优化问题可以通过近端分裂方法（proximal splitting methods）有效求解。</p> <h2 id="pnp方法">PNP方法</h2> <p><img src="/images/2025-12-25-flow/image-20251014191145906.png" alt="image-20251014191145906" class="img-fluid"/></p> <h2 id="pnp-meets-flow-matching"><em>P<strong>N</strong>P</em> <em>MEETS</em> FLOW <em>MATCHING</em></h2> <p>PnP-Flow定义时间相关去噪器： \(D_t := \mathrm{Id} + (1 - t) v^\theta_t,\)</p> <blockquote> <p>这个去噪器的前提是直线路径，所以训练这个去噪器使用OT coupling 或 reflow. \(\pi\) 是耦合（optimal transport耦合可产生直线路径）。</p> </blockquote> <blockquote> <p>对于最优传输（OT）耦合，有：</p> </blockquote> \[v_t(f(t, x)) = T(x) - x, \tag{5}\] <blockquote> <p>其中 \(T\) 是Monge映射。</p> </blockquote> <p>在理想情况下： \(D_t(x) = \mathbb{E}[X_1 \mid X_t = x],\) 其中 \(X_t = (1-t)X_0 + t X_1\)。对于直线路径，去噪损失为0（Proposition 1）。</p> <p><img src="/images/2025-12-25-flow/image-20251014220345497.png" alt="image-20251014220345497" class="img-fluid"/></p> <p>注意：</p> <ol> <li>PnP flow 中的flow 是预训练的。上述算法没有训练，而是迭代的图像恢复过程。</li> <li>其中 \(t_n\) 从小到大。其中 \(\tilde{z}^n\) 这一步有使用插值，使用插值的原因在下一小节。\(t_n\) 越小，插值时，加的噪声越大。</li> </ol> <p><img src="/images/2025-12-25-flow/image-20251014230941949.png" alt="image-20251014230941949" class="img-fluid"/></p> <p>注意 \(t=0\) 时，\(\tilde{z}^n=\epsilon\)，从噪声直接去噪，再指向 \(y\)。</p> <h2 id="为什么要插值">为什么要插值</h2> <p>在PnP-Flow算法中，插入插值步（interpolation step）的目的是确保去噪器 \(D_t\) 能够有效工作，这与Flow Matching（FM）模型的特性以及算法的迭代过程密切相关。以下是详细的解释：</p> <h3 id="原因与背景">原因与背景</h3> <p>PnP-Flow结合了PnP框架与FM模型，其中去噪器 \(D_t\) 是基于预训练的FM速度场 \(v^\theta_t\) 定义的： \(D_t = \mathrm{Id} + (1 - t) v^\theta_t. \tag{6}\) 理想情况下，\(D_t(x)\) 将路径上的点 \(X_t = (1-t)X_0 + t X_1\) 投影回目标分布 \(P_1\) 的样本 \(X_1\)，其中 \(X_0 \sim P_0\)（潜在分布），\(X_1 \sim P_1\)（数据分布），\((X_0, X_1) \sim \pi\)（耦合）。然而，传统PnP-FBS算法的迭代点（通过梯度步更新后）不一定位于FM路径 \(X_t\) 上，而 \(D_t\) 的设计假设输入点在该路径上。如果输入点偏离 \(X_t\) 的支持集，去噪效果会显著下降。</p> <h3 id="插值步的必要性">插值步的必要性</h3> <p>论文在第5页（Section 3.2）中指出，经典PnP-FBS直接在梯度步后应用去噪器，但由于 \(D_t\) 针对 \(X_t\) 优化，若梯度步输出的 \(z\) 不位于 \(X_t\) 支持集，效果不佳。因此，引入插值步将 \(z\) “投影”回FM路径。</p> <h3 id="技术细节与动机">技术细节与动机</h3> <ul> <li><strong>路径一致性</strong>：FM模型（尤其是OT-FM）产生直线路径 \(X_t = (1-t)X_0 + t X_1\)。插值步确保迭代点与此路径对齐，从而利用 \(D_t\) 的最佳性能（Proposition 1表明直线路径下去噪损失为0）。</li> <li><strong>避免退化</strong>：若不插值，\(D_t\) 可能将 \(z^n\) 映射到不相关区域，特别是在 \(t\) 接近1时，\(D_t\) 趋于恒等变换（\(D_1 = \mathrm{Id}\)），导致算法退化为仅依赖数据保真项。</li> <li><strong>噪声引入</strong>：\(\epsilon \sim P_0\) 的随机性模拟FM的潜在分布采样，防止 \(D_t\) 简单地将 \(\tilde{z}^n\) 映射回 \(z^n\)（若 \(\epsilon\) 与 \(z^n\) 耦合，效果会抵消，详见论文Remark 2）。</li> </ul> <h2 id="讨论">讨论</h2> <p>pnp 方法，需要知道 \(H\)，这个比较难办?</p> <h1 id="posterior-mean-rectified-flow">POSTERIOR-MEAN RECTIFIED FLOW</h1> <p>论文《Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration》（PMRF）关注照片真实图像恢复（Photo-Realistic Image Restoration, PIR）问题，即从退化测量（如噪声、模糊图像）中重建视觉上吸引人的图像。该领域算法通常通过失真度量（如PSNR、SSIM、LPIPS）和感知质量度量（如FID、KID、NIQE）评估，目标是实现最低失真而不牺牲感知质量（即重建图像看起来自然）。 现有方法的局限性：</p> <p>后验采样（Posterior Sampling）：许多扩散或流模型（如DPS、DDS）尝试从后验分布 \(p_{X\mid Y}\) 采样，理论上可实现完美感知指数（即重建分布 \(p_{\hat{X}} = p_X\)，其中 \(p_X\) 是真实图像分布）。然而，根据Blau &amp; Michaeli (2018)，其MSE（均方误差）是无约束最小MSE（MMSE）的两倍，即 \(\mathbb{E}[\\mid X - \hat{X}\\mid ^2] = 2 \times \mathrm{MMSE}\)。 GAN+失真损失：优化失真（如MSE）和感知（如GAN）损失的加权和，可遍历失真-感知权衡曲线（Distortion-Perception Tradeoff），但GAN优化困难，尤其当感知损失权重较大时，实际性能不如后验采样。</p> <p>论文的核心动机：针对完美感知指数约束下最小化MSE的最优估计器 \(\hat{X}_0\)，定义为： \(\hat{X}_0 = \arg\min_{p_{\hat{X}\mid Y}} \mathbb{E}[\\mid X - \hat{X}\\mid ^2] \quad \mathrm{s.t.} \quad p_{\hat{X}} = p_X.\) Freirich et al. (2021)证明，\(\hat{X}_0\) 可通过先预测后验均值 \(\hat{X}^* = \mathbb{E}[X\mid Y]\)（MMSE估计），然后将其最优传输（Optimal Transport）到真实分布 \(p_X\) 来构建。该MSE通常严格小于后验采样的MSE（如图1所示）。 受此启发，PMRF提出一个简单高效算法：使用整流流（Rectified Flow）近似最优传输地图，将后验均值预测传输到高质量图像。论文通过理论分析（如Proposition 1）和实验证明，PMRF在去噪、超分辨率、补全、着色和盲面部恢复等任务中优于基线方法。 训练过程 PMRF训练分为两个阶段（见Algorithm 1），假设 \(X\) 是真实图像随机向量，\(Y\) 是退化测量。</p> <p>阶段1：后验均值预测 训练模型 \(f_\omega\) 最小化MSE损失，近似后验均值 \(\mathbb{E}[X\mid Y]\)： \(\omega^* = \arg\min_{\omega} \mathbb{E}\left[ \\mid X - f_\omega(Y)\\mid ^2 \right].\)</p> <p>此阶段可使用现成高PSNR模型跳过。 实际中，\(f_\omega\) 可为CNN或Transformer架构，优化目标是重建接近真实图像的平滑预测。</p> <p>阶段2：整流流模型训练 训练向量场 \(v_\theta\) （Rectified Flow模型），最小化流匹配损失： \(\theta^* = \arg\min_{\theta} \int_0^1 \mathbb{E}\left[ \\mid (X - Z_0) - v_\theta(Z_t, t)\\mid ^2 \right] \, dt,\) 其中：</p> <p>\(Z_t = t X + (1-t) Z_0\)，为直线路径前向过程。 \(Z_0 = f_{\omega^*}(Y) + \sigma_s \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)，\(\sigma_s\) 是小噪声超参数（缓解源/目标分布维数不匹配引起的奇异性）。 \(t \sim \mathcal{U}[0,1]\)。 此损失训练 \(v_\theta\) 预测从后验均值到真实图像的直线方向，近似最优传输地图。</p> <p>训练中，\(v_\theta\) 通常为U-Net架构，输入包括 \(Z_t\) 和时间 \(t\)（通过位置编码嵌入）。 推理过程 推理时，给定退化测量 \(y\)，PMRF解决ODE生成重建图像 \(\hat{x}\)： \(\frac{d \hat{Z}_t}{dt} = v_{\theta^*}(\hat{Z}_t, t), \quad \hat{Z}_0 = f_{\omega^*}(y) + \sigma_s \epsilon,\) 其中 \(\epsilon \sim \mathcal{N}(0, I)\)。 使用Euler方法离散求解（K步）：</p> <p>采样 \(\epsilon \sim \mathcal{N}(0, I)\)。 初始化 \(\hat{x} = f_{\omega^*}(y) + \sigma_s \epsilon\)。 对于 \(i = 0\) 到 \(K-1\)： \(\hat{x} \leftarrow \hat{x} + \frac{1}{K} v_{\theta^*}\left( \hat{x}, \frac{i}{K} \right).\)</p> <p>返回 \(\hat{x}\)。</p> <p>此过程从后验均值开始，通过整流流逐步“校正”到真实分布，生成低失真、高感知质量图像。论文实验显示，PMRF在CelebA-Test盲面部恢复基准上达到SOTA（如表1所示）。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Diffusion</title><link href="https://cekxm.github.io/blog/2025/diffusion/" rel="alternate" type="text/html" title="Diffusion"/><published>2025-12-25T13:42:24+00:00</published><updated>2025-12-25T13:42:24+00:00</updated><id>https://cekxm.github.io/blog/2025/diffusion</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/diffusion/"><![CDATA[<h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] 2022 Understanding Diffusion Models A Unified Perspective.pdf</p> <p>[2] What are Diffusion Models Lilian Weng.pdf</p> <p>[3] <a href="https://drscotthawley.github.io/blog/posts/FlowModels.html">blog - Flow With What You Know</a></p> <h3 id="toy-example">toy example</h3> <ul> <li><a href="https://github.com/varun-ml/diffusion-models-tutorial?tab=readme-ov-file">GitHub - varun-ml/diffusion-models-tutorial: Experiment with diffusion models that you can run on your local jupyter instances</a></li> <li>Classifier-Free-Guidance (CFG) https://github.com/dome272/Diffusion-Models-pytorch</li> <li>Classifier-Free-Guidance (CFG) forked from 上面的链接，增加了一些功能和一个博客 <ul> <li>https://github.com/tcapelle/Diffusion-Models-pytorch</li> <li>https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch–VmlldzoyNzIzNTQ1</li> </ul> </li> <li><a href="https://github.com/dome272/Diffusion-Models-pytorch">GitHub - dome272/Diffusion-Models-pytorch: Pytorch implementation of Diffusion Models (https://arxiv.org/pdf/2006.11239.pdf)</a></li> <li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch?tab=readme-ov-file">GitHub - lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch</a></li> </ul> <h2 id="variational-diffusion-models">Variational Diffusion Models</h2> <h3 id="markovian-hierarchical-variational-autoencoder">Markovian Hierarchical Variational Autoencoder</h3> <p>按照 [1] 的思路，首先介绍 Markovian Hierarchical Variational Autoencoder (MHVA)</p> <p><img src="/images/2025-12-25-diffusion/image-20250826145311374.png" alt="image-20250826145311374" class="img-fluid"/></p> <p>从左往右是编码，从右往左是解码。MHVA 规定，解码时，满足马尔可夫性，也就是生成 \(z_t\) 只依赖于 \(z_{t+1}\)。</p> <p>编码也是马尔可夫的，这个应该是默认的。因此有 \(p(x,z_{1:T})=p(z_T)p_\theta (x\mid z_1 )\prod_{t=2}^Tp_\theta (z_{t-1}\mid z_t) \tag{1}\)</p> \[q_\phi (z_{1:T}\mid x)=q_\phi(z_1\mid x)\prod_{t=2}^T q_\phi (z_{t}\mid z_{t-1}) \tag{2}\] <h3 id="variational-diffusion-models-可以认为是mhva再满足三个约束">Variational Diffusion Models 可以认为是MHVA再满足三个约束</h3> <ol> <li>隐藏变量和数据 x 维度一样</li> <li>编码器的结构不是学习得到的，而是设定为高斯</li> <li>设置编码器的高斯参数随时间而变化，使得T时刻 \(z_T\) 为标准高斯 \(N(0,I)\)。</li> </ol> <p><img src="/images/2025-12-25-diffusion/image-20250826151937355.png" alt="image-20250826151937355" class="img-fluid"/></p> <p>由于 z 和 x 维度一样，从这儿开始，如上图所示，统一用 x 表示。</p> <p>接下来，就是给出满足上述三个约束的 \(q(x_{t}\mid x_{t-1})\) 的参数了。然后推导得到约束3。这个推导参考文献[2]比较详细。这儿不再赘述。 \(q(x_{t}\mid x_{t-1})=N(x_t\mid \sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I )\) 在推导中，还有参数 \(\beta_t\), \(\bar{\alpha}_t\)。这几个参数可以相互转化。</p> <p>上式用<strong>参数化</strong>的技巧，就是 \(x_{t}=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} , \epsilon_{t-1}\sim N(0,I)\) 注意，参数 \(\alpha_t\) (or \(\beta_t\) or 其他变体) 的设置，可以按照预先规定的某种函数（scheduler），或者也可以学习得到。</p> <p>其次，经过推导，还有一个重要的性质 \(x_{t}=\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon , \epsilon\sim N(0,I)\) 也就是说，按编码的流程要采样得到 \(x_{t}\)，可以根据上式一步采样，不需要先采样 \(x_{1},x_{2}...x_{t-1}\)。这一点在训练中是有用的。</p> <h3 id="似然最大化elbo">似然最大化，ELBO</h3> <p>接下来，就是要通过最大似然来学习解码器 \(p_\theta (x_{t-1}\mid x_t)\) 了。具体来说，有满足某种分布的\(x\)数据集（这个 \(x\) 就是上面的 \(x_0\)），要最大化似然 \(\log p(x)=\log \int p(x_{0:T})dx_{1:T}\)</p> <blockquote> <p>这边有个疑惑，就是在后续推导中，有用到 \(q(x_{1:T}\mid x_0)\)。如果先进行编码，再进行解码，岂不是有两个不同的 \(x_1,x_2,...x_{T-1}\)。正确理解应该是，\(x_{1:T}\) 是被 marginalized out 的，所以考虑了所有可能，但每一种可能下，都只有一个值，不会有两个值。不管是p还是q，都是概率模型，所以任意值下都是可计算概率的。可以认为，先进行编码，得到了\(x_{1:T}\)，然后计算在当前模型参数下\(p_\theta (x_{t-1}\mid x_t)\)的似然，及其梯度。</p> </blockquote> <p>\(p_\theta (x_{t-1}\mid x_t)\)可以设为任意某种分布，只要知道\(z_{t-1},z_t\)（由编码生成）就可以计算概率\(p_\theta (x_{t-1}\mid x_t)\)。但是根据大段的推导，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</p> <p>推导参见参考文献。</p> <p>\(\log p(x)=\log \int p(x_{0:T})dx_{1:T}&gt;ELBO\)，ELBO由三项组成</p> <p><img src="/images/2025-12-25-diffusion/image-20250826210812707.png" alt="image-20250826210812707" class="img-fluid"/></p> <p>其中，</p> <ol> <li>第一项类似于VAE中的ELBO，可以通过蒙特卡洛估计来近似和优化。但在我看的代码中，这一项并没有使用。</li> <li>第二项没有需训练的参数，也是等于0.</li> <li>第三项可以推导出，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</li> </ol> <p>关于第一项，在代码实现中没有被使用，AI是这样回答的。</p> <blockquote> <p><img src="/images/2025-12-25-diffusion/image-20250826211846887.png" alt="image-20250826211846887" class="img-fluid"/></p> </blockquote> <p>现在进一步分析上述第三项。可以推导得到，\(q(x_{t-1}\mid x_t,x_0)\) 是一个高斯分布。因此 \(p_\theta (x_{t-1}\mid x_t)\) 也应该是高斯分布，方差和\(q(x_{t-1}\mid x_t,x_0)\)的方差相同（因为方差和\(x_0\)无关，所以可以直接设为相等），均值要尽量接近<img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826222640680.png" alt="image-20250826222640680" style="zoom:50%;"/></p> <p>可以看出 \(\mu_q\) 和 \(x_0\) 有关，而\(p_\theta (x_{t-1}\mid x_t)\) 并没有\(x_0\)的信息，所以可以训练一个神经网络 \(\hat{x}_\theta (x_t,t)\)，从\(x_t\)来预测 \(x_0\)</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223028627.png" alt="image-20250826223028627" style="zoom:50%;"/></p> <p>由于两个高斯的KL距离是可以直接计算，再经过推导，最大化ELBO就等价于，在每个timestep，神经网络的输出 \(\hat{x}_\theta (x_t,t)\)要和\(x_0\)越符合，越好。即最优解为</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223326616.png" alt="image-20250826223326616" style="zoom: 50%;"/></p> <p>上式中的系数，也可以推导等于</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223502799.png" alt="image-20250826223502799" style="zoom:50%;"/></p> <h3 id="训练过程">训练过程</h3> <p>训练采用sgd，对于每一个\(x_0\)，只随机取一个\(t\)，采样得到 \(x_t\)（不用迭代，直接采样），然后计算上式，上式即为 loss。</p> <h3 id="推理过程">推理过程</h3> <p>推理过程则需要执行完整的解码过程。从一个随机向量 \(x_T\)出发，通过神经网络，计算得到 \(\hat{x}_\theta (x_t,t)\)，然后由上面 ，计算得到均值 \(\mu_\theta(x_t,t)\)；计算后验方差（\(p_\theta\) 的方差和后验概率 \(q(x_{t-1}\mid x_t,x_0)\) 的方差相等）；采样得到 \(x_{T-1}\)，如此迭代，最后得到 \(x_0\)。</p> <h3 id="简化的loss">简化的Loss</h3> <p>见[2] 中的一些补充</p> <p>Empirically, Ho et al. (2020) 提出不带上式中系数的Loss</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250827094521508.png" alt="image-20250827094521508" style="zoom:50%;"/></p> <p>其中，\(\epsilon_t\) 是编码过程采样得到 \(x_t\) 的标准高斯噪声，\(\epsilon_\theta\) 是学习的神经网络。大概理解是这样：目标是要让解码的 \(x_{t-1}\) 分布均值接近编码的 \(x_{t-1}\) 分布均值，编码的 \(x_{t-1}\) 分布均值和 \(x_{t}\) 之间有关系，除了乘性系数外，差值为 \(\epsilon_t\) 。解码的 \(x_{t-1}\) 分布均值经过推导式子和编码时大概一样，区别是需要预测一个\(\epsilon_\theta\)。见下图 Algorithm2 Sampling step 4.</p> <p><img src="/images/2025-12-25-diffusion/image-20250827095354468.png" alt="image-20250827095354468" class="img-fluid"/></p> <h4 id="训练和推理">训练和推理</h4> <p><img src="/images/2025-12-25-diffusion/image-20250827143550828.png" alt="image-20250827143550828" class="img-fluid"/></p> <h3 id="学习扩散噪声参数">学习扩散噪声参数</h3> <p>上述，扩散噪声参数是按照scheduler确定的，在实现中，也可以联合学习得到。</p> <h2 id="noise-conditioned-score-networks-ncsn">noise-conditioned score networks (NCSN)</h2> <p>注意：本小节引用了AI的回答，其中 \(p\) 泛指概率，而不是特制反向采样中的概率分布。</p> <h3 id="分数-s_thetax_t-t">分数 \(s_\theta(x_t, t)\)</h3> <p>所谓“分数”，指的是概率密度函数的对数关于数据的梯度，即： \(s(x) = \nabla_x \log p(x)\) 其中 \(p(x)\) 是数据的概率密度函数，分数 \(s(x)\) 描述了数据在概率密度上的局部变化方向和幅度。通过训练一个模型 \(s_\theta(x)\) 来逼近真实的分数 \(\nabla_x \log p(x)\)，可以间接学习数据的概率分布，而无需直接估计 \(p(x)\) 本身。</p> <p>在扩散模型中，分数可以有两种相关定义，具体取决于上下文：</p> <ul> <li><strong>边缘分布的分数</strong>：\(\nabla_{x_t} \log p(x_t)\)，其中 \(p(x_t)\) 是时间步 \(t\) 的边缘分布，即： \(p(x_t) = \int p(x_t \mid x_0) p_{\text{data}}(x_0) \, dx_0\) 这个分数描述了 \(x_t\) 在整个数据分布上的概率密度梯度。然而，\(p(x_t)\) 通常难以直接计算，因为它涉及对所有可能的 \(x_0\) 积分。</li> <li><strong>条件分布的分数</strong>：\(\nabla_{x_t} \log p(x_t \mid x_0)\)，其中 \(p(x_t \mid x_0)\) 是前向过程中给定原始数据 \(x_0\) 的条件分布。根据扩散模型的前向过程： \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\) 这个分数的计算更直接，因为它是高斯分布的梯度： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\)</li> </ul> <p>在实际的扩散模型训练中（如 DDPM），通常使用 <strong>去噪分数匹配（Denoising Score Matching, DSM）</strong>，训练分数模型 \(s_\theta(x_t, t)\) 来逼近 <strong>条件分数</strong> \(\nabla_{x_t} \log p(x_t \mid x_0)\)，而不是边缘分数 \(\nabla_{x_t} \log p(x_t)\)。这是因为条件分数的表达式已知且易于计算，而边缘分数需要复杂的积分。</p> <h3 id="分数如何用在采样中">分数如何用在采样中</h3> <p>大体思路如下：根据扩散模型的推导，在反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计（下面第2点），而\(x_0\) 的估计和分数\(s_\theta(x_t, t)\)有关系（下面第1点）。</p> <ol> <li><strong>分数的可计算性</strong>：</li> </ol> <p>前向过程定义了 \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)，其分数可以通过高斯分布的性质直接计算： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 这个分数是已知的，因此可以作为训练目标。</p> <p>训练好的分数模型 \(s_\theta(x_t, t)\) 可以用来估计 \(x_0\)（原始数据）的值： \(s_\theta(x_t, t) \approx -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 重排后： \(x_0 \approx \frac{x_t + (1 - \bar{\alpha}_t) s_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\)</p> <ol> <li><strong>与反向过程的联系</strong>：</li> </ol> <p>反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计。分数模型 \(s_\theta(x_t, t)\) 提供了从 \(x_t\) 估计 \(x_0\) 的方法。</p> <p>均值可以通过高斯条件分布公式得到： \(\mu_q(x_t, x_0) = \sqrt{\alpha_t} x_{t-1} + \frac{(1 - \alpha_t) \sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_t}\) 将 \(s_\theta(x_t, t)\) 代入反向过程的均值表达式，DDPM 推导出简化的均值形式： \(\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} s_\theta(x_t, t) \right)\) 这个表达式直接用 \(x_t\) 和 \(s_\theta(x_t, t)\) 表示均值。</p> <p>以上公示由AI生成，用作思路理解。</p> <h3 id="和上面简化的loss中-epsilon-的关系">和上面简化的Loss中 \(\epsilon\) 的关系</h3> <p>\(\epsilon\) 是 \(x_t\) 和它分布均值之间的差。即 \(\epsilon =x_t - \sqrt{\bar{\alpha}_t} x_0\) \(\epsilon_\theta(x_t,t)\) 是它的估计。</p> <p>因此有 \(s_\theta(x_t, t) \approx -\frac{\epsilon_\theta(x_t,t)}{1 - \bar{\alpha}_t}\)</p> <h2 id="classifier-guided-diffusion">Classifier Guided Diffusion</h2> <p>分类器引导扩散中，有一个条件 \(y\)，使用了条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\) 来调整无条件扩散模型的采样过程，符合分数模型的核心思想。</p> <p><strong>分类器 \(p(y \mid x_t)\) 和无条件扩散模型 \(s_\theta(x_t, t)\) 都是预训练的</strong>，然后通过调整反向过程的均值。分类器 \(p_\phi(y \mid x_t, t)\) 是预训练的，在带噪声的图像 \(x_t \sim q(x_t \mid x_0)\) 上训练，以预测类别 \(y\)。这确保分类器梯度在不同噪声水平下可靠，为条件分数提供准确的引导。</p> <p>具体来说，论文通过以下方式从分数模型角度解释分类器引导扩散：</p> <ul> <li> <p>它将无条件分数 \(\nabla_{x_t} \log p(x_t)\)（由<strong>预训练</strong>的扩散模型提供）与分类器的梯度 \(\nabla_{x_t} \log p(y \mid x_t)\) 结合，构造条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。这一步通过贝叶斯准则可以推导。 \(\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t)\)</p> </li> <li> <p>反向过程的均值通过条件分数调整，确保采样生成符合特定类别 \(y\) 的样本。</p> </li> </ul> <p>替换为条件分数后，得到：</p> <p>\(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \nabla_{x_t} \log p(x_t \mid y) \right)\) \(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \left( s_\theta(x_t, t) + s \cdot \nabla_{x_t} \log p_\phi(y \mid x_t) \right) \right)\) 这里的 \(s_\theta(x_t, t) \approx \nabla_{x_t} \log p(x_t)\)，而 \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) 是分类器梯度，整体形成条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。</p> <blockquote> <p>“To perform conditional sampling, we train a classifier \(p_\phi(y \mid x_t)\) on noisy images to predict the class label \(y\), and use its gradient \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) to guide the diffusion process. Specifically, we modify the reverse process mean as follows: \(\tilde{\mu}_t(x_t, y) = \mu_t(x_t) + s \cdot \sigma_t^2 \cdot \nabla_{x_t} \log p_\phi(y \mid x_t)\) where \(\mu_t(x_t)\) is the original mean of the reverse process, \(\sigma_t^2\) is the variance, and \(s\) is a scale factor controlling the strength of the classifier guidance.”</p> </blockquote> <p>这边分类器梯度前面有正负不一致。先不管。</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p><strong>论文中的符号和其他文献不同，\(\lambda\) 是时间 \(t\)，但是采样时 \(\lambda\) 从小到大。\(z_\lambda\) 对应 \(x_t\)。</strong></p> <p>CFG 的核心是联合训练一个支持条件和无条件的扩散模型，然后在采样时通过线性组合分数函数来实现引导。论文将扩散模型表述在连续时间框架下，\(\lambda\) 被定义为前向过程的噪声水平，\(\lambda=\log \alpha_\lambda^2/\sigma_\lambda^2\) 即 log SNR。其中前向过程为 \(q(z_\lambda \mid x) = \mathcal{N}(\alpha_\lambda x, \sigma_\lambda^2 I)\)，\(\alpha_\lambda^2 = 1 / (1 + e^{-\lambda})\)，\(\sigma_\lambda^2 = 1 - \alpha_\lambda^2\)。</p> <h3 id="训练过程algorithm-1-in-the-paper">训练过程（Algorithm 1 in the paper）：</h3> <ol> <li> <p>使用单个神经网络参数化分数估计器 \(\epsilon_\theta(z_\lambda, c)\)（条件模型）和 \(\epsilon_\theta(z_\lambda)\)（无条件模型）。</p> </li> <li> <p>在训练时，以概率 \(p_{\text{uncond}}\)（超参数，通常为 0.1-0.2）随机将条件信息 \(c\) 设置为无条件标识符 \(\emptyset\)，从而联合训练两个模型。 训练目标是去噪分数匹配（Denoising Score Matching）：\(\mathbb{E}_{\epsilon, \lambda} [ \\mid \epsilon_\theta(z_\lambda) - \epsilon \\mid _2^2 ]\)，其中 \(z_\lambda = \alpha_\lambda x + \sigma_\lambda \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)。</p> </li> <li> <p>论文描述：“We jointly train the unconditional and conditional models simply by randomly setting \(c\) to the unconditional class identifier \(\emptyset\) with some probability \(p_{\text{uncond}}\), set as a hyperparameter.”</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110741671.png" alt="image-20250930110741671" class="img-fluid"/></p> </li> </ol> <h3 id="采样过程algorithm-2-in-the-paper">采样过程（Algorithm 2 in the paper）：</h3> <ol> <li>从纯噪声 \(z_{\lambda_T} \sim \mathcal{N}(0, I)\) 开始，逐步去噪。</li> <li>在每个时间步 \(t\)，计算引导分数：\(\tilde{\epsilon}_\theta(z_\lambda, c) = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\)，其中 \(w\) 是引导强度（guidance strength，通常 \(w &gt; 0\)）。</li> <li>使用这个引导分数来更新样本，例如在 DDPM 采样中替换原分数函数。 论文解释：“Form the classifier-free guided score at log SNR \(\lambda_t\): \(\tilde{\epsilon}_t = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\).”</li> </ol> <p>直观上，\(w = 0\) 时退化为标准条件采样；\(w &gt; 0\) 时，增强条件分数并减弱无条件分数，从而提高样本质量（更符合条件 \(c\)），但降低多样性。</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110925539.png" alt="image-20250930110925539" class="img-fluid"/></p> <p>该算法中，step 3的解释如下，见[2]</p> <p><img src="/images/2025-12-25-diffusion/image-20250930111119169.png" alt="image-20250930111119169" class="img-fluid"/></p> <p>step 4 中，\(\tilde{x}_t\) 是当前时间步的去噪数据估计，基于引导分数 \(\tilde{\epsilon}_t\)。它表示在条件 \(c\) 引导下，模型预测的原始数据 \(x\)。</p> <p>step 5中，计算均值 \(\tilde{\mu}_t\): \(\tilde{\mu}_t = \alpha_{\lambda_{t+1}} \tilde{x}_t + \sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\)</p> <p>意义：\(\tilde{\mu}_t\) 是条件反向分布 \(p(z_{\lambda_{t+1}} \mid z_{\lambda_t}, c) = \mathcal{N}(z_{\lambda_{t+1}}; \tilde{\mu}_t, \sigma_{\lambda_{t+1}}^2 I)\) 的均值，引导采样朝符合条件 \(c\) 的方向移动。 作为 \(z_t\) 和 \(\tilde{x}_t\) 的函数：公式明确显示 \(\tilde{\mu}_t\) 依赖于当前样本 \(z_t\) 和引导预测器 \(\tilde{x}_t\)。其中：</p> <p>\(\alpha_{\lambda_{t+1}} \tilde{x}_t\): 缩放后的去噪估计，代表信号部分。 \(\sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\): 噪声调整项，通过 \(\tilde{x}_t\) 和 \(z_t\) 的差值引入引导分数的影响。</p> <h3 id="简化采样">简化采样</h3> <p>实际编程中，没有按algorithm2这么复杂。还是按 DDPM 简化Loss的采样过程，不过把 \(\epsilon\) 替换成引导的 \(\tilde{\epsilon}\) 。</p> <h2 id="ddim">DDIM</h2> <p>应该可以肯定的是，DDIM和DDPM的训练过程是一样的，采样过程不一样。</p> <p>但在理论上，forward process（本论文称为 inference）不一样。DDIM引入了非马尔可夫forward process。</p> <h3 id="non-markovian-forward-processes">NON-MARKOVIAN FORWARD PROCESSES</h3> <p>前向过程是一个随机过程，用联合概率密度分布表示。由于在DDPM中，训练过程是在每一个 \(t\)，让神经网络根据 \(x_t\) 去预测分数\(\nabla_{x_t} \log p(x_t \mid x_0)\) （等价于预测 \(x_0\)，或 \(\epsilon(x_t,t)\)）。这个训练过程只和 \(q(x_t\mid x_0)\) 有关，和联合概率密度分布没有关系，实际上有无数联合概率密度分布，可以获得这个边际分布。DDPM定义了其中的具有马尔可夫性的一种。</p> <p>DDIM论文直接设计另一种联合概率分布。给定一个实数向量 \(\sigma \in R^T_{\ge 0}\)，定义联合概率密度分布：</p> <p><img src="/images/2025-12-25-diffusion/image-20251003144728914.png" alt="image-20251003144728914" class="img-fluid"/></p> <p>论文证明，上面的这个联合概率分布，满足 \(q_\sigma(x_t\mid x_0)=N(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) 也就是和DDPM中相同。所以训练过程和 DDPM 一样。</p> <p>这个联合概率分布的设计中，直接给出了后向概率分布 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)，这样在采样时，就可以直接按照这个概率分布进行采样。</p> <blockquote> <p>The magnitude of \(\sigma\) controls the how stochastic the forward process is; when \(\sigma=0\), we reach an extreme case where as long as we observe x0 and xt for some t, then \(x_{t-1}\) become known and fixed.</p> </blockquote> <p>在上面的讨论中，并没有给出潜在的前向过程，也即是如何从 \(x_0\) 逐步推理得到 \(x_T\)。论文说这个过程可以由贝叶斯公式推导得到 \(q_\sigma(x_{t}\mid x_{t-1},x_0)\)，它是一个高斯分布。由于有依赖 \(x_0\)，它不是一个马尔可夫过程。熟悉DDPM的话，可以知道，在训练时，实际上并不需要去执行这个前向过程。</p> <h3 id="sampling-from-generalized-generative-processes">SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</h3> <p>前面说过，生成过程可以直接借助 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)</p> <blockquote> <p>Intuitively, given a noisy observation \(x_t\), we first make a prediction of the corresponding \(x_0\), and then use it to obtain a sample \(x_{t-1}\) through the reverse conditional distribution \(q_\sigma(x_{t-1}\mid x_t,x_0)\), which we have defined.</p> </blockquote> <p>如果我们采用simple loss预测 \(\epsilon\)，则</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153043856.png" alt="image-20251003153043856" class="img-fluid"/></p> <p>上文中，\(f_\theta^t\) 就是对 \(x_0\) 的预测。把式（9）中\(f_\theta^t\)代入式(10)，可得</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153420286.png" alt="image-20251003153420286" class="img-fluid"/></p> <p>上式就是采样过程。其中\(\epsilon_\theta^{(t)}\) 是神经网络的输出。某一个特定的 \(\alpha\)，则退回成DDPM。</p> <p>而 \(\alpha_t=0\)，即为 DDIM。</p> <h3 id="加速采样过程">加速采样过程</h3> <p>上述讨论已经定义了DDIM，但还没涉及到加速采样。加速采样，也有类似的理论，但是训练过程不变，因此也要满足 marginal 要和 DDPM 一样。</p> <p>假设我们已经选择了时间的子集 \(\tau\)，\(\tau_S=T\)；定义联合概率密度</p> <p><img src="/images/2025-12-25-diffusion/image-20251003154717877.png" alt="image-20251003154717877" class="img-fluid"/></p> <p>也就是说， \(\tau\) 内的时间满足上面的非马尔可夫inference过程，其他时间是一个星形的概率图结构。由于marginal 要和 DDPM 一样，所以训练不变。</p> <p>在生成时，只用到了 \(\tau\) 内的时间，其他时间就不管。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Gaussiansplatting</title><link href="https://cekxm.github.io/blog/2025/gaussianSplatting/" rel="alternate" type="text/html" title="Gaussiansplatting"/><published>2025-12-25T12:57:46+00:00</published><updated>2025-12-25T12:57:46+00:00</updated><id>https://cekxm.github.io/blog/2025/gaussianSplatting</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/gaussianSplatting/"><![CDATA[<p>参考资料：</p> <p>[1] https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362/</p> <p>[2] grok</p> <p>代码：</p> <p>[1] <a href="https://github.com/yzslab/gaussian-splatting-lightning">GitHub - yzslab/gaussian-splatting-lightning: A 3D Gaussian Splatting framework with various derived algorithms and an interactive web viewer</a></p> <p>[2] <a href="https://github.com/nerfstudio-project/gsplat">GitHub - nerfstudio-project/gsplat: CUDA accelerated rasterization of gaussian splatting</a>: gsplat is an open-source library for CUDA accelerated rasterization of gaussians with python bindings.</p> <h1 id="3d-gaussian-splatting">3D Gaussian Splatting</h1> <p>本文是grok 关于gaussian splatting 反向传播的回答，但是第一部分描述了前向过程。即</p> <p>从高斯参数 → 投影 → alpha 混合 → 渲染图像</p> <p>3D Gaussian Splatting (3DGS) 的反向传播（back propagation, BP）是其训练核心，用于优化高斯点云的参数（如位置 \(\mu\), 协方差 \(\Sigma\), 不透明度 \(\alpha\), 球谐系数 SH）。与 NeRF 的 MLP 优化不同，3DGS 使用<strong>显式参数</strong>（点云属性），通过可微分渲染（differentiable rendering）计算梯度，结合 PyTorch 的 autograd 实现 BP。以下我详细解释 3DGS 的反向传播机制，结合 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库的代码（基于你的使用场景），从数学到实现，尽量图文并茂（文字描述模拟图形），并回答如何在代码中观察 BP。</p> <hr/> <h4 id="1-反向传播的总体流程">1. <strong>反向传播的总体流程</strong></h4> <p>3DGS 的训练目标是通过多视图图像（e.g., lego 数据集的 PNG + transforms.json）监督，优化高斯参数，最小化渲染图像与真实图像的损失（如 L1 + SSIM）。BP 过程涉及：</p> <ol> <li><strong>前向传播</strong>：从高斯参数 → 投影 → alpha 混合 → 渲染图像。</li> <li><strong>损失计算</strong>：渲染图像与 GT（ground truth）图像比较。</li> <li><strong>反向传播</strong>：通过 autograd 计算梯度，更新高斯参数。</li> <li><strong>密度控制</strong>：非 BP 部分，定期调整点云（克隆/分裂/剔除）。</li> </ol> <p><strong>图示（文字模拟）</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯参数: μ, Σ, α, SH] → [投影: 2D means/scales] → [alpha 混合: RGB] → [Loss: L1+SSIM]
    ↑ (梯度流回)            ↑ (梯度流回)             ↑ (梯度流回)         ← [GT 图像]
</code></pre></div></div> <hr/> <h4 id="2-数学原理可微分渲染与梯度计算">2. <strong>数学原理：可微分渲染与梯度计算</strong></h4> <p>3DGS 的渲染管道是可微分的，梯度通过链式法则从损失函数流回高斯参数。以下拆解关键步骤：</p> <h5 id="21-前向传播渲染"><strong>2.1 前向传播（渲染）</strong></h5> <ul> <li><strong>输入</strong>：N 个高斯，参数为： <ul> <li>\(\mu_i \in \mathbb{R}^3\)：中心位置。</li> <li>\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)：协方差矩阵（分解为缩放 \(S_i\) 和旋转 \(R_i\)）。</li> <li>\(\alpha_i \in [0,1]\)：不透明度（sigmoid 输出）。</li> <li>SH 系数：视图相关颜色 \(\mathbf{c}_i(\mathbf{d})\)（球谐函数）。</li> </ul> </li> <li><strong>投影（Splatting）</strong>： <ul> <li>每个高斯投影到 2D 图像平面： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(P\): 相机投影矩阵（内参）。</li> <li>\(V\): 视图矩阵（外参）。</li> <li>\(J\): 投影的雅可比矩阵（透视效应）。</li> <li>\(W\): 视图旋转部分。</li> </ul> </li> <li>2D 高斯权重：\(w_i(u,v) = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D}))\)。</li> </ul> </li> <li><strong>Alpha 混合</strong>： <ul> <li>像素颜色： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(T_i\): 透射率，模拟光线衰减。</li> <li>按深度排序（front-to-back）。</li> </ul> </li> </ul> </li> </ul> <h5 id="22-损失函数"><strong>2.2 损失函数</strong></h5> <ul> <li>比较渲染图像 \(C(\mathbf{p})\) 与 GT 图像 \(\hat{C}(\mathbf{p})\)： \(\mathcal{L} = \lambda_1 \cdot \|\hat{C} - C\|_1 + \lambda_2 \cdot (1 - \text{SSIM}(\hat{C}, C))\) <ul> <li>典型：\(\lambda_1 = 1, \lambda_2 = 0.2\)（见 <code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）。</li> </ul> </li> <li>损失是标量，驱动梯度计算。</li> </ul> <h2 id="投影和alpha混合的可微性">投影和alpha混合的可微性</h2> <hr/> <p>是的，在 <strong>3D Gaussian Splatting (3DGS)</strong> 中，<strong>投影（Projection/Splatting）</strong> 和 <strong>Alpha 混合（Alpha Blending）</strong> 两个步骤都是<strong>可微分的</strong>（differentiable）。这是 3DGS 能够通过反向传播（back propagation）优化高斯参数（如位置 \(\mu\)、协方差 \(\Sigma\)、不透明度 \(\alpha\)、球谐系数 SH）的关键原因。以下详细解释两者的可微性，结合数学推导、代码实现（基于 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库），并以图文并茂的方式说明（文字模拟图形），回答你的问题。</p> <hr/> <h3 id="1-投影projectionsplatting的可微性">1. <strong>投影（Projection/Splatting）的可微性</strong></h3> <p><strong>投影</strong>步骤将 3D 高斯点云投影到 2D 图像平面，生成每个高斯的 2D 均值 \(\mu_{2D}\)、协方差 \(\Sigma_{2D}\) 和权重 \(w_i\)，用于后续渲染。这个过程是可微的，因为所有变换（矩阵运算、指数函数等）都支持梯度计算。</p> <h4 id="11-数学原理"><strong>1.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>3D 高斯参数：\(\mu_i \in \mathbb{R}^3\)（中心位置）、\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)（协方差，分解为旋转 \(R_i\) 和缩放 \(S_i\)）、\(\alpha_i \in [0,1]\)（不透明度）。</li> <li>相机参数：视图矩阵 \(V\)（外参，4x4）、投影矩阵 \(P\)（内参，透视投影）。</li> </ul> </li> <li><strong>投影公式</strong>： <ol> <li><strong>均值投影</strong>： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) <ul> <li>\(V \cdot \mu_i\)：将 3D 位置从世界坐标系变换到相机坐标系。</li> <li>\(P\)：透视投影（e.g., \(P = \begin{bmatrix} f_x/Z &amp; 0 &amp; c_x \\ 0 &amp; f_y/Z &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)），输出 2D 像素坐标。</li> <li><strong>可微性</strong>：矩阵乘法是线性操作，透视除法 (\(/Z\)) 是可微的（雅可比矩阵 \(J\) 提供导数）。</li> </ul> </li> <li><strong>协方差投影</strong>： \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(W\)：视图矩阵 \(V\) 的 3x3 旋转部分。</li> <li>\(J\)：投影雅可比矩阵，考虑透视畸变： \(J = \begin{bmatrix} \frac{f_x}{Z} &amp; 0 &amp; -\frac{f_x X}{Z^2} \\ 0 &amp; \frac{f_y}{Z} &amp; -\frac{f_y Y}{Z^2} \end{bmatrix}\)</li> <li>\(\Sigma_i = R_i S_i S_i^T R_i^T\)：3D 协方差分解。</li> <li><strong>可微性</strong>：矩阵乘法、转置和 \(J\) 的计算（涉及除法）都可微，梯度通过链式法则流回 \(\Sigma_i\)（即 \(S_i, R_i\)）。</li> </ul> </li> <li><strong>高斯权重</strong>： \(w_i(u,v) = \alpha_i \cdot \exp\left(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\right)\) <ul> <li>\(\alpha_i\)：sigmoid 输出，可微。</li> <li>指数函数和矩阵逆（\(\Sigma_{i,2D}^{-1}\)）可微（指数导数为自身，矩阵逆导数基于线性代数）。</li> </ul> </li> </ol> </li> <li><strong>梯度流</strong>： <ul> <li>损失 \(\mathcal{L}\) 对像素颜色 \(C(\mathbf{p})\) 的梯度 \(\frac{\partial \mathcal{L}}{\partial C}\) 流回 \(w_i\)，再通过 \(w_i\) 流回 \(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i\)，最终到 \(\mu_i, \Sigma_i, \alpha_i\)。</li> </ul> </li> </ul> <h4 id="12-代码实现yzslab"><strong>1.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means3D</span> <span class="o">=</span> <span class="n">means</span>  <span class="c1"># [N, 3]
</span>    <span class="n">view_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">view</span>  <span class="c1"># [4, 4]
</span>    <span class="n">proj_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">proj</span>  <span class="c1"># [3, 4]
</span>    <span class="n">means2D</span> <span class="o">=</span> <span class="n">proj_matrix</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span> <span class="o">@</span> <span class="n">means3D</span><span class="p">)</span>  <span class="c1"># 矩阵乘法
</span>    <span class="n">J</span> <span class="o">=</span> <span class="nf">compute_jacobian</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">.</span><span class="n">focal</span><span class="p">)</span>  <span class="c1"># 雅可比
</span>    <span class="n">cov3D</span> <span class="o">=</span> <span class="nf">scales_to_cov</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">)</span>  <span class="c1"># [N, 3, 3]
</span>    <span class="n">cov2D</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">cov3D</span> <span class="o">@</span> <span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">].</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">J</span><span class="p">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">torch.matmul</code>（矩阵乘法）可微。</li> <li><code class="language-plaintext highlighter-rouge">compute_jacobian</code> 计算 \(J\)（涉及除法 <code class="language-plaintext highlighter-rouge">/Z</code>），PyTorch autograd 自动处理。</li> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 CUDA 实现的，可微分内核，基于 C++ 和 PyBind11（见 gsplat 源码）。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">viewer.py</code> 或 <code class="language-plaintext highlighter-rouge">render.py</code> 加断点，打印 <code class="language-plaintext highlighter-rouge">means2D.grad</code> 检查梯度。</li> </ul> <h4 id="13-图示文字模拟"><strong>1.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3D 高斯 [μ, Σ, α] → [矩阵乘法: V, P] → [2D 均值 μ_2D] → [雅可比 J 变换] → [2D 协方差 Σ_2D]
                                                    ↓
                                                  [高斯权重 w_i: exp(-x^T Σ_2D^-1 x)]
梯度流: Loss ← w_i ← μ_2D, Σ_2D ← μ, Σ, α
</code></pre></div></div> <hr/> <h3 id="2-alpha-混合alpha-blending的可微性">2. <strong>Alpha 混合（Alpha Blending）的可微性</strong></h3> <p><strong>Alpha 混合</strong>将投影后的 2D 高斯按深度排序，累积颜色形成像素值。这个过程也是可微的，因为涉及的排序、乘法和累积操作支持梯度计算。</p> <h4 id="21-数学原理"><strong>2.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>每个高斯的 2D 参数：\(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i, \mathbf{c}_i\)（颜色，SH 插值）。</li> <li>像素坐标 \(\mathbf{p} = (u,v)\)。</li> </ul> </li> <li><strong>混合公式</strong>： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(w_i = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\)：高斯权重。</li> <li>\(T_i\)：透射率，模拟光线穿过前 i-1 个高斯的衰减。</li> <li>按深度 \(z_i\)（相机坐标 Z）排序（front-to-back）。</li> </ul> </li> <li><strong>可微性</strong>： <ul> <li><strong>颜色</strong>：\(\frac{\partial C}{\partial \mathbf{c}_i} = w_i \cdot T_i\)，线性操作，可微。</li> <li><strong>权重</strong>：\(w_i\) 依赖 \(\alpha_i, \mu_{i,2D}, \Sigma_{i,2D}\)，已证明可微（见投影）。</li> <li><strong>透射率</strong>：\(T_i = \prod_{j=1}^{i-1} (1 - w_j)\)，乘法链可微，导数： \(\frac{\partial T_i}{\partial w_k} = -T_i \cdot \frac{1}{1 - w_k} \quad (k &lt; i)\)</li> <li><strong>排序</strong>：深度排序在渲染中是离散操作（不可微），但 3DGS 用 <strong>tile-based rasterization</strong>（分块光栅化），梯度只流经数值计算（不涉及排序本身），通过 CUDA 实现（如 gsplat 的 radix sort）。</li> </ul> </li> </ul> <h4 id="22-代码实现yzslab"><strong>2.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span>  <span class="c1"># 排序+混合
</span>    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 C++/CUDA 实现，内部计算 alpha 混合的梯度。</li> <li>排序在 tile 内（16x16 像素块），梯度流经 \(w_i, T_i\)，不依赖排序顺序。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">training_step()</code> 打印 <code class="language-plaintext highlighter-rouge">rgb.grad</code>： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rendered</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">renderer</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">camera</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">].</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 像素梯度
</span></code></pre></div> </div> </li> </ul> <h4 id="23-图示文字模拟"><strong>2.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯: c_i, w_i] → [排序: z_i] → [T_i = Π(1-w_j)] → [C = Σ(c_i * w_i * T_i)] → Loss
梯度流: Loss ← C ← c_i, w_i ← α_i, μ_i, Σ_i
</code></pre></div></div> <h2 id="排序">排序</h2> <h3 id="1-排序的频率多久进行一次">1. <strong>排序的频率：多久进行一次？</strong></h3> <p>在 3DGS 中，<strong>排序</strong>是光栅化（rasterization）的一部分，具体用于按深度（z 值）对高斯点进行排序（front-to-back），以正确执行 alpha 混合。排序的频率直接取决于<strong>渲染的调用频率</strong>，因为每次渲染都需要对高斯点进行排序。</p> <h4 id="11-排序的时机"><strong>1.1 排序的时机</strong></h4> <ul> <li><strong>渲染触发</strong>： <ul> <li>排序发生在每次<strong>前向传播</strong>（forward pass）中，即每次调用 <code class="language-plaintext highlighter-rouge">render()</code> 函数时（包括训练、验证和渲染阶段）。</li> <li>在训练中，<code class="language-plaintext highlighter-rouge">render()</code> 由 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用，每处理一个批次（batch）数据（一张或多张 lego 图像）就渲染一次，因此排序也执行一次。</li> <li><strong>频率</strong>：<strong>每训练一步（step）</strong>，即每个 batch 都会进行一次排序。</li> </ul> </li> <li><strong>训练频率</strong>： <ul> <li>你的训练日志显示 300 个 epoch，30,000 步（<code class="language-plaintext highlighter-rouge">epoch=300-step=30000.ckpt</code>），每 epoch 100 个 batch（lego 数据集 <code class="language-plaintext highlighter-rouge">train/</code> 有 100 张图像，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 默认 batch_size=1）。</li> <li><strong>排序总次数</strong>：30,000 次（每 step 一次）。</li> <li><strong>时间开销</strong>：排序由 CUDA 加速（gsplat 使用 radix sort），单次排序 ~0.1-1 毫秒（RTX 3090，~100k 高斯），占渲染时间 &lt;10%。</li> </ul> </li> <li><strong>验证/渲染阶段</strong>： <ul> <li>验证（validation）：每 <code class="language-plaintext highlighter-rouge">val_interval</code>（默认 1000 步，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）渲染验证图像，触发排序。</li> <li>手动渲染：运行 <code class="language-plaintext highlighter-rouge">python render.py --ckpt_path outputs/lego_test/checkpoints/epoch=300-step=30000.ckpt</code>，每次生成一张图像或视频帧都排序一次（e.g., 100 帧视频 → 100 次排序）。</li> </ul> </li> <li><strong>Viewer</strong>：运行 <code class="language-plaintext highlighter-rouge">python viewer.py --ply_path outputs/lego_test/checkpoints/epoch=300-step=30000-xyz_rgb.ply</code>，交互式查看时，每帧实时渲染，排序随帧率（如 60 FPS → 每秒 60 次）。</li> </ul> <h4 id="12-代码中的排序"><strong>1.2 代码中的排序</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">16</span>
    <span class="p">)</span>  <span class="c1"># 内部排序
</span>    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>排序位置</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code>（gsplat 库，<code class="language-plaintext highlighter-rouge">gsplat/_torch_impl.py</code> 和 <code class="language-plaintext highlighter-rouge">cuda/rasterize.cu</code>）在每个 tile（16x16 像素块）内按深度 \(z_i\)（从 3D 均值 \(\mu_i\) 投影得到）排序。</li> <li>代码伪逻辑（CUDA）： <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// cuda/rasterize.cu</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">tile</span><span class="o">:</span>
    <span class="n">compute_depths</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">);</span>  <span class="c1">// z_i = (V * μ_i).z</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">depths</span><span class="p">);</span>     <span class="c1">// 按 z_i 排序</span>
    <span class="n">accumulate_colors</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">colors</span><span class="p">);</span> <span class="c1">// alpha 混合</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>频率</strong>：每个 batch 的 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用 <code class="language-plaintext highlighter-rouge">render()</code>，触发一次排序（~100 次/epoch）。</li> </ul> <h4 id="13-频率总结"><strong>1.3 频率总结</strong></h4> <ul> <li><strong>训练</strong>：每 step（batch）排序一次，30,000 步 → 30,000 次排序。</li> <li><strong>验证</strong>：每 1000 步（可调，<code class="language-plaintext highlighter-rouge">val_interval</code>），~30 次。</li> <li><strong>渲染/Viewer</strong>：每帧一次（视频 100 帧 → 100 次，viewer 60 FPS → 60 次/秒）。</li> <li><strong>配置调整</strong>：<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 的 <code class="language-plaintext highlighter-rouge">batch_size</code> 或 <code class="language-plaintext highlighter-rouge">val_interval</code> 影响频率，但排序本身不可跳过（alpha 混合依赖正确顺序）。</li> </ul> <hr/> <h3 id="2-排序的可微性排序是可微的吗">2. <strong>排序的可微性：排序是可微的吗？</strong></h3> <p><strong>简答</strong>：排序（sorting）是<strong>不可微的</strong>（non-differentiable），因为它是一个离散操作（改变高斯索引顺序），无法定义连续的导数。但在 3DGS 中，排序不影响反向传播的可微性，因为梯度流绕过了排序步骤，只依赖数值计算（权重 \(w_i\) 和透射率 \(T_i\)）。这与投影和 alpha 混合的可微性不同。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">COLMAP</title><link href="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/" rel="alternate" type="text/html" title="COLMAP"/><published>2025-12-25T12:25:10+00:00</published><updated>2025-12-25T12:25:10+00:00</updated><id>https://cekxm.github.io/blog/2025/sfm-mvs-rendering</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/"><![CDATA[<h2 id="资源">资源</h2> <ol> <li>https://www.youtube.com/watch?v=diBxFGgqAT0</li> <li>https://mashaan14.github.io/YouTube-channel/nerf/2025_01_25_sfm</li> </ol> <p>这个资源解释了 SFM，two view Geometry. 结合camera_and_stereo.pdf 进行理解。</p> <h2 id="colmap">COLMAP</h2> <p><img src="/images/2025-12-25-sfm-mvs-rendering/35285f8e-982b-4ad8-96f7-1c4c5d09b5c7.png" alt="image" class="img-fluid"/></p> <p>在 COLMAP 的三维重建流水线中，<strong>BA（Bundle Adjustment，捆绑调整）</strong> 是最核心的优化环节。它的基本原理是通过<strong>非线性最小二乘法</strong>，同时优化相机的姿态、内参以及三维点的坐标，从而使重建结果达到最高的几何一致性。</p> <hr/> <h3 id="1-ba-的数学原理">1. BA 的数学原理</h3> <p>BA 的本质是一个<strong>重投影误差（Reprojection Error）</strong>最小化问题。</p> <h3 id="重投影误差公式">重投影误差公式</h3> <p>假设我们有 \(n\) 个三维点 \(X_j\) 和 \(m\) 张图像。对于每一张图像 \(i\) 和它观察到的点 \(j\)，其观测到的像素坐标为 \(x_{ij}\)。而根据当前估计的相机姿态 \(R_i, t_i\)、内参 \(K_i\) 和点 \(X_j\) 计算出的<strong>理论投影坐标</strong>为 \(\hat{x}_{ij} = \pi(K_i, R_i, t_i, X_j)\)。</p> <p>BA 的目标函数可以表示为：</p> \[\min_{K_i, R_i, t_i, X_j} \sum_{i=1}^m \sum_{j=1}^n \rho \left( \| x_{ij} - \pi(K_i, R_i, t_i, X_j) \|^2 \right)\] <ul> <li><strong>\(\pi\)</strong>：投影函数（将 3D 点映射到 2D 像素平面）。</li> <li><strong>\(\| x_{ij} - \hat{x}_{ij} \|^2\)</strong>：重投影误差。</li> <li><strong>\(\rho\)</strong>：鲁棒核函数（如 Huber loss），用于减少外点（错误匹配）对优化的干扰。</li> </ul> <h3 id="优化算法">优化算法</h3> <p>由于投影方程是非线性的，COLMAP 调用 <strong>Ceres Solver</strong> 库，使用 <strong>Levenberg-Marquardt (LM)</strong> 算法进行迭代求解。</p> <hr/> <h3 id="2-ba-在-colmap-中的作用">2. BA 在 COLMAP 中的作用</h3> <p>BA 贯穿于 COLMAP 的增量式重建（Incremental SfM）全过程，主要起到以下作用：</p> <ul> <li><strong>消除累积误差（去漂移）</strong>：在逐张添加图像的过程中，误差会不断累积。BA 能够通过全局约束，将这些微小的误差平摊，防止模型“弯曲”或“变形”。</li> <li><strong>精细化参数</strong>：初始的相机姿态通常由 PnP 算法得到，三维点坐标由三角化得到。BA 能够将这些“毛坯”数据进一步精细化，提高点云的精度。</li> <li><strong>自标定（Self-Calibration）</strong>：如果相机内参未知或不准，BA 可以通过优化过程自动校正焦距（Focal Length）和畸变参数。</li> <li><strong>剔除外点</strong>：在 BA 优化后，重投影误差依然很大的点会被视为无效点（Outliers）并被剔除，从而保证重建的鲁棒性。</li> </ul> <hr/> <h3 id="3-colmap-中的两种-ba-策略">3. COLMAP 中的两种 BA 策略</h3> <p>为了平衡计算速度和精度，COLMAP 将 BA 分为两个级别：</p> <table> <thead> <tr> <th><strong>类型</strong></th> <th><strong>触发时机</strong></th> <th><strong>优化范围</strong></th> <th><strong>目的</strong></th> </tr> </thead> <tbody> <tr> <td><strong>局部 BA (Local BA)</strong></td> <td>每注册一张新图像后</td> <td>当前图像及其邻近图像、可见的三维点</td> <td>确保新加入的图像能够稳定地融合进当前模型。</td> </tr> <tr> <td><strong>全局 BA (Global BA)</strong></td> <td>模型增长达到一定比例时</td> <td><strong>所有</strong>已注册的图像、<strong>所有</strong>三维点</td> <td>消除全局漂移，确保整个场景的几何结构严丝合缝。</td> </tr> </tbody> </table> <h3 id="4-估计的-k-r-t-x-是作为-ba-的初值吗">4. 估计的 \(K, R, t, X\) 是作为 BA 的初值吗？</h3> <p><strong>是的。</strong> BA 本质上是一个<strong>非线性优化</strong>过程，非线性优化必须依赖一个合理的<strong>初始值</strong>（Initial Guess）才能收敛到全局最优解，否则很容易陷入局部极小值。</p> <p>在 COLMAP 的增量式重建流程中，这些初值的来源如下：</p> <ul> <li><strong>相机姿态 (\(R, t\))</strong>：通过 <strong>PnP (Perspective-n-Point)</strong> 算法获得。当一张新图像被注册到现有模型时，系统利用已有的 3D 点和该图像中的 2D 特征匹配点，计算出该图像的位姿。</li> <li><strong>三维点坐标 (\(X\))</strong>：通过 <strong>三角化 (Triangulation)</strong> 获得。利用多张图像的相机中心和匹配特征射线的交点来估计点的空间位置。</li> <li><strong>内参 (\(K\))</strong>：通常来源于图像的 <strong>EXIF 信息</strong>（如焦距）或预设的相机模型。在 BA 过程中，这些参数可以被进一步精细化。</li> </ul> <p><strong>一句话总结：</strong> PnP 和三角化为 BA 提供了“毛坯”模型，而 BA 负责“精加工”。</p> <hr/> <h3 id="5-ba-是逐一优化还是同时优化">5. BA 是逐一优化还是同时优化？</h3> <p>BA 是<strong>同时优化（Joint Optimization）</strong>所有参数的。这意味着在同一个优化循环中，相机参数和三维点坐标是同时更新的。</p> <h4 id="为什么要同时优化">为什么要同时优化？</h4> <p>如果采用“逐一优化”（先固定点优化相机，再固定相机优化点），模型收敛速度会极慢，且容易陷入死循环或无法达到全局最优，这种方法被称为“坐标下降法”。</p> <h4 id="使用的数学技巧舒尔补-schur-complement">使用的数学技巧：舒尔补 (Schur Complement)</h4> <p>由于 BA 涉及成千上万个三维点和数百个相机位姿，直接求逆巨大的海森矩阵（Hessian Matrix）在计算上是不可行的。为了实现高效的<strong>同时优化</strong>，COLMAP（通过其背后的 <strong>Ceres Solver</strong>）使用了一个核心数学技巧：<strong>利用稀疏结构的舒尔补（Schur Complement）</strong>。</p> <h4 id="核心步骤">核心步骤：</h4> <ol> <li> <p>稀疏性分析：</p> <p>在 BA 中，一个 3D 点的投影只与观察到它的相机有关，与其他点无关；同样，一个相机的残差只与它看到的点有关。这导致其对应的正态方程（Normal Equations）具有极其显著的块稀疏结构。</p> \[\begin{bmatrix} B &amp; E \\ E^T &amp; C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v \\ w \end{bmatrix}\] <p>其中 \(B\) 是相机参数块，\(C\) 是三维点参数块（它是对角块矩阵，求逆非常快）。</p> </li> <li> <p>边缘化（Marginalization）：</p> <p>利用舒尔补将三维点块 \(C\) 消去，得到一个只包含相机参数的缩减相机系统（Reduced Camera System）：</p> \[(B - EC^{-1}E^T) \Delta x_c = v - EC^{-1}w\] <p>这个矩阵的维度大大降低（仅取决于相机的数量），求解出相机的增量 \(\Delta x_c\) 后，再通过代回（Back-substitution）快速算出所有点的增量 \(\Delta x_p\)。</p> </li> </ol> <h4 id="其他关键技术">其他关键技术：</h4> <ul> <li><strong>Levenberg-Marquardt (LM) 算法</strong>：一种介于高斯-牛顿法和梯度下降法之间的信赖域方法，保证了在远离最优解时也能稳健收敛。</li> <li><strong>鲁棒核函数 (Robust Loss)</strong>：如 Huber 或 Cauchy 损失，用来自动降低那些误匹配点（Outliers）在优化中的权重，防止它们带偏整个模型。</li> </ul> <p>在 COLMAP 的流水线中，参与 BA 的图像数量并不是一个固定值，它取决于<strong>你所处的重建阶段</strong>以及<strong>场景的规模</strong>。</p> <p>COLMAP 作为一种<strong>增量式 SfM（Incremental SfM）</strong>工具，它通过两种不同规模的 BA 来平衡精度和效率：</p> <h3 id="6-局部-ba-local-bundle-adjustment">6. 局部 BA (Local Bundle Adjustment)</h3> <ul> <li><strong>图像数量：通常为 10 到 30 张左右。</strong></li> <li><strong>原理：</strong> 每当 COLMAP 成功注册（添加）一张新图像并进行三角化后，它不会立即对全场进行优化（太慢了），而是执行一次局部 BA。</li> <li><strong>范围：</strong> 仅包含<strong>当前图像及其在共视图中最近的邻居图像</strong>。</li> <li><strong>作用：</strong> 确保新加入的图像能够正确地锚定在现有模型上，防止局部结构发生剧烈畸变。</li> </ul> <h3 id="7-全局-ba-global-bundle-adjustment">7. 全局 BA (Global Bundle Adjustment)</h3> <ul> <li><strong>图像数量：所有已注册的图像（从几十张到上万张不等）。</strong></li> <li><strong>原理：</strong> 当模型增长到一定程度（例如图像数量增加了 10%，或者达到了特定的步长），COLMAP 会触发全局 BA。</li> <li><strong>范围：</strong> 优化当前已成功进入模型的所有相机位姿和所有 3D 点。</li> <li><strong>规模示例：</strong> <ul> <li><strong>小型物体：</strong> 50 - 200 张图像。</li> <li><strong>中型场景（如建筑）：</strong> 500 - 2,000 张图像。</li> <li><strong>大型城市/测绘：</strong> 5,000 - 10,000+ 张图像。</li> </ul> </li> <li><strong>作用：</strong> 消除长时间增量重建积累的“漂移”误差，确保模型整体的闭环精度。</li> </ul> <hr/> <h3 id="8-影响参与-ba-图像数量的瓶颈">8. 影响参与 BA 图像数量的瓶颈</h3> <p>虽然理论上可以有成千上万张图像参与 BA，但实际操作中受以下因素限制：</p> <ol> <li><strong>内存（RAM）与显存：</strong> <ul> <li>BA 需要构建庞大的 Jacobi 矩阵。虽然有“舒尔补”技巧减小计算量，但当图像超过 <strong>2,000-3,000 张</strong>时，对内存的需求会显著增加。</li> <li>COLMAP 默认使用 <strong>Ceres Solver</strong>，如果内存不足，BA 可能会失败或运行极其缓慢。</li> </ul> </li> <li><strong>计算时间：</strong> <ul> <li>全局 BA 是 SfM 中最耗时的部分。对于上万张图的场景，一次全局 BA 可能需要几小时甚至更久。</li> </ul> </li> <li><strong>连接性（Connectivity）：</strong> <ul> <li>如果图像之间没有共同的特征点（即没有“边”连接），这些图像就不会在同一个 BA 块中被优化。COLMAP 会将它们拆分成不同的子模型（Sub-models）。</li> </ul> </li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[SFM]]></summary></entry></feed>
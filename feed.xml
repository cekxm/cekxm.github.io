<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cekxm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cekxm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-04T09:55:22+00:00</updated><id>https://cekxm.github.io/feed.xml</id><title type="html">计算机视觉</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Tracking</title><link href="https://cekxm.github.io/blog/2026/tracking/" rel="alternate" type="text/html" title="Tracking"/><published>2026-02-04T09:49:40+00:00</published><updated>2026-02-04T09:49:40+00:00</updated><id>https://cekxm.github.io/blog/2026/tracking</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/tracking/"><![CDATA[<h2 id="samv2">SAMv2</h2> <h2 id="dam4samv2">DAM4SAMv2</h2> <p>[jovanavidenovic/DAM4SAM: <a href="https://github.com/jovanavidenovic/DAM4SAM">CVPR 2025] “A Distractor-Aware Memory for Visual Object Tracking with SAM2”</a></p> <p><a href="https://github.com/SebastianJanampa/DAM4SAMv2#">SebastianJanampa/DAM4SAMv2: This repository is based on the DAM4SAM repo. It includes more features such as creating mask using bounding boxes and points.</a></p> <p>速度慢，不适合实时跟踪，但对变形的目标和遮挡效果很不错。</p> <h2 id="mixformerv2">MixFormerV2</h2> <p>[MCG-NJU/MixFormerV2: <a href="https://github.com/MCG-NJU/MixFormerV2/tree/main">NeurIPS 2023] MixFormerV2: Efficient Fully Transformer Tracking</a></p> <h2 id="siamabc">SiamABC</h2> <p><a href="https://github.com/wvuvl/SiamABC">wvuvl/SiamABC: Improving Accuracy and Generalization for Efficient Visual Tracking</a></p> <p>WACV 2025，安装需要 pytorch 1.7.1</p>]]></content><author><name></name></author><summary type="html"><![CDATA[单目标跟踪]]></summary></entry><entry><title type="html">Diffusion Transformer</title><link href="https://cekxm.github.io/blog/2026/DiTF/" rel="alternate" type="text/html" title="Diffusion Transformer"/><published>2026-01-10T10:45:11+00:00</published><updated>2026-01-10T10:45:11+00:00</updated><id>https://cekxm.github.io/blog/2026/DiTF</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/DiTF/"><![CDATA[<h2 id="dit">DiT</h2> <p><strong>Diffusion Transformer (DiT)</strong> 是一种将 <strong>Transformer 架构</strong>与<strong>扩散模型（Diffusion Models）</strong>相结合的新型生成模型架构。它在图像生成领域逐渐取代了传统的以 UNet 为核心的架构（如早期的 Stable Diffusion），并成为当前最先进模型（如 SD3、Flux、Sora）的核心底座 。 以下是根据您提供的论文资料对 DiT 的详细介绍：</p> <h3 id="1-核心定义与架构">1. 核心定义与架构</h3> <p>DiT 摒弃了扩散模型中常用的带有下采样和上采样操作的 UNet 架构，转而使用完全基于 <strong>Transformer</strong> 的块（Blocks）来处理图像数据 。</p> <ul> <li><strong>图像 Patch 化</strong>：输入图像或潜空间表示（Latent）首先被切分为一个个小方块（Patches），并展平为一串序列（Tokens），这与 Vision Transformer (ViT) 的处理方式一致 。</li> <li><strong>AdaLN-zero 调制</strong>：这是 DiT 的关键技术之一。它根据扩散的时间步（Timestep）和文本条件，回归出缩放（Scale）和偏移（Shift）参数，通过 <strong>自适应层归一化（AdaLN）</strong> 来调制 Transformer 块内的特征 。</li> <li><strong>多模态交互（MM-DiT）</strong>：在 SD3 等模型中，DiT 演进为多模态架构，拥有独立的流（Streams）来分别处理视觉特征和文本 Token，并通过注意力机制实现两者的双向信息流转 。 <h3 id="2-dit-的主要优势">2. DiT 的主要优势</h3> <p>相比传统的 UNet 架构，DiT 展现出了以下优越性：</p> </li> <li><strong>可扩展性（Scalability）</strong>：DiT 非常适合通过增加参数量和训练数据来提升性能（Scaling law） 。</li> <li><strong>生成质量与鲁棒性</strong>：在细节生成和图像质量方面表现更好，具有更强的空间感知和语义理解能力 。</li> <li><strong>模型效率</strong>：Transformer 架构在现代硬件上具有极高的计算效率 。 <h2 id="dit-代表">DiT 代表</h2> <p>在当前的人工智能领域，<strong>DiT (Diffusion Transformer)</strong> 架构已经成为生成式模型的主流选择。以下是 DiT 架构最具有代表性的几个模型及其应用场景：</p> <h3 id="1-基础鼻祖dit-peebles--xie-2023">1. 基础鼻祖：DiT (Peebles &amp; Xie, 2023)</h3> </li> <li><strong>地位</strong>：这是该架构的开山之作。</li> <li><strong>特点</strong>：论文《Scalable Diffusion Models with Transformers》首次证明了使用 Transformer 替代传统的 UNet 作为扩散模型的主干网络是可行的，并且具有极强的<strong>可扩展性（Scaling Law）</strong>。</li> <li><strong>代表性设计</strong>：引入了 <strong>AdaLN-Zero</strong>（自适应层归一化）来将时间步和类别信息注入模型。 <h3 id="2-图像生成领域的标杆">2. 图像生成领域的标杆</h3> </li> <li><strong>Stable Diffusion 3 (SD3)</strong>： <ul> <li><strong>地位</strong>：Stability AI 推出的最强开源图像生成模型之一。</li> <li><strong>创新</strong>：使用了 <strong>MM-DiT（Multimodal DiT）</strong>，即文本和图像拥有各自独立的权重流，但在注意力层中进行交互。</li> </ul> </li> <li><strong>Flux.1</strong>： <ul> <li><strong>地位</strong>：目前开源界公认的画质与文字生成最强的模型（由原 SD 核心团队开发的 Black Forest Labs 推出）。</li> <li><strong>特点</strong>：基于大规模参数的 DiT 架构，解决了之前模型在复杂指令遵循和人类手部生成上的难题。</li> </ul> </li> <li><strong>PixArt-α / PixArt-Σ</strong>： <ul> <li><strong>特点</strong>：主打高效训练的 DiT 模型，通过解耦训练策略，在较小的计算资源下达到了接近 Midjourney 的生成质量。 <h3 id="3-视频生成领域的统治者">3. 视频生成领域的统治者</h3> </li> </ul> </li> <li><strong>Sora (OpenAI)</strong>： <ul> <li><strong>地位</strong>：视频生成领域的里程碑。</li> <li><strong>技术核心</strong>：OpenAI 明确指出 Sora 是一种 <strong>Diffusion Transformer</strong>。它将视频切分为时空补丁（Spacetime Patches），在 Transformer 架构中处理，实现了极长且一致的视频生成。</li> </ul> </li> <li><strong>可灵 (Kling) / Vidu</strong>： <ul> <li><strong>地位</strong>：国产大模型中性能顶尖的视频生成模型，其底层架构同样深度参考了 DiT 的时空建模方案。 <h2 id="pixart-α">PixArt-α</h2> <p><img src="/images/2026-01-10-DiTF/image-20260110184036225.png" alt="image-20260110184036225" class="img-fluid" data-zoomable=""/></p> <h2 id="ditf">DiTF</h2> <p>DiFT（Diffusion Transformer Feature）是一项发表于 <strong>NeurIPS 2025</strong> 的研究，其核心结论围绕着如何“释放”预训练 Diffusion Transformer (DiT) 模型在视觉感知（如视觉对应任务）中的潜力。 以下是该论文的主要结论：</p> <h3 id="1-发现-dit-特征中的巨量激活现象-massive-activations">1. 发现 DiT 特征中的“巨量激活”现象 (Massive Activations)</h3> </li> </ul> </li> <li><strong>现象描述</strong>：与传统的 Stable Diffusion (SD) 模型不同，DiT 模型（如 SD3、Flux）在提取特征时，极少数维度的激活值会比其他维度高出 <strong>100 倍以上</strong> 。</li> <li><strong>空间特性</strong>：这些巨量激活并不是随机分布的，而是<strong>固定集中在极少数特定的通道维度</strong>上（例如 SD3-5 在第 676 维，Flux 在第 154 和 1446 维），且出现在图像的所有 patch token 中 。</li> <li><strong>负面影响</strong>：由于这些极值维度不包含局部语义信息，会导致特征向量在余弦相似度计算时表现出极高的相似性，使得特征变得<strong>无区分度</strong>，从而导致在视觉任务中表现糟糕 。 <h3 id="2-揭示巨量激活与-adaln-层的关联">2. 揭示巨量激活与 AdaLN 层的关联</h3> </li> <li><strong>核心成因</strong>：研究发现这些巨量激活的维度与 DiT 内部的 <strong>自适应层归一化 (AdaLN)</strong> 产生的残差缩放因子 \(\alpha_k\) 高度一致 。</li> <li><strong>AdaLN 的双重作用</strong>：虽然 AdaLN 参与了巨量激活的形成，但它同时也具备<strong>定位和抑制</strong>这些激活的能力 。研究证明，通过 AdaLN 进行通道调制，可以有效归一化这些异常值 。 <h3 id="3-提出了免训练的-ditf-提取框架">3. 提出了免训练的 DiTF 提取框架</h3> </li> <li><strong>AdaLN 调制</strong>：DiTF 并不是直接提取原始特征，而是利用模型内置的 AdaLN 对特征进行<strong>通道维度的缩放和偏移调制</strong>。这种操作显著增强了特征的空间语义一致性和边界清晰度 。</li> <li><strong>通道舍弃策略 (Channel Discard)</strong>：对于调制后仍残留的微弱巨量激活维度，DiTF 采用直接<strong>置零</strong>的策略，进一步消除噪声 。</li> <li><strong>无需训练</strong>：该框架是一个 <strong>Training-free</strong>（免训练）的方案，可以直接应用于现有的预训练 DiT 模型 。 <h3 id="4-性能达到-sota-并与-dinov2-互补">4. 性能达到 SOTA 并与 DINOv2 互补</h3> </li> <li><strong>性能优越</strong>：DiTF 在多个视觉对应任务（如 SPair-71k, AP-10K）上超越了基于 DINOv2 和基于 SD 的模型 。例如，DiTF(Flux) 在 SPair-71k 上的表现比之前的 DIFT 高出 <strong>9.4%</strong> 。</li> <li><strong>特征互补性</strong>：实验显示，DiTF 与 DINOv2 的特征具有互补性。将两者结合使用时，性能会进一步大幅提升（例如在 SPair-71k 上从 64.6% 提升至 <strong>72.2%</strong>） 。 <h2 id="ditf-vs-dino2">DiTF vs DINO2</h2> <p>DiT 特征（DiTF）并非简单的直接提取，而是一种<strong>免训练的、基于 AdaLN（自适应层归一化）调制的增强特征提取框架</strong>。 以下是其与 DINOv2 的区别以及优于后者的原因：</p> <h3 id="1-dit-特征与-dinov2-的本质区别">1. DiT 特征与 DINOv2 的本质区别</h3> </li> <li><strong>提取机制不同</strong>：DINOv2 是通过自监督学习直接训练的视觉骨干网络，其特征可直接提取使用 。而 DiT（如 SD3、Flux）在直接提取原始特征时，会由于“<strong>巨量激活</strong>”（Massive Activations）现象导致表现极差 。DiTF 框架通过 DiT 内部自带的 <strong>AdaLN 层</strong>对这些异常激活进行通道维度的调制（缩放和偏移），从而提取出具有语义区分度的特征 。</li> <li><strong>解决的核心问题不同</strong>：DINOv2 侧重于通用视觉表征；而 DiTF 侧重于消除 DiT 特征中极少数固定维度上的极端激活值（这些值比中值大 100 倍以上，且不包含局部信息），使模型能够像 Stable Diffusion (SD) 一样有效地用于视觉感知任务 。 <h3 id="2-为什么-dit-特征优于-dinov2">2. 为什么 DiT 特征优于 DINOv2？</h3> <p>研究表明，DiTF 在视觉对应（Visual Correspondence）任务中建立了一系列新的性能标杆，超越了 DINOv2 和基于 SD 的模型 ：</p> </li> <li><strong>更强的语义边界和一致性</strong>：经过 AdaLN 调制后的 DiT 特征（Post-AdaLN）在空间语义相干性上显著优于 DINOv2 。可视化显示，它能更清晰地界定物体部位的语义边界，而原始特征或某些自监督特征在区分物体与背景时相对较弱 。</li> <li><strong>强大的生成先验</strong>：DiT 模型（如 SD3、Flux）在大规模文本到图像生成任务中预训练，具备极强的空间感知和语义理解能力 。通过 DiTF 框架“释放”这些潜能后，其特征在处理复杂语义对应（如跨物种对应）时表现出更强的鲁棒性 。</li> <li><strong>特征互补性</strong>：虽然 DiTF 本身性能强劲，但它与 DINOv2 特征具有互补性 。实验显示，将 DiTF 与 DINOv2 特征集成后，性能会进一步大幅提升（例如在 Spair-71k 数据集上从 64.6% 提升至 72.2%） 。 <h3 id="3-ditf-的核心改进手段">3. DiTF 的核心改进手段</h3> <ol> <li><strong>AdaLN 通道调制</strong>：利用 DiT 内部的 AdaLN 层，自适应地定位并归一化巨量激活，增强特征的区分度 。</li> <li><strong>通道舍弃策略（Channel Discard）</strong>：在调制后，主动丢弃（置零）那些仍然存在的、包含局部信息极少的异常维度，以进一步消除对表征学习的负面影响 。</li> </ol> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[DiT, SD3, Flux, PixArt, DiTF]]></summary></entry><entry><title type="html">Under water enhancement</title><link href="https://cekxm.github.io/blog/2026/uie/" rel="alternate" type="text/html" title="Under water enhancement"/><published>2026-01-10T03:49:29+00:00</published><updated>2026-01-10T03:49:29+00:00</updated><id>https://cekxm.github.io/blog/2026/uie</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/uie/"><![CDATA[<p><img src="/images/2026-01-10-uie/image-20260110114242376.png" alt="image-20260110114242376" class="img-fluid" data-zoomable=""/></p> <h2 id="five-a-network">Five A+ Network</h2> <p>JIANG J, YE T, CHEN S, et al. Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement [C]// Proceedings of the 34th British Machine Vision Conference (BMVC). 2023.</p> <p><img src="/images/2026-01-10-uie/image-20260110114546113.png" alt="image-20260110114546113" class="img-fluid" data-zoomable=""/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[UIE, U-trans, DNnet, Five A+]]></summary></entry><entry><title type="html">Optical Flow</title><link href="https://cekxm.github.io/blog/2026/optical_flow/" rel="alternate" type="text/html" title="Optical Flow"/><published>2026-01-09T07:08:29+00:00</published><updated>2026-01-09T07:08:29+00:00</updated><id>https://cekxm.github.io/blog/2026/optical_flow</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/optical_flow/"><![CDATA[<h2 id="raft">RAFT</h2> <p><img src="/images/2026-01-09-optical_flow/image-20260108232610993.png" alt="image-20260108232610993" class="img-fluid" data-zoomable=""/></p> <h3 id="核心架构与组成部分">核心架构与组成部分</h3> <p>RAFT 主要由以下三个核心阶段组成：</p> <ul> <li><strong>特征提取（Feature Extraction）：</strong> <ul> <li>使用一个卷积特征编码器对两帧输入图像进行处理，提取每个像素的特征向量 。</li> <li>此外，还包含一个上下文编码器（Context Encoder），仅从第一帧图像中提取特征，用于后续迭代更新时的补充信息。</li> </ul> </li> <li><strong>计算视觉相似度（Computing Visual Similarity）：</strong> <ul> <li><strong>4D 相关体（4D Correlation Volumes）：</strong> 通过计算两帧图像特征向量之间的内积，构建一个包含所有像素对相关性的 4D 体。</li> <li><strong>相关性金字塔（Correlation Pyramid）：</strong> 对 4D 相关体进行多尺度池化，生成一组不同分辨率的相关性卷。这种设计使模型能同时获取大位移和小位移的信息，且由于保留了高分辨率维度，能更好地捕捉小而快移动物体的运动。</li> </ul> </li> <li><strong>迭代更新（Iterative Updates）：</strong> <ul> <li><strong>GRU 单元：</strong> 采用一个基于门控循环单元（GRU）的轻量级更新模块，它通过在相关性金字塔中执行“查表”操作来检索特征，并迭代地更新光流场。</li> <li>与之前常见的“由粗到精”（coarse-to-fine）设计不同，RAFT 始终在固定高分辨率下维护和更新单一的光流场。</li> </ul> </li> </ul> <p>在 RAFT（Recurrent All-Pairs Field Transforms）光流估计方法中，<strong>不需要</strong>像传统的“由粗到精”（coarse-to-fine）方法那样对图像或特征图进行显式的 Warp（重采样/扭曲）操作 。</p> <h2 id="sea-raft">SEA-RAFT</h2> <p><img src="/images/2026-01-09-optical_flow/image-20260108233119443.png" alt="image-20260108233119443" class="img-fluid" data-zoomable=""/></p> <p>SEA-RAFT 旨在通过对原始 RAFT 架构的优化，实现更简单（Simple）、更高效（Efficient）和更准确（Accurate）的光流估计 2。其核心改进包括引入了新的损失函数、直接回归初始光流、刚体运动预训练以及架构简化。</p> <h3 id="核心改进与技术特点">核心改进与技术特点</h3> <ul> <li><strong>混合拉普拉斯损失（Mixture of Laplace Loss）：</strong> <ul> <li>SEA-RAFT 摒弃了标准 RAFT 使用的 \(L_1\) 损失，转而训练网络预测混合拉普拉斯分布的参数。</li> <li>该设计允许模型量化预测的不确定性，特别是在重度遮挡导致的歧义情况下，通过不同的混合分量来应对，从而显著减少过拟合并提升泛化能力。</li> </ul> </li> <li><strong>直接回归初始光流（Directly Regressed Initial Flow）：</strong> <ul> <li>原始 RAFT 将光流初始化为零，这往往偏离真实值较远，需要大量迭代才能收敛。</li> <li>SEA-RAFT 通过复用现有的上下文编码器（Context Encoder），直接根据输入的双帧图像预测一个初始光流估计。</li> <li>这一简单的改变以极低的额外开销大幅减少了所需的迭代次数，提升了效率。</li> </ul> </li> <li><strong>大尺度刚体运动预训练（Rigid-Flow Pre-Training）：</strong> <ul> <li>模型首先在 TartanAir 数据集上进行预训练。虽然该数据集仅包含由相机运动产生的静态场景位移（刚体运动），但其高度的真实感和场景多样性显著增强了模型的跨数据集泛化能力。</li> </ul> </li> <li><strong>架构简化（Architectural Simplifications）：</strong> <ul> <li><strong>标准化主干：</strong> 将 RAFT 原有的定制化编码器替换为标准的 ResNet 模型，使得训练更加稳定。</li> <li>高效 RNN：** 将原始的卷积 GRU 替换为由 ConvNext 模块组成的简单 RNN，这使得模型更容易集成新的神经构建块，且更易于扩展到大规模数据集。</li> </ul> </li> </ul> <h2 id="flowseek">FlowSeek</h2> <p>M. Poggi and F. Tosi, “FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2025, pp. 5667-5679.</p> <p><img src="/images/2026-01-09-optical_flow/image-20260109130959464.png" alt="image-20260109130959464" class="img-fluid" data-zoomable=""/></p> <p>FlowSeek 是一种在 2025 年 ICCV 上提出的新型光流估计方法，其核心理念是通过引入<strong>深度基础模型（Depth Foundation Models）</strong>和<strong>经典运动基底（Motion Bases）</strong>来简化光流任务的训练并提升其泛化能力。</p> <p>以下是 FlowSeek 方法的详细介绍：</p> <h3 id="1-核心设计理念融合三大领域">1. 核心设计理念：融合三大领域</h3> <p>FlowSeek 的架构设计处于以下三个领域的交汇点：</p> <ul> <li><strong>先进光流架构：</strong> 以 <strong>SEA-RAFT</strong> 作为骨干网络（Backbone），继承了其混合拉普拉斯损失和高效的迭代更新机制。</li> <li><strong>深度基础模型 (VFMs)：</strong> 集成了如 <strong>Depth Anything v2</strong> 等在大规模数据集上预训练的模型，利用其丰富的几何和语义先验信息。</li> <li><strong>经典运动参数化：</strong> 引入了 30 年前经典的低维运动基底理论，将相机运动诱导的光流简化为 6 个自由度的线性组合。</li> </ul> <h3 id="2-详细架构组成">2. 详细架构组成</h3> <p>FlowSeek 在 SEA-RAFT 的基础上增加了以下关键模块：</p> <ul> <li><strong>特征增强：</strong> 利用深度基础模型的解码器特征 \(\Phi\) 增强原始的卷积特征 \(F\)，从而构建更具几何感知力的 4D 相关体。</li> <li><strong>BasesNet（基底网络）：</strong> 核心创新模块。它根据预测的逆深度图 \(D_0\) 计算出 8 个运动基底矢量（包括平移和旋转分量），并提取运动特征 \(H_B\)。</li> <li><strong>上下文增强：</strong> 将预测的深度图与原始图像一同输入 ContextNet，以提取更强的上下文特征 \(C\) 和初始隐藏状态 \(H^0\) 。</li> <li><strong>迭代更新：</strong> 改进后的 UpdNet 在迭代过程中同时参考相关性查找结果、隐藏状态以及来自运动基底的特征。</li> </ul> <h3 id="3-主要优势与改进点">3. 主要优势与改进点</h3> <ul> <li><strong>极低的硬件成本：</strong> FlowSeek 仅需 <strong>单张消费级 GPU（如 RTX 3090）</strong> 即可完成训练，相比于 SEA-RAFT 等方法所需的 8 张 GPU，硬件预算降低了约 8 倍。</li> <li><strong>卓越的泛化能力：</strong> 通过利用深度先验，FlowSeek 在未见过的场景（如 Sintel 和 KITTI）中表现出极强的 Zero-shot 泛化性能，比 SEA-RAFT 提升了 10% 到 15%。</li> <li><strong>更细腻的细节表现：</strong> 深度基础模型的引入使得光流预测在物体边缘和精细结构上更加准确，解决了传统模型在跨域测试时细节缺失的问题。</li> </ul> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>SEA-RAFT</strong></th> <th><strong>FlowSeek</strong></th> </tr> </thead> <tbody> <tr> <td><strong>先验知识</strong></td> <td>仅依赖图像特征</td> <td>图像特征 + 深度图 + 运动基底</td> </tr> <tr> <td><strong>硬件需求</strong></td> <td>通常需要 8x 3090 GPU</td> <td><strong>1x 3090 GPU</strong></td> </tr> <tr> <td><strong>性能 (Spring EPE)</strong></td> <td>~4.79 (S版)</td> <td><strong>~3.31 (L版)</strong></td> </tr> <tr> <td><strong>关键改进</strong></td> <td>混合损失、直接回归初始流</td> <td><strong>基础模型特征融合、BasesNet</strong></td> </tr> <tr> <td><strong>复杂度</strong></td> <td>较低</td> <td>较高（需运行额外的深度模型）</td> </tr> </tbody> </table> <h2 id="neuflow-v2">NeuFlow-v2</h2> <p>Z. Zhang, A. Gupta, H. Jiang, and H. Singh, “NeuFlow v2: Push High-Efficiency Optical Flow To the Limit,” in 2024 IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), 2024, doi: 10.1109/IROS58592.2024.10802353.</p> <p><img src="/images/2026-01-09-optical_flow/image-20260109130930368.png" alt="image-20260109130930368" class="img-fluid" data-zoomable=""/></p> <p><strong>NeuFlow-V2</strong>（全称：NeuFlow v2: Push High-Efficiency Optical Flow To the Limit）是 2024 年提出的一款专为<strong>边缘设备和实时机器人应用</strong>设计的高效光流估计算法。</p> <p>它在 NeuFlow-V1 的基础上进行了深度优化，核心目标是：在保持与 SOTA（当前最佳）方法相当精度的同时，极大地降低计算开销。</p> <h3 id="1-核心技术创新点">1. 核心技术创新点</h3> <ul> <li><strong>极简骨干网络 (Simple Backbone)：</strong> <ul> <li>与传统方法（如 RAFT 使用较重的卷积网络）不同，NeuFlow-V2 认为光流任务更依赖<strong>低级特征</strong>而非高级语义。</li> <li>它直接从不同尺度的图像金字塔（1/2, 1/4, 1/8）中提取特征，每个卷积块仅包含两层卷积。这种设计大幅减少了计算量，同时保留了更多的纹理细节。</li> </ul> </li> <li><strong>全局匹配与交叉注意力 (Cross-Attention &amp; Global Matching)：</strong> <ul> <li>为了处理大位移运动，它在 1/16 低分辨率尺度上引入了 <strong>Cross-Attention</strong>。</li> <li>这增强了图像间特征的判别性，通过全局匹配计算出一个初始光流，为后续迭代提供一个良好的起点。</li> </ul> </li> <li><strong>轻量级 RNN 迭代模块：</strong> <ul> <li><strong>抛弃 GRU/LSTM：</strong> 原始 RAFT 使用的 GRU 单元计算相对复杂。NeuFlow-V2 使用基于简单卷积层的 RNN 结构来整合隐藏状态、当前光流和 Warp 后的相关性特征。</li> <li><strong>数值稳定性：</strong> 引入了 <code class="language-plaintext highlighter-rouge">HardTanh</code> 函数来约束隐藏状态的范围，解决了简单 RNN 容易出现的梯度消失或爆炸问题。</li> </ul> </li> <li><strong>多尺度特征融合：</strong> <ul> <li>通过将 1/16 尺度的全局上下文信息与 1/8 尺度的局部细节特征进行融合，弥补了轻量级骨干网络感受野较小的缺点。</li> </ul> </li> </ul> <h3 id="2-性能表现-效率王者">2. 性能表现 (效率王者)</h3> <p>NeuFlow-V2 的最大亮点在于其极致的运行效率，这使它成为目前在嵌入式硬件上表现最好的模型之一：</p> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>表现说明</strong></th> </tr> </thead> <tbody> <tr> <td><strong>推理速度</strong></td> <td>在 <strong>NVIDIA Jetson Orin Nano</strong> 上可达到 <strong>20+ FPS</strong> (512x384 分辨率)。</td> </tr> <tr> <td><strong>加速比</strong></td> <td>相比于 RAFT 或 GMFlow 等主流方法，实现了 <strong>10x - 70x</strong> 的加速。</td> </tr> <tr> <td><strong>精度水平</strong></td> <td>在 Sintel 和 KITTI 基准测试中，精度接近 SEA-RAFT 等 SOTA 模型，但计算成本极低。</td> </tr> <tr> <td><strong>跨域泛化</strong></td> <td>尽管模型轻量，但其设计确保了在真实世界场景（非训练集）中依然保持稳健的预测效果。</td> </tr> </tbody> </table> <h3 id="3-与-raft--sea-raft-的区别">3. 与 RAFT / SEA-RAFT 的区别</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>RAFT / SEA-RAFT</strong></th> <th><strong>NeuFlow-V2</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要目标</strong></td> <td>追求最高精度（科研/离线处理）</td> <td><strong>追求极致实时性（边缘端/机器人）</strong></td> </tr> <tr> <td><strong>迭代核心</strong></td> <td>复杂的 ConvGRU 单元</td> <td>简单的 CNN-based RNN</td> </tr> <tr> <td><strong>匹配方式</strong></td> <td>4D 相关体 + 局部查找</td> <td>全局匹配 (1/16) + 局部细化</td> </tr> <tr> <td><strong>应用场景</strong></td> <td>服务器端视频处理</td> <td><strong>无人机导航、移动机器人 SLAM</strong></td> </tr> </tbody> </table> <h2 id="对比">对比</h2> <p>FlowSeek 和 NeuFlow-V2 代表了光流估计领域的两个极端：<strong>FlowSeek 追求极致的泛化精度与细节还原</strong>（利用基础模型先验），而 <strong>NeuFlow-V2 追求极致的计算效率与嵌入式部署性能</strong>。</p> <p>以下是两者的详细对比分析：</p> <hr/> <h3 id="1-epe-误差精度与泛化能力">1. EPE 误差（精度与泛化能力）</h3> <p><strong>结论：FlowSeek 在绝对精度和零样本（Zero-shot）泛化能力上显著领先。</strong></p> <ul> <li><strong>FlowSeek (高精度标杆)：</strong> <ul> <li><strong>核心优势：</strong> 它引入了“深度基础模型”（如 Depth Anything v2），使模型具备了强大的空间几何感知能力。</li> <li><strong>性能表现：</strong> 在 Sintel Final 和 KITTI 数据集上，其精度比之前的 SOTA 模型 SEA-RAFT 提升了约 <strong>10%-15%</strong>。在 Spring 这种复杂数据集上，其 EPE 表现极其出色（L 版本可达到约 <strong>3.31</strong>）。</li> <li><strong>细节还原：</strong> 能够精确捕捉物体边缘，解决了很多模型在处理细小物体或快速运动时的模糊问题。</li> </ul> </li> <li><strong>NeuFlow-V2 (高效率平衡)：</strong> <ul> <li><strong>核心优势：</strong> 在极低功耗下维持了“可接受”的高精度。</li> <li><strong>性能表现：</strong> 其精度接近但略逊于 SEA-RAFT 和 RAFT。在 Sintel 数据集上的 EPE 通常在 <strong>2.0-3.0</strong> 左右（取决于具体测试子集和训练阶段）。</li> <li><strong>局限性：</strong> 为了换取速度，它使用了极简的骨干网络，在处理极复杂场景（如重度遮挡或光照剧烈变化）时，精度上限不如 FlowSeek。</li> </ul> </li> </ul> <hr/> <h3 id="2-计算量与效率计算开销">2. 计算量与效率（计算开销）</h3> <p><strong>结论：NeuFlow-V2 在推理速度和轻量化方面具有压倒性优势；FlowSeek 在训练成本上表现优异。</strong></p> <ul> <li><strong>NeuFlow-V2 (端侧王者)：</strong> <ul> <li><strong>推理速度：</strong> 专为边缘计算设计。在 <strong>Jetson Orin Nano</strong> 上能跑到 <strong>20+ FPS</strong>，而同类高精度模型通常只有 1-2 FPS 甚至更低。</li> <li><strong>架构极简：</strong> 使用简单的 CNN-RNN 替代了复杂的 GRU，去除了冗余的特征提取层，计算量（FLOPs）远低于其他模型。</li> <li><strong>部署友好：</strong> 它是目前机器人和无人机领域实时光流估计的首选。</li> </ul> </li> <li><strong>FlowSeek (训练高效，推理较重)：</strong> <ul> <li><strong>训练成本：</strong> 最大的卖点之一是<strong>单卡可训</strong>。传统 SOTA 模型往往需要 8 张 A100/H100，而 FlowSeek 仅需一张 RTX 3090 即可完成训练。</li> <li><strong>推理开销：</strong> 尽管在光流分支上进行了优化，但由于其依赖于外部的“深度基础模型”提供先验特征，推理时需要同时运行深度模型和光流模型，总体的计算延迟明显高于 NeuFlow-V2。</li> </ul> </li> </ul> <hr/> <h3 id="3-综合对比总结">3. 综合对比总结</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>FlowSeek</strong></th> <th><strong>NeuFlow-V2</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要定位</strong></td> <td>追求最高泛化精度（科研、离线分析）</td> <td>追求极致实时性（机器人、端侧部署）</td> </tr> <tr> <td><strong>EPE 表现</strong></td> <td><strong>极佳 (SOTA 级别)</strong></td> <td>优秀 (接近 SOTA，但略有妥协)</td> </tr> <tr> <td><strong>硬件需求</strong></td> <td>单张 3090 可训，推理需较强 GPU</td> <td><strong>Jetson 等嵌入式设备即可流畅运行</strong></td> </tr> <tr> <td><strong>核心创新点</strong></td> <td>深度基础模型 + 运动基底 (Motion Bases)</td> <td>极简骨干网络 + 轻量 RNN + 全局匹配</td> </tr> <tr> <td><strong>适用场景</strong></td> <td>高精度视频合成、运动捕捉、电影特效</td> <td><strong>无人机避障、SLAM、增强现实 (AR)</strong></td> </tr> </tbody> </table>]]></content><author><name></name></author><summary type="html"><![CDATA[RAFT, SEA-RAFT, Neuflow]]></summary></entry><entry><title type="html">Teaching Tailored To Talent</title><link href="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/" rel="alternate" type="text/html" title="Teaching Tailored To Talent"/><published>2026-01-01T01:20:30+00:00</published><updated>2026-01-01T01:20:30+00:00</updated><id>https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/"><![CDATA[<p><img src="/images/2026-01-01-Teaching-Tailored-to-Talent/image-20260101091803120.png" alt="image-20260101091803120" class="img-fluid" data-zoomable=""/></p> <h2 id="介绍">介绍</h2> <p>这篇论文介绍了一种名为 <strong>\(T^3\)-DiffWeather</strong> 的新型图像恢复框架，专门用于解决复杂和多变的恶劣天气条件（如雨、雪、雾等）下的图像修复问题。</p> <p>其核心理念是<strong>“因材施教”（Teaching Tailored to Talent）</strong>，通过结合 <strong>Prompt Learning（提示学习）</strong> 和 <strong>Diffusion Model（扩散模型）</strong>，使网络能够根据不同的天气退化情况动态地调整修复策略。</p> <p>以下是该方法的主要组成部分：</p> <h3 id="1-核心管线预测退化残差-degradation-residual">1. 核心管线：预测退化残差 (Degradation Residual)</h3> <p>不同于传统的扩散模型直接从噪声中恢复清晰图像，该论文将扩散模型的目标转向<strong>预测退化残差 \(r_d\)</strong>（即退化图像与清晰图像之间的差值） 3333。这种设计能够更清晰地表示退化特征，从而引导扩散过程更精准地重建背景。</p> <h3 id="2-提示词池-prompt-pool--针对天气退化">2. 提示词池 (Prompt Pool) —— 针对天气退化</h3> <p>为了应对现实世界中不可预测的天气组合，作者设计了一个<strong>提示词池 (Prompt Pool)</strong> ：</p> <ul> <li><strong>自主构建：</strong> 网络能根据输入图像的退化残差，从提示词池中自动选择并组合最相关的“子提示词”（Sub-prompts），构建出针对特定样本的“天气提示词”（Weather-prompts）。</li> <li><strong>灵活性：</strong> 这种方式通过共享子提示词来捕捉天气的相似性（如雾气和低对比度），同时利用独立的子提示词来区分不同天气的独特性。</li> </ul> <h3 id="3-depth-anything-约束的通用提示词--针对场景建模">3. Depth-Anything 约束的通用提示词 —— 针对场景建模</h3> <p>作者观察到，尽管天气千变万化，但被遮挡的图像背景场景往往具有共同特征。</p> <ul> <li><strong>通用提示词 (General Prompts)：</strong> 专门用于捕捉背景场景的共同属性，为扩散过程提供场景级约束。</li> <li><strong>Depth-Anything 约束：</strong> 论文首次提出利用预训练的 <strong>Depth-Anything</strong> 模型提取的鲁棒特征来引导这些通用提示词 10101010。由于 Depth-Anything 模型在极端天气下仍能保持极高的背景鲁棒性，它能提供准确的场景先验，使修复过程不受天气干扰。</li> </ul> <h3 id="4-对比提示词损失-contrastive-prompt-loss">4. 对比提示词损失 (Contrastive Prompt Loss)</h3> <p>为了确保上述两类提示词（天气提示词和场景通用提示词）能够各司其职，作者引入了<strong>对比提示词损失</strong>：</p> <ul> <li><strong>相互推开：</strong> 将针对天气的提示词和针对场景的提示词视为负样本对，通过对比学习增强各自的表征能力。</li> <li><strong>特征拉近：</strong> 引导提示词与其对应的特征嵌入（如 Depth-Anything 特征）在潜层空间中更接近。</li> </ul> <h3 id="5-高效的推理性能">5. 高效的推理性能</h3> <p>由于采用了精准的提示词条件引导和残差预测策略，\(T^3\)-DiffWeather 的效率极高：</p> <ul> <li><strong>采样步数：</strong> 仅需 <strong>2 步</strong> 采样即可达到优秀性能（相比之下，之前的 Weather Diffusion 模型通常需要更多步骤）。</li> <li><strong>计算开销：</strong> 推理时的计算复杂度仅为目前最先进（SOTA）方法的几十分之一。</li> </ul> <p><strong>总结：</strong> 该论文通过“天气提示词池”和“深度约束的背景提示词”实现了对复杂天气退化的解耦处理，在显著提升修复质量的同时，大幅降低了扩散模型的推理成本。</p> <h2 id="promt-的嵌入">Promt 的嵌入</h2> <p>根据论文中的描述，\(\hat{\mathcal{F}}_e\) 的生成和使用方式如下：</p> <h3 id="1-hatmathcalf_e-的生成过程">1. \(\hat{\mathcal{F}}_e\) 的生成过程</h3> <p>\(\hat{\mathcal{F}}_e\) 是通过<strong>交叉注意力（Cross-Attention）</strong>机制将两种不同类型的 Prompt 逐步嵌入到扩散网络（Diffusion Network）的潜层（Latent Layer）中得到的 111111：</p> <ul> <li><strong>第一步：</strong> 将潜层的特征嵌入 \(\mathcal{F}_e\) 作为 Query，与由<strong>天气提示词（Weather-Prompts）</strong> \(\mathcal{P}_w\) 生成的 Key 和 Value 进行注意力计算，得到中间特征 \(\mathcal{F}_e^{\prime}\) 。</li> <li><strong>第二步：</strong> 将 \(\mathcal{F}_e^{\prime}\) 作为 Query，与由<strong>通用提示词（General Prompts）</strong> \(\mathcal{P}_{gd}\) 生成的 Key 和 Value 再次进行注意力计算，最终输出 \(\hat{\mathcal{F}}_e\) 。</li> </ul> <h3 id="2-hatmathcalf_e-的最终用途">2. \(\hat{\mathcal{F}}_e\) 的最终用途</h3> <p>\(\hat{\mathcal{F}}_e\) 最终被用作<strong>扩散模型去噪过程中的核心条件（Condition）</strong>，具体体现在以下几个方面：</p> <ul> <li><strong>作为信息丰富的引导条件：</strong> 论文指出，\(\hat{\mathcal{F}}_e\) 承载了来自提示词池（Prompt Pool）的退化特征信息以及来自 Depth-Anything 约束的场景背景信息。这些信息共同构成了公式（1）中的条件 \(c\)，用于引导扩散模型从噪声中准确恢复出图像。</li> <li><strong>指导退化残差（Degradation Residual）的重建：</strong> 该论文将扩散模型的目标从直接生成清晰图像转变为生成“退化残差” \(r_d\)（即退化图像与清晰图像之差）。\(\hat{\mathcal{F}}_e\) 作为潜层特征，直接参与到这个残差的去噪重建过程中。</li> <li><strong>实现“因材施教”的恢复：</strong> 由于 \(\hat{\mathcal{F}}_e\) 包含了针对具体样本自适应选择的天气属性和稳健的场景先验，它使得扩散模型能够根据输入图像的具体退化类型（如雨、雪、雾的组合）进行针对性的修复。</li> </ul> <p>在公式（2）中，<strong>\(F_e\) 的原始信息确实通过残差连接（或类似的加和操作）被保留了下来</strong>，\(\hat{\mathcal{F}}_e\) 并不是简单地“扔掉”了 \(F_e\)，而是在 \(F_e\) 的基础上进行了<strong>信息增强</strong> 。</p> <p>以下是基于论文内容及引用 [67]（Stable Diffusion / LDM 架构）的详细确认：</p> <h3 id="f_e-的信息依然存在">\(F_e\) 的信息依然存在</h3> <p>在该论文中，作者明确指出其交叉注意力机制<strong>类似于 Stable Diffusion [67] 中的文本嵌入方式</strong> 22。在 SD 的标准实现中，潜层特征 \(F_e\) 的使用遵循以下逻辑：</p> <ul> <li><strong>计算逻辑：</strong> \(F_e\) 作为 <strong>Query (Q)</strong> 输入到 Attention 模块中。</li> <li><strong>残差更新：</strong> 最终传给下一层的特征通常遵循公式：\(\text{Output} = F_e + \text{Attention}(F_e, \text{Prompt})\)。</li> <li><strong>物理意义：</strong> 这意味着 \(\hat{\mathcal{F}}_e\) 实际上是“<strong>原始图像特征 \(F_e\) + 提示词引导的修正/补充信息</strong>”。\(F_e\) 提供了场景的基础结构和纹理，而 Prompt 信息负责告诉网络哪些部分需要针对天气退化进行调整。</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Prompt, Depth-Anything, Diffusion, Image Restoration]]></summary></entry><entry><title type="html">Vins And Esvo2</title><link href="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/" rel="alternate" type="text/html" title="Vins And Esvo2"/><published>2025-12-31T00:19:46+00:00</published><updated>2025-12-31T00:19:46+00:00</updated><id>https://cekxm.github.io/blog/2025/VINS-and-ESVO2</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/"><![CDATA[<h2 id="vins">VINS</h2> <h3 id="1-系统架构通用因子框架">1. 系统架构：通用因子框架</h3> <p>该方法的核心思想是将每种传感器都视为框架中的一个<strong>通用因子（Factor）</strong> 。</p> <ul> <li><strong>多传感器支持</strong>：框架支持多种传感器组合，如单目相机+IMU、双目相机、以及双目相机+IMU。</li> <li><strong>因子图构建</strong>：共享公共状态变量的因子会被累加在一起，构建一个位姿图（Pose Graph）优化问题 。</li> <li><strong>鲁棒性</strong>：由于不依赖特定传感器，系统可以轻松处理传感器失效的情况，通过移除失效因子的方式快速切换传感器组合。</li> </ul> <p>该方法不是学习类的方法。</p> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251226153814897.png" alt="image-20251226153814897" class="img-fluid"/></p> <p>系统通过一个滑动窗口（Sliding Window）来维护待优化的状态变量。以下是该方法中<strong>状态向量（State Vector）</strong>的详细构成：</p> <h4 id="完整的状态向量">完整的状态向量</h4> <p>在滑动窗口优化中，总的状态向量 \(\mathcal{X}\) 被定义为窗口内所有帧的状态、外参以及特征点深度的集合：</p> \[\mathcal{X} = [\mathbf{x}_m, \mathbf{x}_{m+1}, \dots, \mathbf{x}_n, \mathbf{x}_c^b, \lambda_1, \lambda_2, \dots, \lambda_l]\] <p>其中：</p> <ul> <li>\(m, n\) 是滑动窗口的起始帧和结束帧。</li> <li>\(\mathbf{x}_i\) 是第 \(i\) 帧对应的 <strong>IMU 核心状态</strong>。</li> <li>\(\mathbf{x}_c^b\) 是相机与IMU之间的 <strong>外参（Extrinsic Parameters）</strong>。</li> <li>\(\lambda_l\) 是第 \(l\) 个视觉特征点的 <strong>逆深度（Inverse Depth）</strong>。</li> </ul> <p><strong>和论文表示不太一样，但实际是一样的。这边body frame 的位姿状态放在IMU里面了。</strong></p> <hr/> <h5 id="具体的-imu-状态-mathbfx_i">具体的 IMU 状态 (\(\mathbf{x}_i\))</h5> <p>对于窗口中的每一帧 \(i\)，其核心状态变量包含 15 个维度：</p> \[\mathbf{x}_i = [\mathbf{p}_{b_i}^w, \mathbf{v}_{b_i}^w, \mathbf{q}_{b_i}^w, \mathbf{b}_a, \mathbf{b}_g]\] <ul> <li><strong>\(\mathbf{p}_{b_i}^w\) (3D Position)</strong>：IMU 坐标系在世界坐标系下的位置。</li> <li><strong>\(\mathbf{v}_{b_i}^w\) (3D Velocity)</strong>：IMU 在世界坐标系下的速度。</li> <li><strong>\(\mathbf{q}_{b_i}^w\) (Quaternion/Rotation)</strong>：从 IMU 坐标系到世界坐标系的旋转（通常用四元数表示 Hamilton 形式，或旋转矩阵）。</li> <li><strong>\(\mathbf{b}_a\) (Accelerometer Bias)</strong>：加速度计的零偏。</li> <li><strong>\(\mathbf{b}_g\) (Gyroscope Bias)</strong>：陀螺仪的零偏。</li> </ul> <hr/> <h5 id="辅助状态变量">辅助状态变量</h5> <p>为了保证系统的通用性和精度，论文还包括了以下状态：</p> <ul> <li> <p>传感器外参 (\(\mathbf{x}_c^b\))：</p> <p>包含从相机到 IMU 坐标系的变换 \([\mathbf{p}_c^b, \mathbf{q}_c^b]\)。在系统在线运行时，这些外参也可以被放入优化器进行实时修正。</p> </li> <li> <p>视觉特征点状态 (\(\lambda_l\))：</p> <p>VINS 采用<strong>逆深度（Inverse Depth）</strong>作为特征点的参数化方式。相比于直接使用 3D 坐标 \((x, y, z)\)，逆深度可以更好地处理距离相机非常远的特征点，且其分布更接近高斯分布，有助于数值优化的收敛。</p> </li> </ul> <h4 id="因子">因子</h4> <p>每一个方框是一个 factor，也就是一次测量。更细致的话</p> <ul> <li>相机：一个测量对应一个特征点在该时刻的位置。特征点使用KLT进行跟踪，从之前 \(l\) 时刻到当前 \(t\) 时刻有对应关系，这两个时刻在图像上的位置有约束。</li> <li>如果双目相机，则特征点在左边和右边的位置上，也有约束。</li> <li>IMU: 基于预积分结果的相邻关键帧间的运动约束残差。</li> </ul> <h3 id="2-代价函数-cost-function">2. 代价函数 (Cost Function)</h3> <p>该系统将状态估计建模为一个<strong>最大似然估计（MLE）</strong>问题。在假设测量噪声符合高斯分布的条件下，MLE被转化为一个非线性最小二乘问题，即最小化所有传感器测量残差的加权平方和。</p> <p>在滑动窗口优化中，总代价函数（MAP 形式）定义如下：</p> \[\mathcal{X}_{m:n}^* = \arg \min_{\mathcal{X}_{m:n}} \left\{ \sum_{t=m}^{n} \sum_{k \in S} \\mid z_t^k - h_t^k(\mathcal{X}_{m:n}) \\mid _{\Omega_t^k}^2 + \\mid H_p \delta \mathcal{X}_{m:n} - b_p \\mid ^2 \right\}\] <p>该代价函数主要由以下三部分组成：</p> <ul> <li><strong>视觉因子残差（Camera Factor）</strong>：特征点在不同帧之间的重投影误差 。</li> <li><strong>IMU 因子残差（IMU Factor）</strong>：基于预积分结果的相邻关键帧间的运动约束残差。</li> <li><strong>边缘化先验项（Prior Term）</strong>：由滑动窗口中被移除（边缘化）的旧状态所转换而来的约束信息，用于保留历史观测对当前状态的影响。</li> </ul> <h3 id="3-光束法平差-bundle-adjustment-ba">3. 光束法平差 (Bundle Adjustment, BA)</h3> <p>论文将状态估计过程称为<strong>基于滑动窗口的光束法平差</strong>。</p> <ul> <li><strong>基本原理</strong>：BA通过同时优化相机位姿、IMU状态（速度、偏置）以及特征点的逆深度，使得所有观测残差最小化。</li> <li><strong>求解方式</strong>： <ul> <li><strong>线性化</strong>：在当前估计值附近对代价函数进行一阶泰勒展开，将其转化为线性最小二乘问题。</li> <li><strong>迭代优化</strong>：采用 <strong>Gauss-Newton</strong> 或 <strong>Levenberg-Marquardt</strong> 方法进行多次迭代直至收敛。系统具体使用了 <strong>Ceres Solver</strong> 进行高效求解。</li> </ul> </li> <li><strong>计算效率控制</strong>： <ul> <li><strong>滑动窗口（Sliding Window）</strong>：为了维持实时性，优化只在固定大小的窗口（通常是10帧左右）内进行，而不是处理整个轨迹。</li> <li><strong>边缘化（Marginalization）</strong>：当新帧加入导致窗口满时，利用<strong>舒尔补（Schur Complement）</strong>将最老的帧移除，并将其携带的信息转化为当前窗口内状态的先验分布。</li> </ul> </li> </ul> <p>这种基于优化的方法相比于传统的滤波器（如EKF）具有更高的精度，因为它可以在非线性空间内进行多次迭代更新，并能更好地处理传感器之间的时钟同步偏差。</p> <h2 id="esvo2">ESVO2</h2> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251227073033444.png" alt="image-20251227073033444" class="img-fluid"/></p> <p><strong>ESVO2</strong> 是由湖南大学（Yi Zhou 教授团队）与香港科技大学（Shaojie Shen 教授团队）等合作开发的一个<strong>实时、紧耦合的双目事件相机视觉惯性里程计系统</strong> 。</p> <h3 id="概述">概述</h3> <p>相比前代系统，ESVO2 引入了以下四大创新 ：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：提出了一种新型的类图像事件表示方法，能够根据事件的局部动态自动调整累积时间。这使得系统能高效提取边缘轮廓点，而不受运动速度变化的影响。</li> <li><strong>引入 IMU 预积分</strong>：通过将 IMU 测量值作为运动先验，解决了纯视觉追踪在特定旋转维度（如 Pitch 和 Yaw）上的简并（Degeneracy）问题 。</li> <li><strong>紧耦合后端优化</strong>：设计了一个精简的后端，专门优化线性速度和 IMU 偏置（Bias），从而抑制轨迹漂移，确保全局一致性。</li> <li><strong>增强的建图模块</strong>：结合了“时间同步双目（Temporal Stereo）”和“静态双目（Static Stereo）”配置，并引入快速块匹配方案，显著提升了深度图的完整性和局部平滑度。</li> </ul> <h4 id="系统架构">系统架构</h4> <p>ESVO2 采用并行设计，由四个独立线程组成：</p> <ol> <li><strong>预处理线程</strong>：负责事件的自适应累积及时间表面（Time Surface）的更新。</li> <li><strong>追踪线程（Localization）</strong>：执行 3D-2D 时空配准，利用无偏移平滑时间表面（OS-TS）进行位姿估计。</li> <li><strong>建图线程（Mapping）</strong>：实时恢复半稠密深度图并维护局部 3D 地图。</li> <li><strong>后端线程（Back-end）</strong>：进行滑动窗口优化，不断更新速度和 IMU 参数。</li> </ol> <h3 id="mapping">Mapping</h3> <h4 id="temporal-stereo时间双目"><strong>Temporal Stereo（时间双目）</strong></h4> <p>是一种核心的深度估计技术。它通过结合<strong>时间维度</strong>（单相机的运动）和<strong>空间维度</strong>（双相机的基线）来提高建图的鲁棒性。</p> <h5 id="1-定义与核心思想">1. 定义与核心思想</h5> <p><strong>Temporal Stereo</strong> 指的是利用<strong>单只相机在不同时刻（即随时间位移）</strong>观测到的视觉信息来估计物体深度的技术 。</p> <ul> <li> <p><strong>对比 Static Stereo（静态双目）</strong>：静态双目是利用左右两个相机在<strong>同一时刻</strong>的视图差异（视差）来计算深度。</p> </li> <li> <p><strong>结合的意义</strong>：在 ESVO2 中，系统并不只依赖左右相机的瞬时匹配，而是将相机的<strong>运动位移</strong>也作为一种“虚拟基线”。</p> </li> </ul> <h5 id="2-工作原理">2. 工作原理</h5> <p>在 ESVO2 的建图模块中，Temporal Stereo 的具体运作方式如下：</p> <ul> <li> <p><strong>时空观测一致性</strong>：当机器人移动时，同一个空间点会在不同时间点被相机捕获。系统会寻找当前事件流与该相机在不久前的历史观测之间的相关性 。</p> </li> <li> <p><strong>代价合并（Cost Fusion）</strong>：ESVO2 将 Temporal Stereo 和 Static Stereo 的匹配代价（Matching Cost）进行融合 。</p> </li> <li>如果运动方向有利于时间双目（例如向前运动，提供了纵向观测变化），Temporal Stereo 能提供很好的约束。</li> <li>如果左右相机之间的视差更明显，Static Stereo 则占据主导。</li> </ul> <h5 id="3-在-esvo2-中的作用">3. 在 ESVO2 中的作用</h5> <p>引入 Temporal Stereo 对事件相机 SLAM 具有至关重要的作用：</p> <ul> <li> <p><strong>解决观测退化</strong>：事件相机对垂直于其边缘运动的方向敏感。如果机器人仅进行某种特定方向的运动，静态双目可能会失效。Temporal Stereo 增加了额外的观测维度，保证了在 6-DoF（六自由度）复杂运动下的建图完整性 。</p> </li> <li> <p><strong>提高深度图质量</strong>：通过在时间轴上累积观测，系统能够过滤掉瞬时的噪声事件，生成的深度图更加平滑且空洞更少 。</p> </li> <li> <p><strong>快速块匹配</strong>：论文提到在执行 Temporal Stereo 匹配时采用了快速块匹配方案，这使得系统能够在普通 CPU 上实时处理 VGA 分辨率的高频数据 。</p> </li> </ul> <p>在 ESVO2 中，<strong>Temporal Stereo 实际上是将 VIO 系统变成了一个“多视图立体视觉（MVS）”系统</strong>。它不再局限于双目相机那段固定的物理基线，而是将相机的整个运动轨迹都变成了获取几何信息的源泉，从而在各种运动条件下都能输出高精度的半稠密深度图 。</p> <h4 id="fast-static-stereo快速静态双目"><strong>Fast Static Stereo（快速静态双目）</strong></h4> <p><strong>Fast Static Stereo（快速静态双目）</strong> 的目的确实是为了获得三维点的坐标，但它的实现方式和传统的“特征点提取 + 三角化”有所不同。</p> <p>以下是针对您问题的详细解答：</p> <h5 id="1-是为了获得稀疏三维点吗">1. 是为了获得稀疏三维点吗？</h5> <p><strong>是的，但更准确地说是“半稠密（Semi-dense）”的三维点。</strong></p> <ul> <li><strong>非稀疏</strong>：传统的 VO（如 ORB-SLAM）只提取少量的特征点（如角点），产生的点云非常稀疏。</li> <li><strong>非全稠密</strong>：由于事件相机只在亮度变化的地方产生数据（通常是物体的边缘），所以它无法像结构光或 LiDAR 那样获得物体的完整表面。</li> <li><strong>半稠密</strong>：ESVO2 的 Fast Static Stereo 针对事件累积产生的<strong>边缘轮廓</strong>进行匹配，因此得到的是物体轮廓处的 3D 点云，这比传统的稀疏点云要密得多，足以辅助避障和局部建图。</li> </ul> <h5 id="2-只需要做三角化吗">2. 只需要做三角化吗？</h5> <p><strong>不完全是。</strong> 虽然双目外参（变换矩阵）已经离线标定好了，但“三角化”只是最后一步。最难、最核心的步骤是<strong>数据关联（Data Association / Correspondence）</strong>，即：<strong>左目里的这个事件点，对应右目里的哪个像素？</strong></p> <p>在 ESVO2 的 Fast Static Stereo 中，这个过程包含以下关键步骤：</p> <ul> <li><strong>极线搜索 (Epipolar Search)</strong>：利用已标定的外参，在右目的极线上寻找匹配点。</li> <li><strong>块匹配 (Block Matching)</strong>：由于事件数据不是灰度图，ESVO2 使用了 <strong>Time Surface（时间表面）</strong> 或像素块来进行相似度比较。</li> <li><strong>时空一致性检查</strong>：系统会检查左右目事件在时间上的同步性（是否在极短的时间差内发生）以及空间上的几何一致性。</li> <li><strong>三角化 (Triangulation)</strong>：只有当匹配的代价（Cost）足够低且置信度高时，系统才会根据已知的双目基线进行几何三角化，计算出深度 \(Z\)。</li> </ul> <h5 id="3-esvo2-fast-的体现">3. ESVO2 “Fast” 的体现</h5> <p>之所以称为 <strong>Fast</strong>，是因为它做了一些专门针对 CPU 实时性的优化：</p> <ol> <li><strong>查找表 (Look-up Table)</strong>：预先计算极线方向，减少运行时的几何计算。</li> <li><strong>跳过非边缘区域</strong>：只对有事件发生的像素点进行匹配，避开大量空白区域。</li> <li><strong>并行化</strong>：利用多线程并行处理左右目的事件流块。</li> </ol> <p>你理解的“通过三角化获得坐标”是最终目的，但 <strong>Fast Static Stereo 的核心在于如何利用标定好的几何关系，在极高频率、高分辨率的事件流中快速找到左右目的匹配对。</strong></p> <p>值得注意的是，ESVO2 还会将 <strong>Static Stereo</strong>（左右目匹配）的结果与 <strong>Temporal Stereo</strong>（单目随时间运动产生的匹配）进行融合，这样即使在双目基线方向运动导致视差较小时，依然能获得准确的深度。</p> <h4 id="轮廓">轮廓</h4> <p>在 ESVO2 论文中，<strong>Fast Static Stereo（快速静态双目）不使用传统的稀疏特征点（如角点），而是直接使用物体的轮廓（Contour Points）进行匹配</strong> 。</p> <p>这种设计是由事件相机的物理特性决定的。以下是具体的实现逻辑：</p> <h5 id="1-为什么选择轮廓而不是特征点">1. 为什么选择轮廓而不是特征点？</h5> <ul> <li><strong>物理机制</strong>：事件相机只在亮度变化时触发信号，因此其天然的输出就是物体的<strong>边缘和轮廓</strong>。</li> <li><strong>鲁棒性</strong>：在高速运动中，传统的特征点（如 Harris 角点）可能因为观察不全而难以追踪 3。直接使用轮廓点能保留更完整的环境几何结构。</li> </ul> <h5 id="2-esvo2-的具体处理方式">2. ESVO2 的具体处理方式</h5> <p>ESVO2 并没有处理所有的事件点，而是通过以下步骤提取<strong>精准的轮廓点</strong>进行匹配：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：系统会根据事件的动态变化（速度快慢），自动决定累积多长时间的事件来生成一张类似图像的“AA图”。</li> <li><strong>轮廓点采样 (Contour-point Sampling)</strong>：在 AA 图上，系统会采样那些代表瞬时边缘的像素点（即轮廓点） 。</li> <li><strong>排除冗余</strong>：相比于直接使用原始事件流，这种方法筛选掉了大量噪声和冗余点，使得输入点更“精简且准确”。</li> </ul> <h5 id="3-fast-static-stereo-的快体现在哪">3. Fast Static Stereo 的“快”体现在哪？</h5> <p>由于处理的是轮廓点，ESVO2 采用了以下策略实现高效匹配：</p> <ul> <li><strong>块匹配 (Block Matching)</strong>：在左右目相机的 AA 图或时间表面（Time Surface）上，沿着极线对这些<strong>采样后的轮廓像素块</strong>进行相似度搜索。</li> <li><strong>取消非线性精化</strong>：论文提到，由于采样后的轮廓点已经非常精确，系统在 ESVO2 中<strong>取消了耗时的子像素级非线性精化步骤</strong>，从而显著提升了速度（在 VGA 分辨率下达到 20Hz 实时性），且精度几乎没有下降。</li> </ul> <p>在 ESVO2 的 <strong>Fast Static Stereo</strong> 模块中，获得 3D 坐标的过程是一个从“事件像素”到“空间点”的几何推导过程。虽然不使用复杂的特征描述子，但它利用了极其严格的<strong>时空约束</strong>。</p> <p>以下是具体的实现步骤：</p> <h5 id="1-极线约束搜索-epipolar-search">1. 极线约束搜索 (Epipolar Search)</h5> <p>假设左目相机在 \(u_L\) 像素处采样了一个轮廓点，由于双目相机的外参（基线 \(b\)、焦距 \(f\)）已预先标定且图像已做过极线校正（Rectification），那么这个点在右目图像中对应的匹配点一定位于<strong>水平的极线</strong>上。</p> <ul> <li><strong>搜索范围</strong>：系统会在右目的极线上，根据设定的最小和最大深度范围，确定一个搜索区间。</li> </ul> <h5 id="2-基于时间表面的匹配-time-surface-matching">2. 基于“时间表面”的匹配 (Time Surface Matching)</h5> <p>这是最关键的一步。因为事件相机没有灰度值，它使用 <strong>Time Surface (TS)</strong> 或 <strong>AA 图</strong>来计算匹配相似度：</p> <ul> <li><strong>提取 Patch</strong>：在左目 \(u_L\) 周围取一个小的像素块（例如 \(5 \times 5\)）。</li> <li><strong>计算相似度</strong>：在右目极线的搜索区间内滑动，通过 <strong>零均值归一化互相关 (ZNCC)</strong> 或类似的度量函数，寻找与左目 Patch 最相似的区域。</li> <li><strong>原理</strong>：虽然没有颜色，但物体轮廓在左右目形成的时间表面形状是非常相似的。</li> </ul> <h5 id="3-概率深度估计-probabilistic-depth-estimation">3. 概率深度估计 (Probabilistic Depth Estimation)</h5> <p>ESVO2 并不只是简单地计算一个视差，而是为每个点维护一个概率分布：</p> <ul> <li> <p><strong>逆深度建模</strong>：系统将深度表示为逆深度（Inverse Depth），并假设其服从高斯分布。</p> </li> <li> <p>代价聚合：当匹配代价最低的点被找到后，通过视差公式计算初始深度：</p> \[Z = \frac{f \cdot b}{d}\] <p>（其中 \(f\) 是焦距，\(b\) 是基线，\(d\) 是左右目像素的位移差/视差）。</p> </li> </ul> <h5 id="4-空间三角化-triangulation">4. 空间三角化 (Triangulation)</h5> <p>一旦确定了左右目的匹配对 \((u_L, v_L)\) 和 \((u_R, v_R)\)，就可以通过标准的<strong>三角化公式</strong>求出该轮廓点在左目相机坐标系下的 3D 坐标 \((X, Y, Z)\)：</p> <ul> <li> \[Z = \frac{f \cdot b}{u_L - u_R}\] </li> <li> \[X = \frac{(u_L - c_x) \cdot Z}{f}\] </li> <li> \[Y = \frac{(v_L - c_y) \cdot Z}{f}\] </li> </ul> <h5 id="5-esvo2-的特殊优化取消非线性精化">5. ESVO2 的特殊优化：取消非线性精化</h5> <p>在早期的 ESVO 版本中，三角化后通常还需要一个耗时的“非线性最小二乘精化”过程来微调 3D 位置。</p> <p>ESVO2 的改进：由于采用了自适应累积 (AA) 提取的轮廓点非常精准，论文提到他们直接跳过了非线性精化步骤。只要匹配代价足够低，就直接输出三角化的结果。这种“直接法”极大地提升了处理 VGA 分辨率数据的速度。</p> <h5 id="总结">总结</h5> <p>简单来说，获得 3D 坐标的流程是：</p> <p>左目采样轮廓点 \(\rightarrow\) 右目极线搜索匹配块 \(\rightarrow\) 计算视差 \(\rightarrow\) 几何三角化 \(\rightarrow\) 得到 3D 坐标。</p> <p>这一过程生成的点云是<strong>半稠密</strong>的，因为它只存在于有光强变化的边缘位置，但对于机器人感知环境（如避障、路径规划）已经足够丰富。</p> <h3 id="localization">Localization</h3> <h4 id="1-空间上的-3d-2d-配准当前时刻-vs-历史时刻">1. 空间上的 3D-2D 配准（当前时刻 vs 历史时刻）</h4> <p>追踪线程的核心逻辑是：<strong>“用过去建立的地图，来对齐现在的观测。”</strong></p> <ul> <li><strong>3D 信息（来自过去/不同时刻）：</strong> 追踪线程使用的 3D 点云是由“建图线程（Mapping）”提供的。这些 3D 点是根据<strong>之前的一系列时刻</strong>（滑动窗口内的历史帧）计算并累积出来的局部地图。</li> <li><strong>2D 信息（来自当前时刻）：</strong> 追踪线程使用的是<strong>当前最新时刻</strong>产生的事件流（并转化成了 OS-TS 时间表面）。</li> </ul> <p><strong>结论：</strong> 它是将<strong>历史时刻积累的 3D 结构</strong>投影到<strong>当前时刻的 2D 平面</strong>上。如果投影的位置和当前看到的位置重合，就说明位姿估计是准确的。</p> <hr/> <h4 id="2-时间上的时空一致性时空配准">2. 时间上的“时空一致性”（时空配准）</h4> <p>ESVO2 之所以强调“时空（Spatio-temporal）”，是因为它不仅仅看空间位置，还考虑了<strong>事件发生的时间戳</strong>。</p> <ul> <li><strong>Time Surface (TS) 的本质：</strong> 时间表面本身就存储了时间信息（像素值代表该位置最近一次事件发生的时间戳）。</li> <li>配准逻辑： 1. 假设当前时间是 \(t_{now}\)。 <ol> <li>如果 3D 点投影到 \(u\) 位置，而 OS-TS 在 \(u\) 位置记录的事件时间非常接近 \(t_{now}\)，说明这个 3D 点在当前时刻是“活跃”的，配准残差就小。</li> <li>反之，如果该位置没有新事件，或者事件发生的时间很久远，残差就会很大。</li> </ol> </li> </ul> <hr/> <h4 id="3-imu-的作用连接不同时刻的纽带">3. IMU 的作用：连接不同时刻的纽带</h4> <p>IMU 预积分在这里扮演了极其重要的角色：</p> <ul> <li>它利用<strong>两个时刻之间</strong>的高频惯性数据，预测出从上一时刻到当前时刻的相对位姿变化。</li> <li>这个预测值作为初值，告诉追踪线程：“根据 IMU 估计，那些 3D 点现在应该出现在这个位置。” 然后视觉配准再在这个基础上进行微调。</li> </ul> <h3 id="跨时刻的配准">“跨时刻”的配准</h3> <p>“跨时刻”的配准（通常指 SLAM 或视觉里程计中的位姿估计）确实是通过<strong>雅可比矩阵（Jacobian）</strong>来建立误差与位姿增量之间的线性关系，进而通过迭代优化的方式更新位姿的。</p> <p>由于位姿（旋转 + 平移）所在的空间（如 \(SE(3)\)）并不是一个欧几里得空间（你不能简单地把两个旋转矩阵相加），数学上通常会引入<strong>李群（Lie Group）</strong>和<strong>李代数（Lie Algebra）</strong>来处理求导问题。</p> <p>以下是这一过程的数学推导核心步骤：</p> <hr/> <h4 id="1-定义误差函数residual">1. 定义误差函数（Residual）</h4> <p>假设在 \(t\) 时刻，我们观察到一个空间点 \(P\)。在 \(t+1\) 时刻，相机的位姿变为 \(T\)（包含旋转 \(R\) 和平移 \(t\)）。该点在当前相机坐标系下的投影预测值为：</p> \[\hat{z} = h(T, P)\] <p>其中 \(h\) 是投影函数。如果实际观测到的特征点坐标是 \(z\)，那么<strong>误差（残差）</strong>定义为：</p> \[e(T) = z - h(T, P)\] <h4 id="2-引入扰动模型perturbation-model">2. 引入扰动模型（Perturbation Model）</h4> <p>因为直接对旋转矩阵 \(R\) 求导非常复杂且必须保持正交性约束，我们通常给当前的位姿 \(T\) 左乘一个微小的扰动 \(\Delta T\)。</p> <p>在李代数上，这个扰动可以用一个 6 维向量 \(\xi = [\rho, \phi]^T\) 表示（前三维为平移扰动，后三维为旋转扰动）：</p> \[T_{new} = \exp(\xi^{\wedge}) \cdot T_{old}\] <h4 id="3-利用雅可比矩阵进行线性化">3. 利用雅可比矩阵进行线性化</h4> <p>我们要计算的是：当位姿发生微小变化 \(\xi\) 时，误差 \(e\) 发生了多少变化？</p> <p>利用泰勒展开：</p> \[e(T_{new}) = e(\exp(\xi^{\wedge})T_{old}) \approx e(T_{old}) + \frac{\partial e}{\partial \xi} \xi\] <p>这里的 \(J = \frac{\partial e}{\partial \xi}\) 就是你所说的雅可比矩阵。</p> <h4 id="4-雅可比矩阵的具体分解">4. 雅可比矩阵的具体分解</h4> <p>根据链式法则，这个雅可比矩阵通常可以拆解为两部分：</p> \[J = \frac{\partial e}{\partial P'} \cdot \frac{\partial P'}{\partial \xi}\] <ul> <li> <p><strong>第一部分 \(\frac{\partial e}{\partial P'}\)</strong>：像素误差对空间点坐标（在相机坐标系下）的导数。这取决于相机的内参模型（如针孔模型）。</p> </li> <li> <p>第二部分 \(\frac{\partial P'}{\partial \xi}\)：变换后的空间点坐标对位姿扰动的导数。在 \(SE(3)\) 下，对于点 \(P' = [X, Y, Z]^T\)，其推导结果通常是一个 \(3 \times 6\) 的矩阵：</p> \[\frac{\partial P'}{\partial \xi} = \begin{bmatrix} I &amp; -P'^{\wedge} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; Z &amp; -Y \\ 0 &amp; 1 &amp; 0 &amp; -Z &amp; 0 &amp; X \\ 0 &amp; 0 &amp; 1 &amp; Y &amp; -X &amp; 0 \end{bmatrix}\] </li> </ul> <h4 id="5-求解与位姿更新">5. 求解与位姿更新</h4> <p>为了使误差最小化（最小二乘问题），我们构建高斯-牛顿（Gauss-Newton）方程：</p> \[(J^T J) \Delta \xi = -J^T e\] <ul> <li><strong>\(H = J^T J\)</strong>：近似海森矩阵（Hessian）。</li> <li><strong>\(\Delta \xi\)</strong>：计算出的最优位姿增量。</li> </ul> <p>更新步：</p> <p>得到 \(\Delta \xi\) 后，我们将其映射回李群，更新当前的位姿估计：</p> \[T \leftarrow \exp(\Delta \xi^{\wedge}) \cdot T\] <p>通过不断重复上述过程（线性化 -&gt; 求解 -&gt; 更新），位姿会逐渐收敛到能够使跨时刻特征点对齐的最优值。</p> <h3 id="localization-与-mapping并行且循环的工作流">Localization 与 Mapping并行且循环的工作流</h3> <p>在 ESVO2 的追踪线程（Localization）中，<strong>它同时利用了“跨时刻”的 3D 信息和“当前时刻”的 2D 信息。</strong></p> <h4 id="1-追踪线程localization利用过去引导现在">1. 追踪线程（Localization）：利用“过去”引导“现在”</h4> <ul> <li><strong>正确性确认</strong>：是的。追踪线程是第一步。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 2D 信息（OS-TS 时间表面）+ 之前的 Local 3D Map。</li> <li><strong>过程</strong>：它执行 3D-2D 的时空配准。简单说，就是把已经建好的 3D 地图点投影到当前的 2D 画面上，看对不对得上。</li> <li><strong>结果</strong>：计算出当前最准确的 <strong>Camera Pose</strong>（相机位姿）。IMU 在这里提供了一个非常关键的初始预测位姿，使得追踪在剧烈运动时不会丢。</li> </ul> </li> </ul> <h4 id="2-建图线程mapping利用现在更新未来">2. 建图线程（Mapping）：利用“现在”更新“未来”</h4> <ul> <li><strong>正确性确认</strong>：是的。获得当前位姿后，紧接着（或并行）执行 Mapping。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 Camera Pose + 当前的 2D 信息（左右目事件流）。</li> <li><strong>Temporal Stereo 的作用</strong>：正如你所说，Temporal Stereo 需要知道相机的运动轨迹。它利用刚算出来的 <strong>Current Pose</strong> 结合历史位姿，计算出相机在移动过程中产生的“时间视差”。</li> <li><strong>结果</strong>：结合 <strong>Static Stereo</strong>（左右目即时视差）和 <strong>Temporal Stereo</strong>，生成新的 3D 点，并更新 <strong>Local 3D Map</strong>。</li> </ul> </li> </ul> <h4 id="3-为什么这个顺序很重要">3. 为什么这个顺序很重要？</h4> <p>这个循环构成了一个典型的自洽系统：</p> <ol> <li><strong>没有 Pose，无法建图</strong>：尤其是 Temporal Stereo，必须精确知道相机从 A 点挪到了 B 点，才能根据像素的移动反推深度。</li> <li><strong>没有 Map，无法追踪</strong>：追踪线程必须有一个“参照物”（3D Map），才能知道当前看到的 2D 图像代表自己在空间中的什么位置。</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[VINS, ESVO, SFM]]></summary></entry><entry><title type="html">DINO 如何用于密集预测</title><link href="https://cekxm.github.io/blog/2025/dino/" rel="alternate" type="text/html" title="DINO 如何用于密集预测"/><published>2025-12-26T05:54:16+00:00</published><updated>2025-12-26T05:54:16+00:00</updated><id>https://cekxm.github.io/blog/2025/dino</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/dino/"><![CDATA[<h2 id="资料">资料</h2> <ul> <li><a href="https://zhuanlan.zhihu.com/p/1933583851923439816">(3 封私信 / 80 条消息) 万字长文超详解读之DINO全系列—视觉表征对比学习的高峰 - 知乎</a></li> <li><a href="https://zhuanlan.zhihu.com/p/1940400858836742367">(3 封私信 / 80 条消息) 万字长文超详解之DINO-V3（DINO全系列之补充篇） - 知乎</a></li> <li><a href="https://www.youtube.com/watch?v=j2_42Yx_1_w">Inside DINOv2: Architecture Analysis + CIFAR-10 Experiment - YouTube</a></li> <li><a href="https://mashaan14.github.io/YouTube-channel/self_supervised_learning/2025_05_19_SSL">Self-Supervised Learning Review: from SimCLR to DINOv2 | Mashaan blog</a></li> </ul> <blockquote> <p>DINOv3 的发布，标志着计算机视觉进入了类似 NLP 的“GPT-3 时刻”<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p> <p><strong>对于学术界：</strong></p> <ul> <li><strong>Bad News</strong>：传统的“设计一个新 Backbone”、“魔改 Transformer 模块”刷点数的路子越来越窄了。在 7B 模型面前，微小的架构创新几乎没有意义。</li> <li><strong>Good News</strong>：新的研究方向被打开了。 <ul> <li><strong>Post-training</strong>：如何更高效地利用这些冻结特征？</li> <li><strong>多模态对齐</strong>：DINOv3 展示了初步的文本对齐能力，但这方面远未饱和。</li> <li><strong>视频理解</strong>：利用 DINOv3 强大的时序一致性做原生的视频大模型。</li> </ul> </li> </ul> <p><strong>对于工业界/工程师：</strong></p> <ul> <li><strong>这是巨大的利好</strong>。你不再需要收集几十万张标注数据去训练一个分割模型。直接下载 DINOv3 的权重，冻结它，用几百张图训练一个轻量级 Head，你就能得到工业级可用的效果。</li> <li><strong>Deployment</strong>：Meta 提供的蒸馏版小模型（特别是 ViT-Small 和 Base）将是边缘端部署的神器。</li> </ul> <p><strong>DINOv3 并没有让天塌下来，它只是铺平了地基。</strong> 它把提取“好特征”这件最脏最累最费算力的事做完了。现在的我们，可以站在 70 亿参数的肩膀上，去探索视觉智能更上层的逻辑——这何尝不是一种幸运？</p> </blockquote> <h2 id="如何使用-dino">如何使用 DINO</h2> <p>对于 dense prediction，一般要使用多个层的特征，比如 VGGT，以及 dinov3 中的应用部分有提及。</p> <ol> <li><strong>多层特征提取：</strong> 在 VGGT 的实现细节中，为了生成高分辨率的密集输出（如深度图和点图），模型将来自 DINOv2 骨干网络不同阶段的特征提供给 <strong>DPT（密集预测 Transformer）头</strong>进行处理。具体而言，VGGT 会提取 DINOv2（ViT-L/14）中<strong>第 4、11、17 和 23 块（blocks，实际上就是layer）</strong>的令牌（tokens），并将这些中间层的特征输入 DPT 进行上采样。</li> <li><strong>与 DINOv3 结合时的用法：</strong> 在 DINOv3 的后续实验中，研究人员将 VGGT 的图像特征提取器更换为 DINOv3 ViT-L。在这种配置下，他们同样使用了 <strong>4 个中间层特征的拼接（concatenation）</strong> 作为下游模块的输入，而不是仅使用最后一层。实验发现，这种使用多个中间层的方法对 DINOv3 带来了性能提升。</li> </ol> <h2 id="dpt">DPT</h2> <p>R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense prediction,” in <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 12173-12183.</p> <p><img src="/images/2025-12-26-dino/image-20251226103424453.png" alt="image-20251226103424453" class="img-fluid"/></p> <h3 id="encoder">Encoder</h3> <p>DPT共需要4层的特征，如果用ViT/dino，那这四层的尺寸是一样的。</p> <h3 id="reassemble">Reassemble</h3> <p>Read-&gt;Concatenate-&gt;Resample</p> <p>为了更清楚地理解，可以看它在完整流程中的作用：</p> <ol> <li><strong>Read：</strong> 处理 CLS 令牌（这里可能涉及拼接投影等线性运算）。</li> <li><strong>Concatenate：</strong> 将令牌<strong>摆放</strong>回网格位置。</li> <li><strong>Resamples：</strong> 使用 \(1 \times 1\) 和 \(3 \times 3\) 卷积进行通道投影及空间缩放（这里才进行真正的<strong>卷积运算</strong>）。</li> </ol> <p><strong>总结：</strong> Concatenate 阶段就像是<strong>拼图</strong>。Read 操作决定了每一块拼图（token）上的内容，而 Concatenate 只是<strong>按照位置把拼图摆好</strong>。真正的“修图”和“放大”工作是由接下来的 Resamples 卷积层完成的。</p> <h4 id="read">Read</h4> <p>在 DPT（Dense Prediction Transformer）架构中，<strong>Read 操作</strong>是其核心组件“重组操作”（Reassemble）的<strong>第一阶段</strong>。它的主要任务是处理 Vision Transformer (ViT) 输出的特殊令牌，并将令牌序列转换为可以进行空间排列的形式。</p> <p>以下是 Read 操作的详细介绍：</p> <h5 id="1-核心定义与目的">1. 核心定义与目的</h5> <p>在 ViT 中，输出包含 \(N_p\) 个图像块令牌（patch tokens）和 <strong>1 个特殊的“读取令牌”（readout token，通常指 CLS token）</strong>。</p> <ul> <li><strong>输入：</strong> \(N_p + 1\) 个令牌。</li> <li><strong>目的：</strong> 将这 \(N_p + 1\) 个令牌映射回 \(N_p\) 个令牌，以便后续的“拼接操作”（Concatenate）能将它们按照原始图像位置还原成特征图。</li> <li><strong>数学表达式：</strong> \(Read: \mathbb{R}^{(N_p+1) \times D} \to \mathbb{R}^{N_p \times D}\)。</li> </ul> <h5 id="2-三种实现方案">2. 三种实现方案</h5> <p>DPT 论文评估了处理读取令牌（\(t_0\)）与图像块令牌（\(t_1, \dots, t_{N_p}\)）之间关系的三种不同方式：</p> <ul> <li><strong>Readignore（忽略）：</strong> 直接<strong>丢弃读取令牌</strong>，只保留 \(N_p\) 个图像块令牌。这是最简单的方法，即 \(Read_{ignore}(t) = {t_1, \dots, t_{N_p}}\)。</li> <li><strong>Readadd（相加）：</strong> 将读取令牌的信息<strong>加到所有其他令牌上</strong>。即 \(Read_{add}(t) = {t_1 + t_0, \dots, t_{N_p} + t_0}\)。</li> <li><strong>Readproj（投影/默认方案）：</strong> 通过<strong>拼接后投影</strong>的方式融合信息。将读取令牌与每个图像块令牌拼接，然后通过一个线性层（MLP）将维度投影回原始大小 \(D\)。其公式为：\(Read_{proj}(t) = {mlp(cat(t_1, t_0)), \dots, mlp(cat(t_{N_p}, t_0))}\)。</li> </ul> <h5 id="3-性能表现与结论">3. 性能表现与结论</h5> <p>根据来源中的消融实验结果：</p> <ul> <li><strong>Readproj 是默认的最优方案</strong>，在单目深度估计等任务中表现略优于其他方案，因为它能更有效地捕获并分配全局信息。</li> <li>相比之下，<code class="language-plaintext highlighter-rouge">Readadd</code> 的效果甚至差于完全忽略令牌的 <code class="language-plaintext highlighter-rouge">Readignore</code>。</li> </ul> <p><strong>总结：</strong> <strong>Read 操作</strong>就像是一个“令牌筛选与融合器”，它决定了如何将 Transformer 学习到的<strong>全局图像表示（读取令牌）*<em>回馈给各个*</em>局部特征（图像块令牌）</strong>，从而确保在进入后续的卷积解码阶段前，每个像素级别的特征都已融入了全局上下文信息。</p> <h4 id="concatenate">Concatenate</h4> <p>在 DPT（Dense Prediction Transformer）的 <strong>Reassemble</strong> 操作中，<strong>Concatenate（拼接）</strong> 阶段本身<strong>不涉及任何复杂的数学运算或学习参数</strong>，它的本质是一个<strong>形状变换（Reshape/Rearrange）</strong>过程。</p> <h4 id="resample">Resample</h4> <p>这一步才真正涉及空间缩放。</p> <ul> <li><strong>输入：</strong> 空间排列好的特征图，尺寸为 \(\frac{H}{p} \times \frac{W}{p}\)，通道数为 \(D\)。</li> <li><strong>输出：</strong> 缩放后的特征图，尺寸为 \(\frac{H}{s} \times \frac{W}{s}\)，通道数为 \(\hat{D}\)（DPT 默认 \(\hat{D} = 256\)）。</li> </ul> <p>Resample 通过两步卷积运算来完成变换：</p> <ol> <li> <p><strong>通道投影：</strong> 首先使用 <strong>1x1 卷积</strong>。这一步负责将来自不同 Transformer 层的令牌维度（如 ViT-Large 的 1024 维）投影到解码器所需的统一维度 \(\hat{D}\)。</p> </li> <li> <p>空间缩放：</p> <p>随后根据目标缩放比例 \(s\) 与初始图像块大小 \(p\) 的关系，使用</p> <p>3x3 卷积</p> <p>进行调整：</p> <ul> <li><strong>下采样（\(s \ge p\)）：</strong> 使用<strong>步长（strided）为 3x3 的卷积</strong>来降低分辨率。</li> <li><strong>上采样（\(s &lt; p\)）：</strong> 使用<strong>步长为 3x3 的转置卷积（transpose convolution）</strong>来提升分辨率。</li> </ul> </li> </ol> <h3 id="fusion">Fusion</h3> <p>每级上采样2倍。</p> <h2 id="multi-task-image-restoration-guided-by-robust-dino-features">Multi-task image restoration guided by robust DINO features</h2> <p>X. Lin, C. Ren, K. C. Chan, L. Qi, J. Pan, and M. H. Yang, “Multi-task image restoration guided by robust DINO features,” <em>arXiv preprint arXiv:2312.01677</em>, 2023 (v3 revised 2024).</p> <p><img src="/images/2025-12-26-dino/image-20251226133529550.png" alt="image-20251226133529550" class="img-fluid"/></p> <p>其核心思路是：传统的图像恢复模型在任务数量增加时性能会下降，而 DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节，因此可以作为一种<strong>退化无关的表示</strong>来引导恢复过程。</p> <p>以下是 DINO-IR 的具体实现方法和核心组件：</p> <h3 id="1-核心架构与模块">1. 核心架构与模块</h3> <p>DINO-IR 基于 <strong>Restormer</strong> 架构，并集成了以下三个关键组件：</p> <ul> <li>像素-语义融合模块 (PSF, Pixel-Semantic Fusion)： <ul> <li><strong>目的：</strong> 动态融合 DINOv2 不同层级的特征。由于浅层包含像素信息，深层包含语义信息，该模块负责提取并加权这些特征。</li> <li><strong>实现：</strong> 采用<strong>门控网络（Gating Network）*<em>和多个*</em>专家网络（Expert Networks）</strong>。门控网络会根据输入图像自适应地学习浅层、中层和深层特征的权重，将对恢复任务最有益的特征赋予更高的权重进行融合。</li> </ul> </li> <li>DINO-Restore (D-R) 适配与融合模块： <ul> <li><strong>目的：</strong> 将 DINOv2 的特征集成到图像恢复主模型中。</li> <li><strong>实现：</strong> 首先通过适配层调整 PSF 融合特征的通道数和尺度，使其与恢复模型对齐。然后采用<strong>基于自注意力的融合方式</strong>：将适配后的 DINO 特征作为 <strong>Query (Q)</strong>，而将恢复模型的中间特征作为 <strong>Key (K)</strong> 和 <strong>Value (V)</strong>，通过交叉注意力机制实现特征融合。</li> </ul> </li> </ul> <h3 id="2-dino-感知对比损失-dpc-loss">2. DINO 感知对比损失 (DPC Loss)</h3> <p>为了约束模型训练，DINO-IR 提出了一种基于 DINOv2 特征空间的<strong>对比学习损失</strong>：</p> <ul> <li><strong>原理：</strong> 提取恢复后的输出图像、原始清晰图像（正样本）和退化输入图像（负样本）在 DINOv2 隐藏层中的特征。</li> <li><strong>目标：</strong> 强制要求输出图像的 DINO 特征在空间中尽可能<strong>靠近清晰目标图像</strong>，并尽可能<strong>远离低质量输入图像</strong>。这种损失利用了 DINOv2 特征区分图像质量的能力来提升视觉效果。</li> </ul> <h3 id="3-方法优势">3. 方法优势</h3> <ul> <li><strong>退化鲁棒性：</strong> DINOv2 特征在不同噪声水平和退化类型下表现出极高的稳定性（方差远低于图像像素特征），这使得模型在处理冲突任务（如去噪需要滤除高频，而去模糊需要增强高频）时更加稳定。</li> <li><strong>泛化能力：</strong> 实验证明 DINO-IR 在<strong>未见过的退化级别</strong>（如更高强度的噪声）和<strong>未见过的测试数据集</strong>上具有更好的泛化效果。</li> <li><strong>性能提升：</strong> 在 deraining, denoising, deblurring, dehazing 四项任务的平均 PSNR 表现上，DINO-IR 优于 AirNet 和 PromptIR 等现有先进的多任务恢复方法。</li> </ul> <h3 id="note">Note</h3> <p>作者有做额外实验，DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节。</p> <p>It is known that the features extracted from shallow layersof DINOv2 (M, T, and T 2023) can discern low- and high-quality images。</p> <p>根据 DINO-IR（基于鲁棒 DINO 特征引导的多任务图像恢复）的研究资料，其提出的 <strong>DINO 感知对比损失（DINO Perception Contrastive Loss，简称 \(L_{DINO}\) 或 DPC Loss）</strong> 的公式及相关总损失公式如下：</p> <p>该损失函数旨在通过对比学习，使恢复后的图像在 DINOv2 的特征空间中靠近清晰图像，并远离退化的输入图像。公式表达为：</p> \[L_{DINO} = L(v, v^+, v^-) = \sum_{i=1}^n w_i \frac{D(\Psi_i(v), \Psi_i(v^+))}{D(\Psi_i(v), \Psi_i(v^-))}\] <p><strong>参数含义：</strong></p> <ul> <li><strong>\(v\)</strong>：恢复模型生成的输出图像。</li> <li><strong>\(v^+\)</strong>：正样本，即对应的<strong>清晰目标图像（Ground Truth）</strong>。</li> <li><strong>\(v^-\)</strong>：负样本，即<strong>低质量的退化输入图像</strong>。</li> <li><strong>\(\Psi_i\)</strong>：表示从固定的预训练 DINOv2 模型中提取的第 \(i\) 个隐藏层特征。</li> <li><strong>\(D(x, y)\)</strong>：表示 \(x\) 与 \(y\) 之间的 <strong>\(L1\) 距离</strong>。</li> <li><strong>\(w_i\)</strong>：对应层级的权重系数。</li> </ul> <p>作者并没有给出是第几层。但太深的层应该没用。</p> <h2 id="处理任意输入大小图片">处理任意输入大小图片</h2> <p>DINO（包括 DINOv2 和 DINOv3）处理任意大小图片的核心机制在于其 <strong>Transformer 架构的灵活性</strong>、<strong>分块（Patchification）策略</strong>以及<strong>位置编码的动态插值或旋转机制</strong>。</p> <p>以下是具体的实现方式：</p> <h3 id="1-灵活的序列长度处理set-to-set-架构">1. 灵活的序列长度处理（Set-to-set 架构）</h3> <p>DINO 系列模型基于 Vision Transformer (ViT)。与传统的卷积神经网络不同，Transformer 是一种<strong>“集合到集合”（set-to-set）的架构</strong>，它将图像视为一系列令牌（tokens）。</p> <ul> <li><strong>分块机制：</strong> 图像被切分为固定大小的 patch（例如 DINOv2 使用 \(14 \times 14\)，DINOv3 使用 \(16 \times 16\)）。</li> <li><strong>令牌数量随分辨率变化：</strong> 当输入图像变大时，模型只会产生更多的令牌，而 Transformer 的自注意力机制（Self-attention）天然可以处理任意长度的输入序列。</li> </ul> <h3 id="2-位置编码的适配核心技术">2. 位置编码的适配（核心技术）</h3> <p>由于 Transformer 本身无法感知令牌的空间顺序，必须加入位置编码。处理不同分辨率图像的关键在于如何让固定长度的位置编码适应变动的令牌数量：</p> <ul> <li><strong>线性插值（DINOv2/DPT 方案）：</strong> 在 DINOv2 和 DPT 中，如果输入图像的分辨率与训练时的分辨率不同，模型会对预训练的<strong>位置嵌入（Position Embeddings）进行线性插值</strong>。这使得模型能够动态适配到新的令牌网格尺寸，确保每个令牌都能获得其在图像中相对位置的信息。</li> <li><strong>旋转位置编码（DINOv3 的 RoPE 机制）：</strong> <strong>DINOv3</strong> 引入了更先进的 <strong>RoPE（Rotary Positional Embeddings）</strong> 机制。它将每个 patch 的坐标分配在一个归一化的 \([-1, 1]\) 框内，并在多头注意力操作中根据 patch 间的相对位置应用偏差。</li> <li><strong>无缝缩放：</strong> 依靠 RoPE 和坐标框抖动（box jittering）技术，DINOv3 能够<strong>在不进行任何适配的情况下无缝处理不同分辨率的图像</strong>。实验显示，即使在远超训练分辨率（如 4k 分辨率）的情况下，DINOv3 仍能保持稳定的特征表现。</li> </ul> <h3 id="3-全局感受野的维持">3. 全局感受野的维持</h3> <p>在卷积网络中，感受野随层数增加而受限，但在 DINO 中，由于使用了全局自注意力机制，<strong>每一个阶段（stage）都拥有全局感受野</strong>。</p> <ul> <li>这意味着无论图像多大，每个令牌都能与图像中的所有其他令牌进行交互。</li> <li>这种特性确保了模型在处理任意大小图片时，都能产生<strong>全局连贯（globally coherent）</strong>且精细的预测结果。</li> </ul> <h3 id="总结">总结</h3> <p>DINO 处理任意大小图片的逻辑可以类比为<strong>“拼图”</strong>：</p> <ul> <li><strong>分块</strong>是把图片切成小拼块，图越大拼块越多。</li> <li><strong>Transformer</strong> 是拼图者，他能处理任意数量的拼块。</li> <li><strong>RoPE 或插值编码</strong> 就像是在每一块拼图背面标注坐标的记号笔，通过动态缩放记号的刻度（坐标），拼图者总能知道每一块在大图中的精确位置。</li> </ul> <h3 id="dinov2-处理任意大小图像的代码">DINOv2 处理任意大小图像的代码</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">skimage.color</span> <span class="kn">import</span> <span class="n">hsv2rgb</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="c1"># --- 1. 配置参数 ---
# 图像文件路径 (已设置为您的文件)
</span><span class="n">IMAGE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fruits.jpg</span><span class="sh">'</span> 
<span class="c1"># DINOv2 模型 ID (Base 版本，公开且无需权限)
</span><span class="n">MODEL_ID</span> <span class="o">=</span> <span class="sh">"</span><span class="s">facebook/dinov2-base</span><span class="sh">"</span> 
<span class="n">DEVICE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="c1"># DINOv2-base 默认 Patch size 为 14x14
</span><span class="n">PATCH_SIZE</span> <span class="o">=</span> <span class="mi">14</span> 
<span class="c1"># 设置一个最小的安全尺寸，防止原图太小
</span><span class="n">MIN_SIZE</span> <span class="o">=</span> <span class="mi">224</span> 

<span class="k">def</span> <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    加载 DINOv2 模型，提取 Patch 特征，使用 PCA 降维并可视化。
    图像尺寸会调整到最接近原始尺寸且是 Patch Size 的整数倍。
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">错误: 图像文件未找到于 </span><span class="sh">'</span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="sh">'</span><span class="s">。请检查路径并重试。</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 2. 初始化模型和处理器 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">正在加载 DINOv2 模型: </span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s"> 到 </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># 评估模式
</span>    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型加载失败。请检查模型 ID 或网络连接。</span><span class="se">\n</span><span class="s">错误信息: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 3. 图像处理与特征提取 (重点修改部分) ---
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">原始图像尺寸 (W x H): </span><span class="si">{</span><span class="n">W_orig</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_orig</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># a. 计算目标输入尺寸 (必须是 PATCH_SIZE 的整数倍)
</span>    <span class="c1"># 取最接近原始尺寸且小于等于原始尺寸的 PATCH_SIZE 倍数
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">H_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    
    <span class="c1"># 确保尺寸不小于最小安全尺寸
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">W_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型目标输入尺寸 (W x H): </span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># b. 预处理
</span>    <span class="c1"># 显式传递 size 和 crop_size 参数，控制预处理器的缩放行为
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">processor</span><span class="p">(</span>
        <span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> 
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 缩放或调整到目标尺寸
</span>        <span class="n">crop_size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 确保不进行中心裁剪
</span>        <span class="n">do_center_crop</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 明确禁用中心裁剪
</span>    <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 实际输入模型张量的尺寸
</span>    <span class="n">h_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">w_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    
    <span class="c1"># 重新计算 Patch 网格尺寸 (H, W)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">h_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    
    <span class="c1"># c. 提取特征
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># **inputs 解包字典作为命名参数
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> 
        <span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="c1"># d. 移除 CLS Token
</span>    <span class="k">if</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">已移除 CLS Token。剩余 Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- 4. PCA 降维 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">正在进行 PCA 降维...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="c1"># 归一化到 [0, 1] 范围
</span>    <span class="n">pca_min</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pca_max</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">pca_max</span> <span class="o">-</span> <span class="n">pca_min</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-8</span> 
    <span class="n">pca_features_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pca_features</span> <span class="o">-</span> <span class="n">pca_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># --- 5. 可视化映射 ---
</span>
    <span class="c1"># a. 重塑为网格形状 (H, W, 3)
</span>    <span class="n">pca_grid</span> <span class="o">=</span> <span class="n">pca_features_norm</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># b. 映射到 HSV 颜色空间 
</span>    <span class="n">hsv_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pca_grid</span><span class="p">)</span>
    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Hue (色调)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>              <span class="c1"># Saturation (饱和度)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Value (亮度)
</span>
    <span class="c1"># 转换为 RGB 颜色
</span>    <span class="n">rgb_vis</span> <span class="o">=</span> <span class="nf">hsv2rgb</span><span class="p">(</span><span class="n">hsv_image</span><span class="p">)</span>
    
    <span class="c1"># c. 缩放可视化结果到原始图像大小
</span>    <span class="n">rgb_vis_upscaled</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span>
        <span class="n">rgb_vis</span><span class="p">,</span> 
        <span class="p">(</span><span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span><span class="p">),</span> <span class="c1"># 使用原图尺寸进行缩放
</span>        <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">INTER_NEAREST</span> <span class="c1"># 最近邻插值保持 Patch 块状效果
</span>    <span class="p">)</span>

    <span class="c1"># --- 6. 显示和保存结果 ---
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">原始图像 (Original Image: fruits.jpg)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rgb_vis_upscaled</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">DINOv2 特征 PCA 可视化 (</span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s">x</span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="s"> 输入)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">output_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dinov2_fruits_pca_visualization.png</span><span class="sh">"</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">output_filename</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">可视化结果已保存为 </span><span class="si">{</span><span class="n">output_filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">IMAGE_PATH</span><span class="p">,</span> <span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">PATCH_SIZE</span><span class="p">,</span> <span class="n">MIN_SIZE</span><span class="p">)</span>
</code></pre></div></div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="https://zhuanlan.zhihu.com/p/1986387271688139358">(3 封私信 / 80 条消息) DINOv3 is All You Need? 为什么 DINOv3 发布后，CV 圈感觉“天塌了”？ - 知乎</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[如何使用DINO, DPT, RoPE]]></summary></entry><entry><title type="html">3D 任务：SFM, MVS, NVS, VO, VIO, SLAM</title><link href="https://cekxm.github.io/blog/2025/3dtasks/" rel="alternate" type="text/html" title="3D 任务：SFM, MVS, NVS, VO, VIO, SLAM"/><published>2025-12-26T00:00:00+00:00</published><updated>2025-12-26T00:00:00+00:00</updated><id>https://cekxm.github.io/blog/2025/3dtasks</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/3dtasks/"><![CDATA[<h2 id="多视角几何mvs新视角生成nvs对比">多视角几何（MVS），新视角生成（NVS）对比</h2> <p><img src="/images/2025-12-26-3dtasks/d3dcd8d3-35cb-403b-998d-64256b21ba06.png" alt="SfM_NeRF" class="img-fluid"/></p> <h3 id="mvssfm-vs-vggt-vs-nvs-nerfgs-综合对比表">(MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>(MVS) SfM (如 COLMAP)</strong></th> <th><strong>VGGT (Visual Geometry Grounded Transformer)</strong></th> <th><strong>NVS (NeRF / 3D GS)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>核心任务目标</strong></td> <td><strong>三维重建</strong>：求解精确的相机姿态和场景几何（点云/深度）。</td> <td><strong>统一几何推断</strong>：一站式、秒级预测相机、点图、深度图和追踪。</td> <td><strong>新视角合成</strong>：在未拍摄过的角度生成照片级逼真的图像。</td> </tr> <tr> <td><strong>底层表示</strong></td> <td><strong>离散几何</strong>：稀疏或稠密的 3D 点云、深度图。</td> <td><strong>稠密几何图</strong>：Point Maps (\(H \times W \times 3\)) 和 Depth Maps。</td> <td><strong>光场表示</strong>：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。</td> </tr> <tr> <td><strong>运行机制</strong></td> <td><strong>优化驱动</strong>：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。</td> <td><strong>前馈推理 (Feed-forward)</strong>：一次性通过 Transformer 模型直接“看”出几何。</td> <td><strong>视图对齐优化</strong>：通过渲染结果与原图的颜色误差来反向训练。</td> </tr> <tr> <td><strong>对相机的需求</strong></td> <td><strong>未知或已知</strong>：通常用于解算未知相机参数。</td> <td><strong>无需预设</strong>：直接预测相机的 9 维参数（内外参）。</td> <td><strong>必须已知</strong>：通常依赖 SfM (COLMAP) 提供位姿初始化。</td> </tr> <tr> <td><strong>几何精确度</strong></td> <td><strong>高（度量级）</strong>：数学推导严谨，但易受弱纹理、模糊影响。</td> <td><strong>高且鲁棒</strong>：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。</td> <td><strong>中/低</strong>：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。</td> </tr> <tr> <td><strong>渲染视觉效果</strong></td> <td><strong>差</strong>：只有离散点，无法生成连续、真实的图像。</td> <td><strong>中</strong>：提供稠密几何，但主要用于几何任务而非美学渲染。</td> <td><strong>极高</strong>：支持照片级渲染、阴影、反射和透明效果。</td> </tr> <tr> <td><strong>处理速度</strong></td> <td><strong>极慢</strong>：通常需要数分钟到数小时。</td> <td><strong>极快</strong>：全流程通常在 <strong>1 秒以内</strong>。</td> <td><strong>训练慢/渲染快</strong>：NeRF 训练慢，GS 渲染极快，但都需要初始化。</td> </tr> </tbody> </table> <p>简而言之：<strong>(MVS)SfM</strong> 是传统的“几何尺子”；<strong>VGGT</strong> 是现代的“几何大模型”；而 <strong>NVS</strong> 是“虚拟照相机”。</p> <p>根据您上传的论文内容及相关技术背景，<strong>ESVO2 是一个 VIO（视觉惯性里程计）系统</strong>，它是基于纯视觉算法 ESVO 的增强版，引入了惯性测量单元（IMU）来提高鲁棒性。</p> <p>以下是关于 ESVO2 的定位及其与 SLAM 区别的详细介绍：</p> <h2 id="vio-与-slam-的区别">VIO 与 SLAM 的区别</h2> <p>虽然两者都旨在解决“我在哪”的问题，但它们在功能覆盖和目标重点上有所不同：</p> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>视觉惯性里程计 (VIO)</strong></th> <th><strong>同步定位与建图 (SLAM)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要组成</strong></td> <td>视觉处理 + IMU 预积分 + 后端优化</td> <td>VIO (前端) + <strong>回环检测</strong> + 全局优化</td> </tr> <tr> <td><strong>误差累计</strong></td> <td>会随着位移增加而产生<strong>累计漂移（Drift）</strong></td> <td>通过回环检测修正累计误差，具有<strong>全局一致性</strong></td> </tr> <tr> <td><strong>地图规模</strong></td> <td>通常只维护一个局部地图（滑动窗口内）</td> <td>维护全局地图，允许机器人回到已知点时重定位</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>相对较低，适合实时性要求极高的场景</td> <td>较高，需要存储和检索大量历史关键帧数据</td> </tr> <tr> <td><strong>典型代表</strong></td> <td>VINS-Mono (VIO模式), ESVO2, OKVIS</td> <td>ORB-SLAM3, VINS-Fusion (含回环)</td> </tr> </tbody> </table> <h2 id="slam">SLAM</h2> <p>在 SLAM（同步定位与建图）框架中，<strong>回环检测（Loop Closure Detection）</strong>和<strong>全局优化（Global Optimization）</strong>是消除累计误差、保证地图全局一致性的核心机制。</p> <p>如果没有这两个部分，系统仅仅是一个<strong>里程计（Odometry）</strong>，位姿误差会随着时间的推移不断增加，轨迹最终会“漂散”。</p> <hr/> <h3 id="回环检测-loop-closure-detection">回环检测 (Loop Closure Detection)</h3> <p>回环检测的任务是：<strong>识别机器人是否回到了曾经到过的地方。</strong></p> <ul> <li> <p>为什么要检测？</p> <p>里程计每一步都会引入微小的误差，经过长距离运行后，系统估计的当前位置与真实位置可能相差巨大。如果能识别出“旧地重游”，就能通过这个历史“锚点”来纠正累积误差。</p> </li> <li> <p><strong>如何实现？（主流方法：词袋模型 BoW）</strong></p> <ol> <li><strong>特征提取</strong>：提取当前图像的特征点（如 ORB）。</li> <li><strong>词袋描述</strong>：将特征点转化为一个数值向量（类似一篇文章的关键词提取）。</li> <li><strong>相似度检索</strong>：在历史图像库中寻找与当前图像最相似的帧。</li> <li><strong>几何验证</strong>：通过对极几何等方法，确认两张图不仅长得像，而且在空间逻辑上也是匹配的，从而防止“感知歧义”（例如两间长得一模一样的办公室）。</li> </ol> </li> </ul> <hr/> <h3 id="全局优化-global-optimization">全局优化 (Global Optimization)</h3> <p>一旦回环检测发现当前帧 $i$ 与历史帧 $j$ 匹配成功，系统就获得了一个闭环约束。接下来需要通过全局优化来调整整个轨迹。</p> <h4 id="位姿图优化-pose-graph-optimization"><strong>位姿图优化 (Pose Graph Optimization)</strong></h4> <p>在回环发生后，为了保持实时性，通常不再优化复杂的 3D 空间点，而仅仅优化<strong>相机的位姿节点</strong>。</p> <ul> <li><strong>节点（Nodes）</strong>：机器人每一时刻的位姿。</li> <li><strong>边（Edges）</strong>： <ul> <li><strong>里程计边</strong>：相邻帧之间的约束（由 VIO 提供）。</li> <li><strong>回环边</strong>：当前帧与历史帧之间的约束（由回环检测提供）。</li> </ul> </li> <li><strong>优化目标</strong>：调整所有节点，使得所有“边”的残差总和最小。这就像是一个弹簧网，回环边强行把漂移的轨迹拉回到历史位置，而其他轨迹点则像弹簧一样跟随调整。</li> </ul> <hr/> <h3 id="区别总结vio-vs-slam">区别总结：VIO vs SLAM</h3> <table> <thead> <tr> <th><strong>模块</strong></th> <th><strong>VIO (如 ESVO2, VINS 前端)</strong></th> <th><strong>SLAM (如 VINS 后端, ORB-SLAM3)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>漂移</strong></td> <td>随距离/时间线性增长</td> <td>遇到回环时清零</td> </tr> <tr> <td><strong>一致性</strong></td> <td>局部一致</td> <td>全局一致</td> </tr> <tr> <td><strong>地图</strong></td> <td>瞬时、局部</td> <td>持久、可重用</td> </tr> </tbody> </table> <p><strong>COLMAP</strong> 通常不被认为是一个 <strong>VO（视觉里程计）</strong> 方法，它是一个标准的 <strong>SfM（运动恢复结构）</strong> 框架。</p> <p>虽然两者底层都使用了相似的技术（如特征提取、对极几何、Bundle Adjustment），但它们在设计目标、运行方式和应用场景上有显著区别。</p> <hr/> <h2 id="sfm-与-vo-的本质区别">SfM 与 VO 的本质区别</h2> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>COLMAP (SfM)</strong></th> <th><strong>Visual Odometry (VO)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>处理时效</strong></td> <td><strong>离线 (Offline)</strong>：处理一组预先采集好的图片或视频序列。</td> <td><strong>实时 (Real-time)</strong>：随着相机的移动即时计算位姿。</td> </tr> <tr> <td><strong>图像顺序</strong></td> <td><strong>无序/有序</strong>：可以处理乱序的照片（如不同人的街拍），通过特征匹配寻找联系。</td> <td><strong>必须有序</strong>：依赖视频流的连续性进行帧间追踪。</td> </tr> <tr> <td><strong>优化范围</strong></td> <td><strong>全局 (Global)</strong>：通常对所有相机位姿和所有点进行全局优化（Global BA）。</td> <td><strong>局部 (Local)</strong>：通常只在滑动窗口内进行优化以维持实时性。</td> </tr> <tr> <td><strong>应用目标</strong></td> <td>追求<strong>极致精度</strong>和高质量的 3D 重建（如制作模型、地图）。</td> <td>追求<strong>低延迟</strong>和机器人的实时定位（如无人机飞行）。</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>极高：可能需要数小时或数天处理大型场景。</td> <td>较低：需要在嵌入式设备或普通 CPU/GPU 上实时运行。</td> </tr> </tbody> </table> <hr/> <h3 id="它们之间的联系">它们之间的联系</h3> <p>虽然它们定位不同，但有很深的血缘关系：</p> <ul> <li><strong>技术基础一致</strong>：它们都遵循“特征提取 $\rightarrow$ 特征匹配 $\rightarrow$ 位姿估计 $\rightarrow$ 三角化建图 $\rightarrow$ 后端优化”的流程。</li> <li><strong>增量式 SfM</strong>：COLMAP 属于“增量式（Incremental）”SfM，它的工作方式是一张一张地把新照片注册到现有地图中。这种“一张张增加”的逻辑在形式上非常接近 VO，只是 COLMAP 每增加一张都会做大量的全局检查以保证精度，而 VO 只看局部。</li> <li><strong>VO 是 SfM 的子集</strong>：你可以把 VO 看作是一种追求实时性、牺牲全局一致性、且只能处理有序序列的特殊 SfM。</li> </ul> <hr/> <h3 id="为什么不把-colmap-当作-vo-使用">为什么不把 COLMAP 当作 VO 使用？</h3> <p>如果你尝试用 COLMAP 跑实时定位，会遇到以下问题：</p> <ol> <li><strong>速度太慢</strong>：COLMAP 的每一步（尤其是匹配和全局优化）都非常耗时，无法做到每秒处理 30 帧。</li> <li><strong>内存消耗</strong>：COLMAP 试图建立和维护完整的场景模型，随着照片增加，内存开销会爆炸。</li> <li><strong>缺乏状态估计</strong>：VO 通常会融合 <strong>IMU（惯性测量单元）</strong> 来处理快速运动（如 VINS-Fusion），而 COLMAP 主要是纯视觉处理。</li> </ol> <h3 id="总结">总结</h3> <p><strong>COLMAP 是用来“建图”的利器，而 VO 是用来“带路”的工具。</strong></p> <ul> <li>如果你有 1000 张从各个角度拍的景区照片，想做一个 3D 模型，用 <strong>COLMAP</strong>。</li> <li>如果你有一个机器人正在屋里跑，需要知道它现在在哪，用 <strong>VINS-Mono</strong> 或 <strong>ORB-SLAM3</strong>。</li> </ul> <h2 id="研究方向">研究方向</h2> <p>在 2025 年的时间节点上，<strong>Novel View Synthesis (NVS)</strong> 在学术热度和资本市场显然更“火”，但 <strong>Multi-View Stereo (MVS)</strong> 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。</p> <p>这两者并非孤立竞争，而是呈现出一种<strong>深度融合</strong>的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：</p> <hr/> <h3 id="1-热门程度novel-view-synthesis-nvs-占据-c-位">1. 热门程度：Novel View Synthesis (NVS) 占据 C 位</h3> <p><strong>核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D</strong></p> <ul> <li><strong>学术热度：</strong> 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 <strong>3DGS (3D 高斯溅射)</strong> 和 <strong>生成式新视角合成</strong> 的论文数量呈爆炸式增长。</li> <li><strong>AIGC 助力：</strong> 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 <strong>Generative NVS</strong>）成了最热门的方向。</li> <li><strong>用户感知度：</strong> NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。</li> </ul> <h3 id="2-发展前景mvs-正在向几何大模型进化">2. 发展前景：MVS 正在向“几何大模型”进化</h3> <p><strong>核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry</strong></p> <ul> <li><strong>从“工具”到“大脑”：</strong> 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 <strong>VGGT</strong> 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个<strong>前馈网络（Feed-forward）</strong>。</li> <li><strong>解决“不可能任务”：</strong> 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。</li> <li><strong>工业刚需：</strong> 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的<strong>精确绝对坐标（Metric Geometry）</strong>。</li> </ul> <hr/> <h3 id="3-2025-年的关键趋势两者边界的模糊融合">3. 2025 年的关键趋势：两者边界的模糊（融合）</h3> <p>如果你在考虑职业发展或研究方向，<strong>“几何感知的 NVS” (Geometry-aware NVS)</strong> 是 2025 年最具前景的方向。</p> <ol> <li><strong>MVS 为前，NVS 为后：</strong> 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。</li> <li><strong>可推广性 (Generalizability)：</strong> 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 <strong>LRM (Large Reconstruction Models)</strong>，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。</li> <li><strong>动态场景：</strong> 静态场景的重建基本解决，2025 年的蓝海是<strong>动态 4D 重建</strong>（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。</li> </ol> <hr/> <h3 id="总结建议">总结建议</h3> <ul> <li><strong>如果你追求“视觉震撼”和“快速产出”：</strong> 选择 <strong>Novel View Synthesis (尤其是 3DGS)</strong>。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。</li> <li><strong>如果你追求“底层技术”和“稳健性”：</strong> 选择 <strong>MVS/几何大模型</strong>。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。</li> <li><strong>最具潜力的路径：</strong> 研究<strong>如何将 MVS 的几何约束引入 NVS</strong>（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。</li> </ul> <p><strong>结论：</strong> <strong>NVS 更“热门”（Hotter）</strong>，但 <strong>MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）</strong>。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[多视角几何，新视角生成]]></summary></entry><entry><title type="html">Matplotlib输出中文</title><link href="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/" rel="alternate" type="text/html" title="Matplotlib输出中文"/><published>2025-12-25T14:06:17+00:00</published><updated>2025-12-25T14:06:17+00:00</updated><id>https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"><![CDATA[<h4 id="步骤一在-ubuntu-系统上安装中文字体并清理缓存">步骤一：在 Ubuntu 系统上安装中文字体并清理缓存</h4> <p>您需要在 Ubuntu 24 上安装一个包含中文支持的字体包。</p> <ol> <li> <p><strong>安装中文字体（推荐：文泉驿）：</strong> 打开您的终端，执行以下命令安装常用的开源中文字体包：</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt update
sudo apt install fonts-wqy-zenhei
</code></pre></div> </div> </li> <li> <p><strong>清理 Matplotlib 缓存：</strong> Matplotlib 会缓存系统中的字体信息。如果您不清理缓存，即使安装了新字体，它也可能无法立即识别。</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 找到缓存目录的位置
python -c "import matplotlib; print(matplotlib.get_cachedir())"
   
# 通常该目录是 ~/.cache/matplotlib，清理它
rm -rf ~/.cache/matplotlib
   
# （重要）清理后，请确保重新启动您的 Cursor 终端或整个 IDE，以使新的环境变量和字体生效。
</code></pre></div> </div> </li> </ol> <h4 id="步骤二修改-python-代码配置-matplotlib-字体">步骤二：修改 Python 代码配置 Matplotlib 字体</h4> <p>在您的 Python 脚本开头，导入 Matplotlib 之后，添加字体配置代码，指定 Matplotlib 优先使用支持中文的字体。</p> <p><strong>请用下面这段代码替换您脚本开头的所有 <code class="language-plaintext highlighter-rouge">import</code> 语句以及它们之后的代码（在 <code class="language-plaintext highlighter-rouge">visualize_dinov2_features_variable_res</code> 函数定义之前）。</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># =========================================================
# Matplotlib 中文字体配置（解决 UserWarning 问题）
# =========================================================
</span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># 尝试设置 Matplotlib 的字体
</span>    <span class="c1"># 使用一个字体列表，优先尝试安装的文泉驿字体，确保兼容性
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">sans-serif</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.sans-serif</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">WenQuanYi Zen Hei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Microsoft YaHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DejaVu Sans</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 解决负号显示问题 (可选，但推荐)
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">axes.unicode_minus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matplotlib 已配置中文字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">配置中文字体失败: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">。将使用默认字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># =========================================================
</span></code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Mamba</title><link href="https://cekxm.github.io/blog/2025/mamba/" rel="alternate" type="text/html" title="Mamba"/><published>2025-12-25T14:03:54+00:00</published><updated>2025-12-25T14:03:54+00:00</updated><id>https://cekxm.github.io/blog/2025/mamba</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/mamba/"><![CDATA[<h2 id="mamba">Mamba</h2> <p>Mamba 处理的是序列的建模问题，由输入 \(x(t)\) 得到输出 \(y(t)\)，所以它更像是一个特征提取。它的优势是对于序列长度 \(L\) 来说，复杂度线性。</p> <p>S4 是 structured state space sequence model. 结构化指的是A 是 HiPPO 矩阵（是这样吗？）。但是它的参数依然是时不变的。</p> <p>Mamba 是S6，加了 selective scan，使得参数时变，依赖于输入 \(x\)。</p> <p>如果有一定的基础，可以直接看这个对比。其中 \(B,L,D,N\) 分别是 batch，序列长度，输入（输出）的特征维度，内部特征的维度。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111607190.png" alt="image-20250904111607190" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904112651698.png" alt="image-20250904112651698" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904113042955.png" alt="image-20250904113042955" class="img-fluid"/></p> <p>它会独立的把 \(x\) 的每一通道到输出 \(y\) 的每一维，中间通过一个更维的隐藏状态 \(h\)。如上图，\(x\) 的每一通道是独立计算的，因此 \(\bar{A}\)，\(\bar{B}\) 的尺寸为 \((B,L,D,N)\)。但是\(x\) 的每一通道的参数和整个 \(x\) 有关系。 \(\bar{A}\)，\(\bar{B}\) 的离散化见下图公式（4），结合上面的表格，在计算中，尺寸有变化，因此在代码中，有大量的 einsum 操作。</p> <p>由 \(A\)，\(B\) 求 \(\bar{A}\)，\(\bar{B}\) 的过程是离散化。这个和信号与系统的知识有关。它内在的过程还是连续的，但是取值的时间是离散的，所以是去算一个微分方程+初值在 \(\Delta\) 时间后的状态。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111925419.png" alt="image-20250904111925419" class="img-fluid"/></p> <h3 id="矩阵-a">矩阵 \(A\)</h3> <p>在 Mamba 模型中，矩阵 A 是对角阵（diagonal matrix），而非 HiPPO 阵。HiPPO 是一种用于初始化矩阵 A 的策略，主要出现在早期的 S4 模型中，但 Mamba 采用了对角结构并使用不同的初始化方式，以实现更高效的计算。</p> <p>在 Mamba 中，虽然矩阵 A A A 被简化为对角矩阵，但其对角元素的初始化方式仍然受到 HiPPO 矩阵的启发，具体体现在以下几个方面：</p> <ol> <li>特征值的分布 <ul> <li>HiPPO 矩阵的特征值通常被设计为负实部，以确保系统的稳定性。在 Mamba 中，对角矩阵 A A A 的对角元素（即其特征值）被初始化为负值，模仿 HiPPO 矩阵的稳定性特性。这确保了 Mamba 在处理长序列时不会出现数值不稳定的问题。</li> <li>具体来说，Mamba 的对角元素通常被初始化为负的、对数分布的值（如 −1,−2,−4,…-1, -2, -4, \ldots−1,−2,−4,… 或类似的分布），这与 HiPPO 矩阵的特征值分布有相似的动机，即通过控制特征值的范围来平衡短期和长期记忆。</li> </ul> </li> <li>动态生成对角元素 <ul> <li>Mamba 的对角矩阵 A A A 的对角元素是由网络参数动态生成的，而不是固定的。这些参数在训练开始时会根据 HiPPO 的思想进行初始化。例如，Mamba 可能通过对数尺度（log-scale）初始化对角元素，以模拟 HiPPO 矩阵在不同时间尺度上的记忆能力。</li> <li>这种初始化方式使得 Mamba 能够在训练初期就具备捕捉长程依赖的能力，而无需像 S4 那样依赖稠密的 HiPPO 矩阵。</li> </ul> </li> <li>高效性与简化 <ul> <li>HiPPO 矩阵通常是稠密的，计算成本较高。Mamba 通过将 A A A 限制为对角矩阵，极大地降低了计算复杂度（从 O(N2) O(N^2) O(N2) 降到 O(N) O(N) O(N)，其中 N N N 是状态维度）。</li> <li>尽管结构上简化了，Mamba 仍然通过借鉴 HiPPO 的初始化策略，保留了其在序列建模中的核心优势，如对长序列的记忆能力和稳定性。</li> </ul> </li> </ol> <h3 id="gpu-sram--gpu-hbm">GPU SRAM + GPU HBM</h3> <p>这一点也是 mamba 效率的关键。</p> <p>参见1 <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a> Hardware-aware Algorithm</p> <p>以及原论文 sec3.3.2</p> <h2 id="参考文献">参考文献</h2> <h3 id="博客">博客</h3> <ol> <li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a></li> </ol> <h3 id="代码">代码</h3> <h4 id="自然语言处理">自然语言处理</h4> <ol> <li> <p><a href="https://github.com/johnma2006/mamba-minimal/tree/master">GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.</a></p> <p>这个代码给出了 mamba 的简化结构代码，没有训练，但是有调用预训练模型（<a href="https://huggingface.co/state-spaces/mamba-370m/tree/main">需要从 huggingface 上下载，大小1.5G</a>）。</p> <p>可以理解它的底层代码。</p> </li> <li> <p><a href="https://readmedium.com/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049">Building Mamba from Scratch: A Comprehensive Code Walkthrough</a></p> </li> </ol> <p>这个代码是整套的，包含底层结构和训练，使用 enwiki8 数据。可以运行，我把它放到了 colab 上。</p> <p><img src="/images/2025-12-25-mamba/image-20250904095945491.png" alt="image-20250904095945491" class="img-fluid"/></p> <p>刚开始的 loss 和初始化比较有关系。（昨天因为这个原因，在 M4 上暂停了，回头再跑一次）。</p> <h4 id="图像">图像</h4> <p><a href="https://github.com/pprp/Vision-Mamba-CIFAR10">GitHub - pprp/Vision-Mamba-CIFAR10</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry></feed>
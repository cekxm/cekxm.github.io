<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cekxm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cekxm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-25T14:08:57+00:00</updated><id>https://cekxm.github.io/feed.xml</id><title type="html">计算机视觉</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Matplotlib输出中文</title><link href="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/" rel="alternate" type="text/html" title="Matplotlib输出中文"/><published>2025-12-25T14:06:17+00:00</published><updated>2025-12-25T14:06:17+00:00</updated><id>https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"><![CDATA[<h4 id="步骤一在-ubuntu-系统上安装中文字体并清理缓存">步骤一：在 Ubuntu 系统上安装中文字体并清理缓存</h4> <p>您需要在 Ubuntu 24 上安装一个包含中文支持的字体包。</p> <ol> <li> <p><strong>安装中文字体（推荐：文泉驿）：</strong> 打开您的终端，执行以下命令安装常用的开源中文字体包：</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt update
sudo apt install fonts-wqy-zenhei
</code></pre></div> </div> </li> <li> <p><strong>清理 Matplotlib 缓存：</strong> Matplotlib 会缓存系统中的字体信息。如果您不清理缓存，即使安装了新字体，它也可能无法立即识别。</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 找到缓存目录的位置
python -c "import matplotlib; print(matplotlib.get_cachedir())"
   
# 通常该目录是 ~/.cache/matplotlib，清理它
rm -rf ~/.cache/matplotlib
   
# （重要）清理后，请确保重新启动您的 Cursor 终端或整个 IDE，以使新的环境变量和字体生效。
</code></pre></div> </div> </li> </ol> <h4 id="步骤二修改-python-代码配置-matplotlib-字体">步骤二：修改 Python 代码配置 Matplotlib 字体</h4> <p>在您的 Python 脚本开头，导入 Matplotlib 之后，添加字体配置代码，指定 Matplotlib 优先使用支持中文的字体。</p> <p><strong>请用下面这段代码替换您脚本开头的所有 <code class="language-plaintext highlighter-rouge">import</code> 语句以及它们之后的代码（在 <code class="language-plaintext highlighter-rouge">visualize_dinov2_features_variable_res</code> 函数定义之前）。</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># =========================================================
# Matplotlib 中文字体配置（解决 UserWarning 问题）
# =========================================================
</span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># 尝试设置 Matplotlib 的字体
</span>    <span class="c1"># 使用一个字体列表，优先尝试安装的文泉驿字体，确保兼容性
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">sans-serif</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.sans-serif</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">WenQuanYi Zen Hei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Microsoft YaHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DejaVu Sans</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 解决负号显示问题 (可选，但推荐)
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">axes.unicode_minus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matplotlib 已配置中文字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">配置中文字体失败: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">。将使用默认字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># =========================================================
</span></code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Mamba</title><link href="https://cekxm.github.io/blog/2025/mamba/" rel="alternate" type="text/html" title="Mamba"/><published>2025-12-25T14:03:54+00:00</published><updated>2025-12-25T14:03:54+00:00</updated><id>https://cekxm.github.io/blog/2025/mamba</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/mamba/"><![CDATA[<h2 id="mamba">Mamba</h2> <p>Mamba 处理的是序列的建模问题，由输入 \(x(t)\) 得到输出 \(y(t)\)，所以它更像是一个特征提取。它的优势是对于序列长度 \(L\) 来说，复杂度线性。</p> <p>S4 是 structured state space sequence model. 结构化指的是A 是 HiPPO 矩阵（是这样吗？）。但是它的参数依然是时不变的。</p> <p>Mamba 是S6，加了 selective scan，使得参数时变，依赖于输入 \(x\)。</p> <p>如果有一定的基础，可以直接看这个对比。其中 \(B,L,D,N\) 分别是 batch，序列长度，输入（输出）的特征维度，内部特征的维度。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111607190.png" alt="image-20250904111607190" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904112651698.png" alt="image-20250904112651698" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904113042955.png" alt="image-20250904113042955" class="img-fluid"/></p> <p>它会独立的把 \(x\) 的每一通道到输出 \(y\) 的每一维，中间通过一个更维的隐藏状态 \(h\)。如上图，\(x\) 的每一通道是独立计算的，因此 \(\bar{A}\)，\(\bar{B}\) 的尺寸为 \((B,L,D,N)\)。但是\(x\) 的每一通道的参数和整个 \(x\) 有关系。 \(\bar{A}\)，\(\bar{B}\) 的离散化见下图公式（4），结合上面的表格，在计算中，尺寸有变化，因此在代码中，有大量的 einsum 操作。</p> <p>由 \(A\)，\(B\) 求 \(\bar{A}\)，\(\bar{B}\) 的过程是离散化。这个和信号与系统的知识有关。它内在的过程还是连续的，但是取值的时间是离散的，所以是去算一个微分方程+初值在 \(\Delta\) 时间后的状态。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111925419.png" alt="image-20250904111925419" class="img-fluid"/></p> <h3 id="矩阵-a">矩阵 \(A\)</h3> <p>在 Mamba 模型中，矩阵 A 是对角阵（diagonal matrix），而非 HiPPO 阵。HiPPO 是一种用于初始化矩阵 A 的策略，主要出现在早期的 S4 模型中，但 Mamba 采用了对角结构并使用不同的初始化方式，以实现更高效的计算。</p> <p>在 Mamba 中，虽然矩阵 A A A 被简化为对角矩阵，但其对角元素的初始化方式仍然受到 HiPPO 矩阵的启发，具体体现在以下几个方面：</p> <ol> <li>特征值的分布 <ul> <li>HiPPO 矩阵的特征值通常被设计为负实部，以确保系统的稳定性。在 Mamba 中，对角矩阵 A A A 的对角元素（即其特征值）被初始化为负值，模仿 HiPPO 矩阵的稳定性特性。这确保了 Mamba 在处理长序列时不会出现数值不稳定的问题。</li> <li>具体来说，Mamba 的对角元素通常被初始化为负的、对数分布的值（如 −1,−2,−4,…-1, -2, -4, \ldots−1,−2,−4,… 或类似的分布），这与 HiPPO 矩阵的特征值分布有相似的动机，即通过控制特征值的范围来平衡短期和长期记忆。</li> </ul> </li> <li>动态生成对角元素 <ul> <li>Mamba 的对角矩阵 A A A 的对角元素是由网络参数动态生成的，而不是固定的。这些参数在训练开始时会根据 HiPPO 的思想进行初始化。例如，Mamba 可能通过对数尺度（log-scale）初始化对角元素，以模拟 HiPPO 矩阵在不同时间尺度上的记忆能力。</li> <li>这种初始化方式使得 Mamba 能够在训练初期就具备捕捉长程依赖的能力，而无需像 S4 那样依赖稠密的 HiPPO 矩阵。</li> </ul> </li> <li>高效性与简化 <ul> <li>HiPPO 矩阵通常是稠密的，计算成本较高。Mamba 通过将 A A A 限制为对角矩阵，极大地降低了计算复杂度（从 O(N2) O(N^2) O(N2) 降到 O(N) O(N) O(N)，其中 N N N 是状态维度）。</li> <li>尽管结构上简化了，Mamba 仍然通过借鉴 HiPPO 的初始化策略，保留了其在序列建模中的核心优势，如对长序列的记忆能力和稳定性。</li> </ul> </li> </ol> <h3 id="gpu-sram--gpu-hbm">GPU SRAM + GPU HBM</h3> <p>这一点也是 mamba 效率的关键。</p> <p>参见1 <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a> Hardware-aware Algorithm</p> <p>以及原论文 sec3.3.2</p> <h2 id="参考文献">参考文献</h2> <h3 id="博客">博客</h3> <ol> <li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a></li> </ol> <h3 id="代码">代码</h3> <h4 id="自然语言处理">自然语言处理</h4> <ol> <li> <p><a href="https://github.com/johnma2006/mamba-minimal/tree/master">GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.</a></p> <p>这个代码给出了 mamba 的简化结构代码，没有训练，但是有调用预训练模型（<a href="https://huggingface.co/state-spaces/mamba-370m/tree/main">需要从 huggingface 上下载，大小1.5G</a>）。</p> <p>可以理解它的底层代码。</p> </li> <li> <p><a href="https://readmedium.com/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049">Building Mamba from Scratch: A Comprehensive Code Walkthrough</a></p> </li> </ol> <p>这个代码是整套的，包含底层结构和训练，使用 enwiki8 数据。可以运行，我把它放到了 colab 上。</p> <p><img src="/images/2025-12-25-mamba/image-20250904095945491.png" alt="image-20250904095945491" class="img-fluid"/></p> <p>刚开始的 loss 和初始化比较有关系。（昨天因为这个原因，在 M4 上暂停了，回头再跑一次）。</p> <h4 id="图像">图像</h4> <p><a href="https://github.com/pprp/Vision-Mamba-CIFAR10">GitHub - pprp/Vision-Mamba-CIFAR10</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Flow</title><link href="https://cekxm.github.io/blog/2025/flow/" rel="alternate" type="text/html" title="Flow"/><published>2025-12-25T13:44:12+00:00</published><updated>2025-12-25T13:44:12+00:00</updated><id>https://cekxm.github.io/blog/2025/flow</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/flow/"><![CDATA[<h1 id="flow-matching">Flow matching</h1> <h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <table> <tbody> <tr> <td>[1] [Flow Matching For Generative Models From Scratch</td> <td>by Nikolaus Correll</td> <td>Toward Humanoids</td> <td>Medium](https://medium.com/@nikolaus.correll/flow-matching-for-generative-models-from-scratch-8264bad4e0ba)</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>[2] [A Visual Dive into Conditional Flow Matching</td> <td>ICLR Blogposts 2025](https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/)</td> </tr> </tbody> </table> <h3 id="toy-example">toy example</h3> <p><a href="https://github.com/Whalefishin/Latent_Flow_Matching_MNIST">GitHub - Whalefishin/Latent_Flow_Matching_MNIST: A minimal example for training a flow matching model in a pretrained VAE’s latent space to generate MNIST digits.</a></p> <p><a href="https://github.com/lebellig/discrete-fm">GitHub - lebellig/discrete-fm: Educational implementation of the Discrete Flow Matching paper</a></p> <h2 id="理论">理论</h2> <h3 id="continuous-normalizing-flows">Continuous Normalizing Flows</h3> <p>以下主要参考 [2]</p> <p><img src="/images/2025-12-25-flow/image-20250911165058417.png" alt="image-20250911165058417" class="img-fluid"/></p> <p>首先要理解这个图。</p> <ul> <li> <p>概率路径 \(p_t\) 指 \(t\) 时刻 \(x\) 的概率分布 \(p_t(x)\)，一般令 \(p_0(t)\) 是一个标准的高斯分布，\(p_1(t)\) 是我们要建模的、未知的分布。概率路径就是从0时刻到1时刻，概率分布的变化过程。</p> </li> <li> <p>速度场 \(u(x,t)\) 比较好理解</p> </li> <li> <p>flow 就是流，类似水流，水流中 \(x\) 位置的一个水滴（假设它有独特的标记），经过 \(t\) 时候，会跟随水流移动到 \(f(x,t)\) 位置。 \(f(x,t)\) 可以看成一个映射，\(f^u: \mathbb{R}^d \times[0, 1] \to \mathbb{R}^d\)</p> </li> <li> <p>和 flow 有关系的另一个变量是 \(x(t)\) ,也是表示原处于 \(x_0\) 位置的点随时间t的变化，见下面这个常微分方程（ODE），它很好理解，速度是位置的导数。</p> </li> </ul> \[\begin{cases} x(0) = x_0 \\ \partial_t x(t) = u(x(t), t) \quad \forall t \in [0, 1] \end{cases}\] <p>在文献[2]中，\(\partial_t\) 就是指 \(\frac{\partial}{\partial t}\)。上式也叫 initial value problem（初值问题）。flow \(f^u(x,t)\) defined as the solution at time \(t\) to the initial value problem driven by \(u\) with initial condition \(x(0)=x\).</p> <p>上面这个图，三个要素形成三角关系。</p> <ul> <li>flow 和速度场之间是 ODE 的关系，知道速度场，可以求解 flow。\(f^u(x,t)\) 是它的解，\(x\) 实际上是初值。</li> <li>概率路径和速度场之间满足连续性方程</li> </ul> \[\begin{equation}\label{eq:continuity_eq} \partial_t p_t + \nabla\cdot u_t p_t = 0 \end{equation}\] <p>其中 \(\nabla\cdot\) 表示散度。这个方程保证了概率质量的守恒：概率密度在空间中的变化（通过时间导数 \(\frac{\partial p_t(x)}{\partial t}\)）由概率流量的散度决定。</p> <ul> <li>概率路径和flow之间满足 <em>change-of-variable formula</em>，也称为 pushforward \(p_t = f^u(\cdot, t)\# p_0\)。就是说通过这个流，把 \(p_0\) 经时间t，变为 \(p_t\)。</li> </ul> <p>以下是AI关于change-of-variable formula的介绍。</p> <p><strong>Change-of-variable formula</strong> 描述了概率密度如何随这种映射变化。对于一个可逆的映射 \(\phi_t: x_0 \mapsto x_t\) （可逆映射在这边就是flow，它是一个映射，并且是可逆的），概率密度 \(p_t(x_t)\) 与初始密度 \(p_0(x_0)\) 的关系为： \(p_t(x_t) = p_0(x_0) \cdot \left| \det\left( \frac{\partial \phi_t^{-1}}{\partial x_t} \right) \right| = p_0(\phi_t^{-1}(x_t)) \cdot \left| \det\left( \frac{\partial x_0}{\partial x_t} \right) \right|\) 其中：</p> <ul> <li>\(\phi_t^{-1}\) 是逆映射，从 \(x_t\) 映射回 \(x_0\)。</li> <li>\(\det\left( \frac{\partial x_0}{\partial x_t} \right)\) 是逆映射的雅可比行列式的绝对值。</li> </ul> <p>在流匹配中，雅可比行列式反映了流映射如何缩放空间，从而影响概率密度。</p> <blockquote> <p>我之前有疑问，从 \(x_0\) 到 \(x_t\) 可能经过了很长的距离变化，怎么这两个位置之间的密度能建立联系？答案可能是因为流是连续的，\(x_0\) 附近的质量都会流到 \(x_t\) 附近，这个变化由雅可比行列式来决定。</p> </blockquote> <h3 id="conditional-flow-matching">Conditional Flow Matching</h3> <p>可能，flow matching 就是指 conditional flow matching.</p> <p>CFM 核心思想是选择一个条件变量 \(z\)，以及一个条件概率路径 \(p(x\mid t,z)\)，满足两点</p> <ol> <li>由 \(p(x\mid t,z)\)​ 推导出的全局概率路径 \(p(x\mid t)\)​ 可以把 \(p_0\)​ 转化为 \(p_{data}\)​。即要求 \(p(x\mid t,z)\) 在 t=0,t=1 的边际概率 \(p(x\mid t=0)\)，\(p(x\mid t=1)\)符合</li> </ol> \[\begin{align*} \forall x \space E_z [ p(x \vert z, t=0) ] = p_0(x) , \\ \forall x \space E_z [ p(x \vert z, t=1) ] = p_{data}(x). \end{align*}\] <ol> <li>\(p(x\mid t,z)\) 对应的条件速度场 \(u^{cond}(x,t,z)\) （回忆一下，二者的关系是连续性方程）具有一个解析的形式，这是因为要使用一个神经网络来回归条件速度场。</li> </ol> <p>文中用下图进行概括：</p> <p><img src="/images/2025-12-25-flow/image-20250911233204539.png" alt="image-20250911233204539" class="img-fluid"/></p> <ul> <li> <p>首先，要做出 choice 1</p> </li> <li> <p>然后做出 choice 2（必须满足要求1）</p> </li> <li> <p>\(p(x\mid t,z)\) 能够确定 \(u^{cond}(x,t,z)\) ，它们之间满足连续性方程</p> </li> <li> <p>\(p(x\mid t,z)\) 边际化 \(z\) 可以获得 \(p(x\mid t)\)</p> </li> <li> <p>\(p(x\mid t)\) 和速度场 \(u(x,t)\) 之间满足连续性方程</p> </li> <li> <p>\(u(x,t)\)可由 \(u^{cond}(x,t,z)\)​ 显式的表达，这个关系就是文中的Theorem 1:</p> </li> </ul> \[\begin{align} \forall t, x, \, \, u(x,t) &amp;= E_{z\mid x, t} {u^{cond} }(x,t,z) \end{align}\] <ul> <li>实际计算中，并不是通过这个表达式去获得 \(u(x,t)\)。而是使用一个神经网络来回归条件速度场。那回归条件速度场有什么用呢？这个解释是文中的 Theorem 2. 即使用下面这个Loss来回归条件速度场，</li> </ul> \[\begin{aligned} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \overset{\mathrm{def}}{=} E_{ \substack{t \sim \mathcal{U}([0, 1]) \\ z \sim p_z \\ x \sim p( \cdot \mid t, z) } }{\lVert u_\theta^{CFM}(x,t) - \underbrace{u^{cond}(x,t,z)}_{\substack{ \text{chosen to be} \\ \text{explictly defined}, \\ \text{cheap to compute}, \\ \text{e.g., } x_1 - x_0}} \rVert^2} \enspace, \end{aligned}\] <p>等价于回归不可知的速度场</p> \[\begin{align*} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \underset{(\text{proof below})}{=} E_{\substack{ t \sim \mathcal{U}([0, 1]) \\ x \sim p_t} } \Vert{u_\theta^{CFM}(x,t) - \underbrace{u(x,t)}_{\substack{\text{implicitly defined,} \\ \text{hard/expensive} \\ \text{to compute}}}}\Vert^2 + \underbrace{C}_{\text{indep. of } \theta} \end{align*}\] <p>Theorem 2 的证明使用了Theorem 1.</p> <p>因此，我们用神经网络去逼近条件速度 \(u^{cond}(x,t,z)\)，最终学习得到的是经过点 \((x,t)\) 的所有轨迹的平均速度，这个平均速度也就是 \(u(x,t)\)。</p> <h1 id="mean-flow">Mean flow</h1> <h2 id="mean-flow-论文中的-flow-matching-定义">mean flow 论文中的 flow matching 定义</h2> <p>flow 的 0 时刻为 \(p_{data}(x)\)，1 时刻为 \(p_{prior}(\epsilon)\)。flow 是把数据映射为先验（一版是高斯噪声），和之前定义的映射反过来了。给定\(x\sim p_{prior}(\epsilon)\)， \(x\sim p_{data}(x)\)，定义 flow path: \(z_t=a_t x+b_t \epsilon\)，其中 \(a_t\) , \(b_t\) 是 predifined schedules.</p> <blockquote> <p>可能，只要满足 \(z_0=\epsilon\), \(z_1=x\) 就行。</p> </blockquote> <p>比较常用的是，\(a_t=1-t\), \(b_t=t\)。由于速度 \(v_t = z'_t=a'_t x+b'_t \epsilon\)，因此 \(v_t = \epsilon - x\)。</p> <p>给定速度场 \(v(z_t,t)\)，通过求解 ODE 来进行数据采样 \(z_t\): \(\frac{d}{dt} z_t=v(z_t,t)\) starting from \(z_1=\epsilon\)。注意，这个初值问题的初值是 \(z_1\)。所以是从 \(t=1\) 倒推的。解可以写成 \(z_r=z_t - \int_r^t v(z_r,r)dr\) 实现时，是用数值解，比如欧拉法 \(z_{t_{i+1}}=z_{t_{i}}+(t_{i+1}-t_i)v(z_{t_{i}},t_i)\) 注意，这两个式子没有矛盾。倒推时，\(t_{i+1}-t_i&lt;0\)，速度取的时间是 \(t_i\)。</p> <h2 id="mean-flows">mean flows</h2> <p>定义平均速度 \(u\)： \(u(z_t,r,t) \triangleq \frac{1}{t-r}\int_r^tv(z_r,r)dr\) 用一个神经网络 \(u_\theta(z_t,r,t)\) 来预测平均速度 \(u\)。在训练时，就需要它的真值。经过推导可得 \(u(z_t,r,t) = v(z_t,t)-(t-r)\frac{d}{dt}u(z_t,r,t)\)</p> \[\frac{d}{dt}u(z_t,r,t) =v(z_t,t)\partial_z u +\partial_t u\] <p>可以看出，要知道真值，需要真值对时间的微分，这没法获得。所以实际使用的真值是 \(u_{tgt} = v(z_t,t)-(t-r)(v(z_t,t)\partial_z u_\theta +\partial_t u_\theta)\) 即在计算微分时，用参数化的 \(\partial u_\theta\) 来代替 \(\partial u\)。并且回顾一下神经网络的训练，真值用于计算 loss，它本身一般是一个固定值。而在这边真值和 \(u_\theta\) 有关系，所以需要额外设定它不参与微分计算，否则就会 “double backpropagation”。 \(\mathcal{L}(\theta)=E\parallel u_\theta(z_t,r,t) - sg(u_{tgt})\parallel_2^2\) <img src="/images/2025-12-25-flow/image-20251012141549566.png" alt="image-20251012141549566" class="img-fluid"/></p> <h2 id="mean-flows-with-guidance">Mean Flows with Guidance</h2> <p>classifier-free guidance</p> <p>推导看得不是很懂，但是训练过程，如下</p> <p><img src="/images/2025-12-25-flow/image-20251013095354856.png" alt="image-20251013095354856" class="img-fluid"/></p> <p>有区别的地方在于式（19），\(v_t\) 是 \(\epsilon -x\)，\(u_\theta^{cfg}(z_t,t,t)\)注意后面的两个时间都是 t，它是一个速度。</p> <p>上面 \(\omega\) 表示引导的强度。对照原版的CFG，原版的CFG还在训练时是没有 \(\omega\) 的，只在采样时使用，在训练时，有一步骤是以一定概率不给class，即 \(c=\empty\)。此处，同样有这步。</p> <h1 id="2025-cvpr-reversing-flow-for-image-restoration">2025 CVPR Reversing Flow for Image Restoration</h1> <p>论文强调了<strong>不确定范围（uncertainty scope）</strong>的概念（如图1所示）：从HQ到LQ的退化过程遵循数据处理不等式（Data Processing Inequality, DPI），即HQ与中间图像之间的互信息（mutual information）逐渐减少。随着退化加深，LQ图像的“不确定范围”扩大——多个HQ图像可能退化到相似的LQ图像（例如，不同清晰图像添加雾霾后变得相似）。图1直观描绘了这一过程：灰色区域表示从中间状态的不确定范围，随着从LQ向HQ逆转，不确定范围缩小，互信息增加。</p> <p>现有生成模型（如扩散模型或分数匹配模型）通常将退化过程建模为<strong>随机变换（stochastic transformation）</strong>，从高斯噪声开始逆向生成HQ图像。这引入了不必要的复杂性和计算开销（如数百步采样），因为<strong>LQ图像已提供结构信息，无需从纯噪声重构</strong>。论文指出，这种随机性导致训练和推理效率低下，且忽略了退化过程的确定性本质。</p> <p>ResFlow的核心动机：将退化过程重新定义为<strong>确定性路径（deterministic path）</strong>，使用连续归一化流（Continuous Normalizing Flows）实现可逆映射，从而高效逆转退化，仅需少于4步采样。</p> <blockquote> <p>这是论文的第一个评论，扩散模型属于随机前向过程，论文认为是不好的。flow matching 是确定性前向过程。但这个评论和从高斯噪声开始采样不是同一个问题。</p> <p>论文认为，从高斯噪声开始采样没有必要，因为LQ图像已提供结构信息。</p> <p>有一些扩散模型的方法从LQ图像开始恢复，比如论文第二页罗列的一些。但是论文说这些方法依然是随机前向过程。However, these approaches still treat the degradation process as a progressively diffusing stochastic forward process, which seems unnecessary and introduces additional complexity and inefficiency. Given that the degraded image is already known, the degradation process could be redefined as a deterministic forward process.</p> </blockquote> <table> <thead> <tr> <th>方法</th> <th>t=0 分布（起始/目标分布）</th> <th>t=T 分布（结束/噪声分布）</th> <th>关键特点</th> </tr> </thead> <tbody> <tr> <td><strong>DDRM [38]</strong></td> <td>清晰图像分布（HQ/clean image distribution）。恢复过程的最终输出 x_0 是估计的 HQ 图像。</td> <td>近似噪声分布（approximated noise distribution，通常高斯噪声 N(0, I)）。x_T 是 Markov 链的起始，条件于退化观测 y = H x + z（H 是退化算子，z 是已知噪声）。</td> <td>基于预训练 DDPM，解决线性逆问题（如去模糊、超分辨率）。前向从 HQ 扩散到噪声；逆向从噪声恢复 HQ，条件于 LQ（degraded image）。</td> </tr> <tr> <td><strong>IR-SDE [64]</strong></td> <td>清晰图像分布（HQ/high-quality image x(0)）。</td> <td>退化图像的噪声版本（LQ + fixed Gaussian noise, μ + ε，其中 μ 是 LQ，ε ~ N(0, σ²I)）。</td> <td>使用 mean-reverting SDE 建模退化过程，从 HQ 扩散到 noisy LQ。逆向从 noisy LQ 开始恢复 HQ。非 Markov 链，而是连续 SDE。</td> </tr> <tr> <td><strong>I2SB [55]</strong></td> <td>一个给定分布（通常清晰图像分布 HQ/clean data distribution）。</td> <td>另一个给定分布（退化图像分布 LQ/degraded data distribution）。</td> <td>构建 Schrödinger bridge（扩散桥），直接连接两个分布（clean 和 degraded）。非线性扩散过程，从 LQ 桥接到 HQ。边界对：t=0 为 clean，t=T 为 degraded（或反之）。</td> </tr> <tr> <td><strong>ResShift [112]</strong></td> <td>高分辨率图像分布（HR/high-resolution image distribution）。初始状态近似 HR。</td> <td>低分辨率图像分布（LR/low-resolution image distribution）。最终状态近似 LR。</td> <td>通过 shifting residual 在 HR 和 LR 之间构建 Markov 链。针对超分辨率（SR），减少步数；t=0 为 HR，t=T 为 LR。</td> </tr> <tr> <td><strong>RDDM [57]</strong></td> <td>目标图像分布（HQ/target image distribution）。</td> <td>纯噪声分布（pure noise, N(0, I)）用于生成；或噪声携带的输入图像（noise-carrying input, LQ + noise）用于恢复。</td> <td>双重扩散：residual diffusion（从 HQ 到 LQ 的定向扩散）和 noise diffusion（随机扰动）。统一生成和恢复；t=T 根据任务调整（纯噪声或 noisy LQ）。</td> </tr> <tr> <td><strong>Resfusion [89]</strong></td> <td>原图像分布（ground truth/original image distribution）。</td> <td>noisy degraded 图像分布（noisy degraded images, LQ + weighted residual noise）。</td> <td>将 residual term 引入前向过程，从 noisy LQ 开始逆过程。预测 resnoise（weighted residual + noise）；t=0 为 HQ，t=T 为 noisy LQ。统一训练/推理。</td> </tr> </tbody> </table> <p>因此论文，要 reverses the deterministic paths between HQ and LQ images for image restoration。即要使用flow matching 方法，同时从 LQ开始进行恢复。这样的问题是前面谈到的不确定范围。</p> <p>但直接用flow来建模HQ到LQ的退化是不可以的。这是因为退化导致互信息下降（信息处理不等式），而flow对应的ODE却会保持互信息不变。见命题1</p> <h2 id="命题1">命题1</h2> <p>flow 的 OED 保持互信息不变。 \(MI(z_{t_1}, r) = MI(z_{t_2}, r),\) 其中：</p> <p>\(z_t\) 是随时间 \(t\) 变化的随机过程（random process），由普通微分方程（ODE）定义：\(\frac{\partial z_t}{\partial t} = v(z_t, t)\)。 \(r\) 是任意参考随机变量（reference random variable），可以是 \(z_t\) 的任意状态（如 \(z_0\) 或其他 \(z_{t'}\)）。 \(MI(\cdot, \cdot)\) 表示互信息，定义为两个随机变量之间的共同信息量，数学上等价于： \(MI(X, Y) = H(X) + H(Y) - H(X, Y),\) 其中 \(H(X)\) 和 \(H(Y)\) 分别是 \(X\) 和 \(Y\) 的熵，\(H(X, Y)\) 是联合熵。</p> <h2 id="逆转流的基本形式reversing-flow-for-image-restoration">逆转流的基本形式（Reversing Flow for Image Restoration）</h2> <p>退化过程定义为随机过程{zt | 0 ≤ t ≤ 1}上的ODE： \(\frac{\partial z_t}{\partial t} = v(z_t, t); \quad 0 \leq t \leq 1,\) 其中v是速度场（velocity field），z_0对应HQ图像x_HQ，z_1对应LQ图像x_LQ。</p> <table> <tbody> <tr> <td>为使过程可逆，引入辅助过程{yt</td> <td>0 ≤ t ≤ 1}，增强状态：</td> </tr> </tbody> </table> \[z_t^T = [x_t^T; y_t^T], \quad z_0^T = [x_{HQ}^T; y_0^T], \quad z_1^T = [x_{LQ}^T; y_1^T].\] <p>yt编码“信息丢失”，与不确定范围耦合：当x_t接近x_LQ时，y_t与x_0的互信息增加，以保持整体MI(z_t, z_0)恒定</p> <p>。图2框架：zt由数据组件x_t（HQ到LQ）和辅助组件y_t（不确定范围缩小）组成。前向过程通过插值定义，逆向过程通过匹配速度场学习。神经网络v_θ估计速度： \(\frac{\partial [x_t^T; y_t^T]^T}{\partial t} = v_\theta(x_t, y_t, t).\)</p> <p>在实际实现中，\(y_0=0\), \(y_1\sim N(0,I)\).从 \(y_0\) 到 \(y_1\) 熵是增加的。</p> <h2 id="resflow-训练过程算法步骤">ResFlow 训练过程算法步骤</h2> <p>论文中的训练过程基于速度场匹配（velocity field matching），通过最小化Eq. (9)的损失函数实现。采用U-Net架构作为v_θ，Adam优化器，训练在256分辨率图像crops上进行。以下是算法步骤伪代码： text算法: ResFlow 训练过程</p> <p>输入: HQ-LQ图像对数据集 {(x0, x1)}，超参数 β=10, γ=1.75, 学习率 (详见Appendix C) 输出: 训练好的速度场网络 v_θ</p> <ol> <li> <p>初始化神经网络 v_θ (采用DDPM的U-Net架构，timestep t 通过adaptive layer normalization嵌入)</p> </li> <li> <p>对于每个训练epoch: a. 从数据集采样一个batch的HQ-LQ对 (x0, x1) b. 对于每个样本: i. 设置 y0 = 0 (零向量) ii. 采样 y1 ~ N(0, I) (标准高斯分布) iii. 定义退化调度: α^x_t = 1 - t, σ^x_t = t (对于数据组件 x) α^y_t = 1 - σ^y_t, σ^y_t = β / (1 - t + β) (对于辅助组件 y, 熵保持) iv. 计算路径点 (geodesics): x_t = α^x_t * x0 + σ^x_t * x1 y_t = α^y_t * y0 + σ^y_t * y1 z_t = [x_t; y_t] (增强状态) ˙z_t = [˙α^x_t * x0 + ˙σ^x_t * x1; ˙α^y_t * y0 + ˙σ^y_t * y1] (真实速度) v. 通过网络预测速度: v_θ(x_t, y_t, t) vi. 计算时间权重: λ(t) = [cos(π/2 * (t - 2)) + 1]^γ (强调t接近1) vii. 计算损失: L = ∫_0^1 λ(t) * ||v_θ(x_t, y_t, t) - ˙z_t||^2_2 dt (积分近似或蒙特卡罗采样t) c. 平均batch损失 d. 使用Adam优化器更新 θ (反向传播)</p> </li> <li> <p>重复步骤2直到收敛 (详见Appendix C的超参数，如学习率、batch size) 注意：损失优化确保凸传输成本非增，且无需模拟ODE（与传统流方法不同）。训练强调t接近1的困难样本，以平衡梯度。 ResFlow 采样过程算法步骤 论文中的采样（推理）过程基于逆向求解ODE Eq. (6)，从LQ图像开始，仅需4步采样（uniform time schedule）。以下是算法步骤伪代码： text算法: ResFlow 采样过程 (图像恢复)</p> </li> </ol> <p>输入: 低质量图像 x1 (LQ), 训练好的 v_θ, 步数 N=4 (默认) 输出: 恢复的高质量图像 ˆx0 (HQ)</p> <ol> <li> <p>采样辅助变量: y1 ~ N(0, I) (标准高斯分布)</p> </li> <li> <p>初始化增强状态: z1 = [x1; y1]</p> </li> <li> <p>设置时间步: t 从1到0，分N=4步 (uniform schedule, e.g., Δt = 1/N)</p> </li> <li> <p>对于每个时间步 i 从1到N: a. 当前 t = 1 - (i-1)/N b. 预测速度: v = v_θ(z_t 的 x 组件, z_t 的 y 组件, t) c. 更新状态: z_{t-Δt} = z_t - v * Δt (Euler方法或更高阶如Heun求解ODE dz/dt = v(z, t)) d. (可选) 替换中间 ˆy_t 为 ground-truth y_t = α^y_t * 0 + σ^y_t * y1 (基于Eq.(5)，但概念上丢弃 ˆy_t)</p> </li> <li> <p>从最终 z0 提取 ˆx0 (丢弃 ˆy0)</p> </li> <li> <p>输出 ˆx0 作为恢复图像 (全分辨率测试) 注意：采样是确定性的（无随机噪声注入），通过辅助y消除不确定性。论文实验显示此过程在&lt;4步内完成，适用于实时应用。</p> </li> </ol> <h1 id="pnp-flow-plug-and-play-image-restoration-with-flow-matching">PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING</h1> <p>pnp 方法的洞见是 <em>the proximal</em> step on the regularization term is effectively a denoising operation. 因此近端算子可以用BM3D或神经网络。</p> <p>本文的出发点是，近来生成模型提供了智能的框架来从数据直接学习 priors，可以超越人工设计或神经网络去噪器。</p> <h2 id="图像恢复的数学问题">图像恢复的数学问题</h2> <p>在论文的引言部分，图像恢复（image restoration）问题被表述为从退化观测（degraded observation）\(y\) 恢复未知图像 \(x\) 的逆问题（inverse problem），其中 \(y = Hx + \xi\) 这里，\(H\) 是一个（线性）退化算子（degradation operator），\(\xi\) 表示加性噪声（additive noise）模型。由于该问题是病态的（ill-posed）和高维的，求解具有挑战性。 论文假设图像 \(x\) 来自具有密度 \(p_X\) 的随机变量 \(X\)，观测 \(y\) 来自具有密度 \(p_Y\) 的随机变量 \(Y\)。然后，使用最大后验（maximum a posteriori, MAP）估计器求解具有最高后验概率的值： \(\arg\max_{x \in \mathbb{R}^d} \left[ \log p_{X\mid Y=y}(x) \right] = \arg\max_{x \in \mathbb{R}^d} \left[ \log p_{Y\mid X=x}(y) + \log p_X(x) \right],\) 其中右侧第一项是数据保真（fidelity to the data），第二项是图像的先验分布（prior distribution）。 由于 \(p_X\) 通常未知，且缺乏训练数据，论文转而考虑一个正则化优化问题（regularized optimization problem）： \(\arg\min_{x \in \mathbb{R}^d} \left\{ F(x) + R(x) \right\},\) 其中 \(F(x) := -\log p_{Y\mid X=x}(y)\) 表示数据保真项（data-fidelity term），\(R : \mathbb{R}^d \to \mathbb{R}\) 通常强制对解的一些假设（enforces some assumptions on the solution），以确保（唯一）最小化器的存在。例如，对于高斯噪声 \(\mathcal{N}(0, \sigma^2 I_d)\)，数据保真项对应 \(F(x) = \frac{1}{2\sigma^2} \\mid Hx - y\\mid ^2.\) 该优化问题可以通过近端分裂方法（proximal splitting methods）有效求解。</p> <h2 id="pnp方法">PNP方法</h2> <p><img src="/images/2025-12-25-flow/image-20251014191145906.png" alt="image-20251014191145906" class="img-fluid"/></p> <h2 id="pnp-meets-flow-matching"><em>P<strong>N</strong>P</em> <em>MEETS</em> FLOW <em>MATCHING</em></h2> <p>PnP-Flow定义时间相关去噪器： \(D_t := \mathrm{Id} + (1 - t) v^\theta_t,\)</p> <blockquote> <p>这个去噪器的前提是直线路径，所以训练这个去噪器使用OT coupling 或 reflow. \(\pi\) 是耦合（optimal transport耦合可产生直线路径）。</p> </blockquote> <blockquote> <p>对于最优传输（OT）耦合，有：</p> </blockquote> \[v_t(f(t, x)) = T(x) - x, \tag{5}\] <blockquote> <p>其中 \(T\) 是Monge映射。</p> </blockquote> <p>在理想情况下： \(D_t(x) = \mathbb{E}[X_1 \mid X_t = x],\) 其中 \(X_t = (1-t)X_0 + t X_1\)。对于直线路径，去噪损失为0（Proposition 1）。</p> <p><img src="/images/2025-12-25-flow/image-20251014220345497.png" alt="image-20251014220345497" class="img-fluid"/></p> <p>注意：</p> <ol> <li>PnP flow 中的flow 是预训练的。上述算法没有训练，而是迭代的图像恢复过程。</li> <li>其中 \(t_n\) 从小到大。其中 \(\tilde{z}^n\) 这一步有使用插值，使用插值的原因在下一小节。\(t_n\) 越小，插值时，加的噪声越大。</li> </ol> <p><img src="/images/2025-12-25-flow/image-20251014230941949.png" alt="image-20251014230941949" class="img-fluid"/></p> <p>注意 \(t=0\) 时，\(\tilde{z}^n=\epsilon\)，从噪声直接去噪，再指向 \(y\)。</p> <h2 id="为什么要插值">为什么要插值</h2> <p>在PnP-Flow算法中，插入插值步（interpolation step）的目的是确保去噪器 \(D_t\) 能够有效工作，这与Flow Matching（FM）模型的特性以及算法的迭代过程密切相关。以下是详细的解释：</p> <h3 id="原因与背景">原因与背景</h3> <p>PnP-Flow结合了PnP框架与FM模型，其中去噪器 \(D_t\) 是基于预训练的FM速度场 \(v^\theta_t\) 定义的： \(D_t = \mathrm{Id} + (1 - t) v^\theta_t. \tag{6}\) 理想情况下，\(D_t(x)\) 将路径上的点 \(X_t = (1-t)X_0 + t X_1\) 投影回目标分布 \(P_1\) 的样本 \(X_1\)，其中 \(X_0 \sim P_0\)（潜在分布），\(X_1 \sim P_1\)（数据分布），\((X_0, X_1) \sim \pi\)（耦合）。然而，传统PnP-FBS算法的迭代点（通过梯度步更新后）不一定位于FM路径 \(X_t\) 上，而 \(D_t\) 的设计假设输入点在该路径上。如果输入点偏离 \(X_t\) 的支持集，去噪效果会显著下降。</p> <h3 id="插值步的必要性">插值步的必要性</h3> <p>论文在第5页（Section 3.2）中指出，经典PnP-FBS直接在梯度步后应用去噪器，但由于 \(D_t\) 针对 \(X_t\) 优化，若梯度步输出的 \(z\) 不位于 \(X_t\) 支持集，效果不佳。因此，引入插值步将 \(z\) “投影”回FM路径。</p> <h3 id="技术细节与动机">技术细节与动机</h3> <ul> <li><strong>路径一致性</strong>：FM模型（尤其是OT-FM）产生直线路径 \(X_t = (1-t)X_0 + t X_1\)。插值步确保迭代点与此路径对齐，从而利用 \(D_t\) 的最佳性能（Proposition 1表明直线路径下去噪损失为0）。</li> <li><strong>避免退化</strong>：若不插值，\(D_t\) 可能将 \(z^n\) 映射到不相关区域，特别是在 \(t\) 接近1时，\(D_t\) 趋于恒等变换（\(D_1 = \mathrm{Id}\)），导致算法退化为仅依赖数据保真项。</li> <li><strong>噪声引入</strong>：\(\epsilon \sim P_0\) 的随机性模拟FM的潜在分布采样，防止 \(D_t\) 简单地将 \(\tilde{z}^n\) 映射回 \(z^n\)（若 \(\epsilon\) 与 \(z^n\) 耦合，效果会抵消，详见论文Remark 2）。</li> </ul> <h2 id="讨论">讨论</h2> <p>pnp 方法，需要知道 \(H\)，这个比较难办?</p> <h1 id="posterior-mean-rectified-flow">POSTERIOR-MEAN RECTIFIED FLOW</h1> <p>论文《Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration》（PMRF）关注照片真实图像恢复（Photo-Realistic Image Restoration, PIR）问题，即从退化测量（如噪声、模糊图像）中重建视觉上吸引人的图像。该领域算法通常通过失真度量（如PSNR、SSIM、LPIPS）和感知质量度量（如FID、KID、NIQE）评估，目标是实现最低失真而不牺牲感知质量（即重建图像看起来自然）。 现有方法的局限性：</p> <p>后验采样（Posterior Sampling）：许多扩散或流模型（如DPS、DDS）尝试从后验分布 \(p_{X\mid Y}\) 采样，理论上可实现完美感知指数（即重建分布 \(p_{\hat{X}} = p_X\)，其中 \(p_X\) 是真实图像分布）。然而，根据Blau &amp; Michaeli (2018)，其MSE（均方误差）是无约束最小MSE（MMSE）的两倍，即 \(\mathbb{E}[\\mid X - \hat{X}\\mid ^2] = 2 \times \mathrm{MMSE}\)。 GAN+失真损失：优化失真（如MSE）和感知（如GAN）损失的加权和，可遍历失真-感知权衡曲线（Distortion-Perception Tradeoff），但GAN优化困难，尤其当感知损失权重较大时，实际性能不如后验采样。</p> <p>论文的核心动机：针对完美感知指数约束下最小化MSE的最优估计器 \(\hat{X}_0\)，定义为： \(\hat{X}_0 = \arg\min_{p_{\hat{X}\mid Y}} \mathbb{E}[\\mid X - \hat{X}\\mid ^2] \quad \mathrm{s.t.} \quad p_{\hat{X}} = p_X.\) Freirich et al. (2021)证明，\(\hat{X}_0\) 可通过先预测后验均值 \(\hat{X}^* = \mathbb{E}[X\mid Y]\)（MMSE估计），然后将其最优传输（Optimal Transport）到真实分布 \(p_X\) 来构建。该MSE通常严格小于后验采样的MSE（如图1所示）。 受此启发，PMRF提出一个简单高效算法：使用整流流（Rectified Flow）近似最优传输地图，将后验均值预测传输到高质量图像。论文通过理论分析（如Proposition 1）和实验证明，PMRF在去噪、超分辨率、补全、着色和盲面部恢复等任务中优于基线方法。 训练过程 PMRF训练分为两个阶段（见Algorithm 1），假设 \(X\) 是真实图像随机向量，\(Y\) 是退化测量。</p> <p>阶段1：后验均值预测 训练模型 \(f_\omega\) 最小化MSE损失，近似后验均值 \(\mathbb{E}[X\mid Y]\)： \(\omega^* = \arg\min_{\omega} \mathbb{E}\left[ \\mid X - f_\omega(Y)\\mid ^2 \right].\)</p> <p>此阶段可使用现成高PSNR模型跳过。 实际中，\(f_\omega\) 可为CNN或Transformer架构，优化目标是重建接近真实图像的平滑预测。</p> <p>阶段2：整流流模型训练 训练向量场 \(v_\theta\) （Rectified Flow模型），最小化流匹配损失： \(\theta^* = \arg\min_{\theta} \int_0^1 \mathbb{E}\left[ \\mid (X - Z_0) - v_\theta(Z_t, t)\\mid ^2 \right] \, dt,\) 其中：</p> <p>\(Z_t = t X + (1-t) Z_0\)，为直线路径前向过程。 \(Z_0 = f_{\omega^*}(Y) + \sigma_s \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)，\(\sigma_s\) 是小噪声超参数（缓解源/目标分布维数不匹配引起的奇异性）。 \(t \sim \mathcal{U}[0,1]\)。 此损失训练 \(v_\theta\) 预测从后验均值到真实图像的直线方向，近似最优传输地图。</p> <p>训练中，\(v_\theta\) 通常为U-Net架构，输入包括 \(Z_t\) 和时间 \(t\)（通过位置编码嵌入）。 推理过程 推理时，给定退化测量 \(y\)，PMRF解决ODE生成重建图像 \(\hat{x}\)： \(\frac{d \hat{Z}_t}{dt} = v_{\theta^*}(\hat{Z}_t, t), \quad \hat{Z}_0 = f_{\omega^*}(y) + \sigma_s \epsilon,\) 其中 \(\epsilon \sim \mathcal{N}(0, I)\)。 使用Euler方法离散求解（K步）：</p> <p>采样 \(\epsilon \sim \mathcal{N}(0, I)\)。 初始化 \(\hat{x} = f_{\omega^*}(y) + \sigma_s \epsilon\)。 对于 \(i = 0\) 到 \(K-1\)： \(\hat{x} \leftarrow \hat{x} + \frac{1}{K} v_{\theta^*}\left( \hat{x}, \frac{i}{K} \right).\)</p> <p>返回 \(\hat{x}\)。</p> <p>此过程从后验均值开始，通过整流流逐步“校正”到真实分布，生成低失真、高感知质量图像。论文实验显示，PMRF在CelebA-Test盲面部恢复基准上达到SOTA（如表1所示）。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Diffusion</title><link href="https://cekxm.github.io/blog/2025/diffusion/" rel="alternate" type="text/html" title="Diffusion"/><published>2025-12-25T13:42:24+00:00</published><updated>2025-12-25T13:42:24+00:00</updated><id>https://cekxm.github.io/blog/2025/diffusion</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/diffusion/"><![CDATA[<h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] 2022 Understanding Diffusion Models A Unified Perspective.pdf</p> <p>[2] What are Diffusion Models Lilian Weng.pdf</p> <p>[3] <a href="https://drscotthawley.github.io/blog/posts/FlowModels.html">blog - Flow With What You Know</a></p> <h3 id="toy-example">toy example</h3> <ul> <li><a href="https://github.com/varun-ml/diffusion-models-tutorial?tab=readme-ov-file">GitHub - varun-ml/diffusion-models-tutorial: Experiment with diffusion models that you can run on your local jupyter instances</a></li> <li>Classifier-Free-Guidance (CFG) https://github.com/dome272/Diffusion-Models-pytorch</li> <li>Classifier-Free-Guidance (CFG) forked from 上面的链接，增加了一些功能和一个博客 <ul> <li>https://github.com/tcapelle/Diffusion-Models-pytorch</li> <li>https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch–VmlldzoyNzIzNTQ1</li> </ul> </li> <li><a href="https://github.com/dome272/Diffusion-Models-pytorch">GitHub - dome272/Diffusion-Models-pytorch: Pytorch implementation of Diffusion Models (https://arxiv.org/pdf/2006.11239.pdf)</a></li> <li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch?tab=readme-ov-file">GitHub - lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch</a></li> </ul> <h2 id="variational-diffusion-models">Variational Diffusion Models</h2> <h3 id="markovian-hierarchical-variational-autoencoder">Markovian Hierarchical Variational Autoencoder</h3> <p>按照 [1] 的思路，首先介绍 Markovian Hierarchical Variational Autoencoder (MHVA)</p> <p><img src="/images/2025-12-25-diffusion/image-20250826145311374.png" alt="image-20250826145311374" class="img-fluid"/></p> <p>从左往右是编码，从右往左是解码。MHVA 规定，解码时，满足马尔可夫性，也就是生成 \(z_t\) 只依赖于 \(z_{t+1}\)。</p> <p>编码也是马尔可夫的，这个应该是默认的。因此有 \(p(x,z_{1:T})=p(z_T)p_\theta (x\mid z_1 )\prod_{t=2}^Tp_\theta (z_{t-1}\mid z_t) \tag{1}\)</p> \[q_\phi (z_{1:T}\mid x)=q_\phi(z_1\mid x)\prod_{t=2}^T q_\phi (z_{t}\mid z_{t-1}) \tag{2}\] <h3 id="variational-diffusion-models-可以认为是mhva再满足三个约束">Variational Diffusion Models 可以认为是MHVA再满足三个约束</h3> <ol> <li>隐藏变量和数据 x 维度一样</li> <li>编码器的结构不是学习得到的，而是设定为高斯</li> <li>设置编码器的高斯参数随时间而变化，使得T时刻 \(z_T\) 为标准高斯 \(N(0,I)\)。</li> </ol> <p><img src="/images/2025-12-25-diffusion/image-20250826151937355.png" alt="image-20250826151937355" class="img-fluid"/></p> <p>由于 z 和 x 维度一样，从这儿开始，如上图所示，统一用 x 表示。</p> <p>接下来，就是给出满足上述三个约束的 \(q(x_{t}\mid x_{t-1})\) 的参数了。然后推导得到约束3。这个推导参考文献[2]比较详细。这儿不再赘述。 \(q(x_{t}\mid x_{t-1})=N(x_t\mid \sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I )\) 在推导中，还有参数 \(\beta_t\), \(\bar{\alpha}_t\)。这几个参数可以相互转化。</p> <p>上式用<strong>参数化</strong>的技巧，就是 \(x_{t}=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} , \epsilon_{t-1}\sim N(0,I)\) 注意，参数 \(\alpha_t\) (or \(\beta_t\) or 其他变体) 的设置，可以按照预先规定的某种函数（scheduler），或者也可以学习得到。</p> <p>其次，经过推导，还有一个重要的性质 \(x_{t}=\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon , \epsilon\sim N(0,I)\) 也就是说，按编码的流程要采样得到 \(x_{t}\)，可以根据上式一步采样，不需要先采样 \(x_{1},x_{2}...x_{t-1}\)。这一点在训练中是有用的。</p> <h3 id="似然最大化elbo">似然最大化，ELBO</h3> <p>接下来，就是要通过最大似然来学习解码器 \(p_\theta (x_{t-1}\mid x_t)\) 了。具体来说，有满足某种分布的\(x\)数据集（这个 \(x\) 就是上面的 \(x_0\)），要最大化似然 \(\log p(x)=\log \int p(x_{0:T})dx_{1:T}\)</p> <blockquote> <p>这边有个疑惑，就是在后续推导中，有用到 \(q(x_{1:T}\mid x_0)\)。如果先进行编码，再进行解码，岂不是有两个不同的 \(x_1,x_2,...x_{T-1}\)。正确理解应该是，\(x_{1:T}\) 是被 marginalized out 的，所以考虑了所有可能，但每一种可能下，都只有一个值，不会有两个值。不管是p还是q，都是概率模型，所以任意值下都是可计算概率的。可以认为，先进行编码，得到了\(x_{1:T}\)，然后计算在当前模型参数下\(p_\theta (x_{t-1}\mid x_t)\)的似然，及其梯度。</p> </blockquote> <p>\(p_\theta (x_{t-1}\mid x_t)\)可以设为任意某种分布，只要知道\(z_{t-1},z_t\)（由编码生成）就可以计算概率\(p_\theta (x_{t-1}\mid x_t)\)。但是根据大段的推导，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</p> <p>推导参见参考文献。</p> <p>\(\log p(x)=\log \int p(x_{0:T})dx_{1:T}&gt;ELBO\)，ELBO由三项组成</p> <p><img src="/images/2025-12-25-diffusion/image-20250826210812707.png" alt="image-20250826210812707" class="img-fluid"/></p> <p>其中，</p> <ol> <li>第一项类似于VAE中的ELBO，可以通过蒙特卡洛估计来近似和优化。但在我看的代码中，这一项并没有使用。</li> <li>第二项没有需训练的参数，也是等于0.</li> <li>第三项可以推导出，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</li> </ol> <p>关于第一项，在代码实现中没有被使用，AI是这样回答的。</p> <blockquote> <p><img src="/images/2025-12-25-diffusion/image-20250826211846887.png" alt="image-20250826211846887" class="img-fluid"/></p> </blockquote> <p>现在进一步分析上述第三项。可以推导得到，\(q(x_{t-1}\mid x_t,x_0)\) 是一个高斯分布。因此 \(p_\theta (x_{t-1}\mid x_t)\) 也应该是高斯分布，方差和\(q(x_{t-1}\mid x_t,x_0)\)的方差相同（因为方差和\(x_0\)无关，所以可以直接设为相等），均值要尽量接近<img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826222640680.png" alt="image-20250826222640680" style="zoom:50%;"/></p> <p>可以看出 \(\mu_q\) 和 \(x_0\) 有关，而\(p_\theta (x_{t-1}\mid x_t)\) 并没有\(x_0\)的信息，所以可以训练一个神经网络 \(\hat{x}_\theta (x_t,t)\)，从\(x_t\)来预测 \(x_0\)</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223028627.png" alt="image-20250826223028627" style="zoom:50%;"/></p> <p>由于两个高斯的KL距离是可以直接计算，再经过推导，最大化ELBO就等价于，在每个timestep，神经网络的输出 \(\hat{x}_\theta (x_t,t)\)要和\(x_0\)越符合，越好。即最优解为</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223326616.png" alt="image-20250826223326616" style="zoom: 50%;"/></p> <p>上式中的系数，也可以推导等于</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223502799.png" alt="image-20250826223502799" style="zoom:50%;"/></p> <h3 id="训练过程">训练过程</h3> <p>训练采用sgd，对于每一个\(x_0\)，只随机取一个\(t\)，采样得到 \(x_t\)（不用迭代，直接采样），然后计算上式，上式即为 loss。</p> <h3 id="推理过程">推理过程</h3> <p>推理过程则需要执行完整的解码过程。从一个随机向量 \(x_T\)出发，通过神经网络，计算得到 \(\hat{x}_\theta (x_t,t)\)，然后由上面 ，计算得到均值 \(\mu_\theta(x_t,t)\)；计算后验方差（\(p_\theta\) 的方差和后验概率 \(q(x_{t-1}\mid x_t,x_0)\) 的方差相等）；采样得到 \(x_{T-1}\)，如此迭代，最后得到 \(x_0\)。</p> <h3 id="简化的loss">简化的Loss</h3> <p>见[2] 中的一些补充</p> <p>Empirically, Ho et al. (2020) 提出不带上式中系数的Loss</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250827094521508.png" alt="image-20250827094521508" style="zoom:50%;"/></p> <p>其中，\(\epsilon_t\) 是编码过程采样得到 \(x_t\) 的标准高斯噪声，\(\epsilon_\theta\) 是学习的神经网络。大概理解是这样：目标是要让解码的 \(x_{t-1}\) 分布均值接近编码的 \(x_{t-1}\) 分布均值，编码的 \(x_{t-1}\) 分布均值和 \(x_{t}\) 之间有关系，除了乘性系数外，差值为 \(\epsilon_t\) 。解码的 \(x_{t-1}\) 分布均值经过推导式子和编码时大概一样，区别是需要预测一个\(\epsilon_\theta\)。见下图 Algorithm2 Sampling step 4.</p> <p><img src="/images/2025-12-25-diffusion/image-20250827095354468.png" alt="image-20250827095354468" class="img-fluid"/></p> <h4 id="训练和推理">训练和推理</h4> <p><img src="/images/2025-12-25-diffusion/image-20250827143550828.png" alt="image-20250827143550828" class="img-fluid"/></p> <h3 id="学习扩散噪声参数">学习扩散噪声参数</h3> <p>上述，扩散噪声参数是按照scheduler确定的，在实现中，也可以联合学习得到。</p> <h2 id="noise-conditioned-score-networks-ncsn">noise-conditioned score networks (NCSN)</h2> <p>注意：本小节引用了AI的回答，其中 \(p\) 泛指概率，而不是特制反向采样中的概率分布。</p> <h3 id="分数-s_thetax_t-t">分数 \(s_\theta(x_t, t)\)</h3> <p>所谓“分数”，指的是概率密度函数的对数关于数据的梯度，即： \(s(x) = \nabla_x \log p(x)\) 其中 \(p(x)\) 是数据的概率密度函数，分数 \(s(x)\) 描述了数据在概率密度上的局部变化方向和幅度。通过训练一个模型 \(s_\theta(x)\) 来逼近真实的分数 \(\nabla_x \log p(x)\)，可以间接学习数据的概率分布，而无需直接估计 \(p(x)\) 本身。</p> <p>在扩散模型中，分数可以有两种相关定义，具体取决于上下文：</p> <ul> <li><strong>边缘分布的分数</strong>：\(\nabla_{x_t} \log p(x_t)\)，其中 \(p(x_t)\) 是时间步 \(t\) 的边缘分布，即： \(p(x_t) = \int p(x_t \mid x_0) p_{\text{data}}(x_0) \, dx_0\) 这个分数描述了 \(x_t\) 在整个数据分布上的概率密度梯度。然而，\(p(x_t)\) 通常难以直接计算，因为它涉及对所有可能的 \(x_0\) 积分。</li> <li><strong>条件分布的分数</strong>：\(\nabla_{x_t} \log p(x_t \mid x_0)\)，其中 \(p(x_t \mid x_0)\) 是前向过程中给定原始数据 \(x_0\) 的条件分布。根据扩散模型的前向过程： \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\) 这个分数的计算更直接，因为它是高斯分布的梯度： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\)</li> </ul> <p>在实际的扩散模型训练中（如 DDPM），通常使用 <strong>去噪分数匹配（Denoising Score Matching, DSM）</strong>，训练分数模型 \(s_\theta(x_t, t)\) 来逼近 <strong>条件分数</strong> \(\nabla_{x_t} \log p(x_t \mid x_0)\)，而不是边缘分数 \(\nabla_{x_t} \log p(x_t)\)。这是因为条件分数的表达式已知且易于计算，而边缘分数需要复杂的积分。</p> <h3 id="分数如何用在采样中">分数如何用在采样中</h3> <p>大体思路如下：根据扩散模型的推导，在反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计（下面第2点），而\(x_0\) 的估计和分数\(s_\theta(x_t, t)\)有关系（下面第1点）。</p> <ol> <li><strong>分数的可计算性</strong>：</li> </ol> <p>前向过程定义了 \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)，其分数可以通过高斯分布的性质直接计算： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 这个分数是已知的，因此可以作为训练目标。</p> <p>训练好的分数模型 \(s_\theta(x_t, t)\) 可以用来估计 \(x_0\)（原始数据）的值： \(s_\theta(x_t, t) \approx -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 重排后： \(x_0 \approx \frac{x_t + (1 - \bar{\alpha}_t) s_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\)</p> <ol> <li><strong>与反向过程的联系</strong>：</li> </ol> <p>反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计。分数模型 \(s_\theta(x_t, t)\) 提供了从 \(x_t\) 估计 \(x_0\) 的方法。</p> <p>均值可以通过高斯条件分布公式得到： \(\mu_q(x_t, x_0) = \sqrt{\alpha_t} x_{t-1} + \frac{(1 - \alpha_t) \sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_t}\) 将 \(s_\theta(x_t, t)\) 代入反向过程的均值表达式，DDPM 推导出简化的均值形式： \(\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} s_\theta(x_t, t) \right)\) 这个表达式直接用 \(x_t\) 和 \(s_\theta(x_t, t)\) 表示均值。</p> <p>以上公示由AI生成，用作思路理解。</p> <h3 id="和上面简化的loss中-epsilon-的关系">和上面简化的Loss中 \(\epsilon\) 的关系</h3> <p>\(\epsilon\) 是 \(x_t\) 和它分布均值之间的差。即 \(\epsilon =x_t - \sqrt{\bar{\alpha}_t} x_0\) \(\epsilon_\theta(x_t,t)\) 是它的估计。</p> <p>因此有 \(s_\theta(x_t, t) \approx -\frac{\epsilon_\theta(x_t,t)}{1 - \bar{\alpha}_t}\)</p> <h2 id="classifier-guided-diffusion">Classifier Guided Diffusion</h2> <p>分类器引导扩散中，有一个条件 \(y\)，使用了条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\) 来调整无条件扩散模型的采样过程，符合分数模型的核心思想。</p> <p><strong>分类器 \(p(y \mid x_t)\) 和无条件扩散模型 \(s_\theta(x_t, t)\) 都是预训练的</strong>，然后通过调整反向过程的均值。分类器 \(p_\phi(y \mid x_t, t)\) 是预训练的，在带噪声的图像 \(x_t \sim q(x_t \mid x_0)\) 上训练，以预测类别 \(y\)。这确保分类器梯度在不同噪声水平下可靠，为条件分数提供准确的引导。</p> <p>具体来说，论文通过以下方式从分数模型角度解释分类器引导扩散：</p> <ul> <li> <p>它将无条件分数 \(\nabla_{x_t} \log p(x_t)\)（由<strong>预训练</strong>的扩散模型提供）与分类器的梯度 \(\nabla_{x_t} \log p(y \mid x_t)\) 结合，构造条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。这一步通过贝叶斯准则可以推导。 \(\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t)\)</p> </li> <li> <p>反向过程的均值通过条件分数调整，确保采样生成符合特定类别 \(y\) 的样本。</p> </li> </ul> <p>替换为条件分数后，得到：</p> <p>\(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \nabla_{x_t} \log p(x_t \mid y) \right)\) \(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \left( s_\theta(x_t, t) + s \cdot \nabla_{x_t} \log p_\phi(y \mid x_t) \right) \right)\) 这里的 \(s_\theta(x_t, t) \approx \nabla_{x_t} \log p(x_t)\)，而 \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) 是分类器梯度，整体形成条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。</p> <blockquote> <p>“To perform conditional sampling, we train a classifier \(p_\phi(y \mid x_t)\) on noisy images to predict the class label \(y\), and use its gradient \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) to guide the diffusion process. Specifically, we modify the reverse process mean as follows: \(\tilde{\mu}_t(x_t, y) = \mu_t(x_t) + s \cdot \sigma_t^2 \cdot \nabla_{x_t} \log p_\phi(y \mid x_t)\) where \(\mu_t(x_t)\) is the original mean of the reverse process, \(\sigma_t^2\) is the variance, and \(s\) is a scale factor controlling the strength of the classifier guidance.”</p> </blockquote> <p>这边分类器梯度前面有正负不一致。先不管。</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p><strong>论文中的符号和其他文献不同，\(\lambda\) 是时间 \(t\)，但是采样时 \(\lambda\) 从小到大。\(z_\lambda\) 对应 \(x_t\)。</strong></p> <p>CFG 的核心是联合训练一个支持条件和无条件的扩散模型，然后在采样时通过线性组合分数函数来实现引导。论文将扩散模型表述在连续时间框架下，\(\lambda\) 被定义为前向过程的噪声水平，\(\lambda=\log \alpha_\lambda^2/\sigma_\lambda^2\) 即 log SNR。其中前向过程为 \(q(z_\lambda \mid x) = \mathcal{N}(\alpha_\lambda x, \sigma_\lambda^2 I)\)，\(\alpha_\lambda^2 = 1 / (1 + e^{-\lambda})\)，\(\sigma_\lambda^2 = 1 - \alpha_\lambda^2\)。</p> <h3 id="训练过程algorithm-1-in-the-paper">训练过程（Algorithm 1 in the paper）：</h3> <ol> <li> <p>使用单个神经网络参数化分数估计器 \(\epsilon_\theta(z_\lambda, c)\)（条件模型）和 \(\epsilon_\theta(z_\lambda)\)（无条件模型）。</p> </li> <li> <p>在训练时，以概率 \(p_{\text{uncond}}\)（超参数，通常为 0.1-0.2）随机将条件信息 \(c\) 设置为无条件标识符 \(\emptyset\)，从而联合训练两个模型。 训练目标是去噪分数匹配（Denoising Score Matching）：\(\mathbb{E}_{\epsilon, \lambda} [ \\mid \epsilon_\theta(z_\lambda) - \epsilon \\mid _2^2 ]\)，其中 \(z_\lambda = \alpha_\lambda x + \sigma_\lambda \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)。</p> </li> <li> <p>论文描述：“We jointly train the unconditional and conditional models simply by randomly setting \(c\) to the unconditional class identifier \(\emptyset\) with some probability \(p_{\text{uncond}}\), set as a hyperparameter.”</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110741671.png" alt="image-20250930110741671" class="img-fluid"/></p> </li> </ol> <h3 id="采样过程algorithm-2-in-the-paper">采样过程（Algorithm 2 in the paper）：</h3> <ol> <li>从纯噪声 \(z_{\lambda_T} \sim \mathcal{N}(0, I)\) 开始，逐步去噪。</li> <li>在每个时间步 \(t\)，计算引导分数：\(\tilde{\epsilon}_\theta(z_\lambda, c) = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\)，其中 \(w\) 是引导强度（guidance strength，通常 \(w &gt; 0\)）。</li> <li>使用这个引导分数来更新样本，例如在 DDPM 采样中替换原分数函数。 论文解释：“Form the classifier-free guided score at log SNR \(\lambda_t\): \(\tilde{\epsilon}_t = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\).”</li> </ol> <p>直观上，\(w = 0\) 时退化为标准条件采样；\(w &gt; 0\) 时，增强条件分数并减弱无条件分数，从而提高样本质量（更符合条件 \(c\)），但降低多样性。</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110925539.png" alt="image-20250930110925539" class="img-fluid"/></p> <p>该算法中，step 3的解释如下，见[2]</p> <p><img src="/images/2025-12-25-diffusion/image-20250930111119169.png" alt="image-20250930111119169" class="img-fluid"/></p> <p>step 4 中，\(\tilde{x}_t\) 是当前时间步的去噪数据估计，基于引导分数 \(\tilde{\epsilon}_t\)。它表示在条件 \(c\) 引导下，模型预测的原始数据 \(x\)。</p> <p>step 5中，计算均值 \(\tilde{\mu}_t\): \(\tilde{\mu}_t = \alpha_{\lambda_{t+1}} \tilde{x}_t + \sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\)</p> <p>意义：\(\tilde{\mu}_t\) 是条件反向分布 \(p(z_{\lambda_{t+1}} \mid z_{\lambda_t}, c) = \mathcal{N}(z_{\lambda_{t+1}}; \tilde{\mu}_t, \sigma_{\lambda_{t+1}}^2 I)\) 的均值，引导采样朝符合条件 \(c\) 的方向移动。 作为 \(z_t\) 和 \(\tilde{x}_t\) 的函数：公式明确显示 \(\tilde{\mu}_t\) 依赖于当前样本 \(z_t\) 和引导预测器 \(\tilde{x}_t\)。其中：</p> <p>\(\alpha_{\lambda_{t+1}} \tilde{x}_t\): 缩放后的去噪估计，代表信号部分。 \(\sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\): 噪声调整项，通过 \(\tilde{x}_t\) 和 \(z_t\) 的差值引入引导分数的影响。</p> <h3 id="简化采样">简化采样</h3> <p>实际编程中，没有按algorithm2这么复杂。还是按 DDPM 简化Loss的采样过程，不过把 \(\epsilon\) 替换成引导的 \(\tilde{\epsilon}\) 。</p> <h2 id="ddim">DDIM</h2> <p>应该可以肯定的是，DDIM和DDPM的训练过程是一样的，采样过程不一样。</p> <p>但在理论上，forward process（本论文称为 inference）不一样。DDIM引入了非马尔可夫forward process。</p> <h3 id="non-markovian-forward-processes">NON-MARKOVIAN FORWARD PROCESSES</h3> <p>前向过程是一个随机过程，用联合概率密度分布表示。由于在DDPM中，训练过程是在每一个 \(t\)，让神经网络根据 \(x_t\) 去预测分数\(\nabla_{x_t} \log p(x_t \mid x_0)\) （等价于预测 \(x_0\)，或 \(\epsilon(x_t,t)\)）。这个训练过程只和 \(q(x_t\mid x_0)\) 有关，和联合概率密度分布没有关系，实际上有无数联合概率密度分布，可以获得这个边际分布。DDPM定义了其中的具有马尔可夫性的一种。</p> <p>DDIM论文直接设计另一种联合概率分布。给定一个实数向量 \(\sigma \in R^T_{\ge 0}\)，定义联合概率密度分布：</p> <p><img src="/images/2025-12-25-diffusion/image-20251003144728914.png" alt="image-20251003144728914" class="img-fluid"/></p> <p>论文证明，上面的这个联合概率分布，满足 \(q_\sigma(x_t\mid x_0)=N(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) 也就是和DDPM中相同。所以训练过程和 DDPM 一样。</p> <p>这个联合概率分布的设计中，直接给出了后向概率分布 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)，这样在采样时，就可以直接按照这个概率分布进行采样。</p> <blockquote> <p>The magnitude of \(\sigma\) controls the how stochastic the forward process is; when \(\sigma=0\), we reach an extreme case where as long as we observe x0 and xt for some t, then \(x_{t-1}\) become known and fixed.</p> </blockquote> <p>在上面的讨论中，并没有给出潜在的前向过程，也即是如何从 \(x_0\) 逐步推理得到 \(x_T\)。论文说这个过程可以由贝叶斯公式推导得到 \(q_\sigma(x_{t}\mid x_{t-1},x_0)\)，它是一个高斯分布。由于有依赖 \(x_0\)，它不是一个马尔可夫过程。熟悉DDPM的话，可以知道，在训练时，实际上并不需要去执行这个前向过程。</p> <h3 id="sampling-from-generalized-generative-processes">SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</h3> <p>前面说过，生成过程可以直接借助 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)</p> <blockquote> <p>Intuitively, given a noisy observation \(x_t\), we first make a prediction of the corresponding \(x_0\), and then use it to obtain a sample \(x_{t-1}\) through the reverse conditional distribution \(q_\sigma(x_{t-1}\mid x_t,x_0)\), which we have defined.</p> </blockquote> <p>如果我们采用simple loss预测 \(\epsilon\)，则</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153043856.png" alt="image-20251003153043856" class="img-fluid"/></p> <p>上文中，\(f_\theta^t\) 就是对 \(x_0\) 的预测。把式（9）中\(f_\theta^t\)代入式(10)，可得</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153420286.png" alt="image-20251003153420286" class="img-fluid"/></p> <p>上式就是采样过程。其中\(\epsilon_\theta^{(t)}\) 是神经网络的输出。某一个特定的 \(\alpha\)，则退回成DDPM。</p> <p>而 \(\alpha_t=0\)，即为 DDIM。</p> <h3 id="加速采样过程">加速采样过程</h3> <p>上述讨论已经定义了DDIM，但还没涉及到加速采样。加速采样，也有类似的理论，但是训练过程不变，因此也要满足 marginal 要和 DDPM 一样。</p> <p>假设我们已经选择了时间的子集 \(\tau\)，\(\tau_S=T\)；定义联合概率密度</p> <p><img src="/images/2025-12-25-diffusion/image-20251003154717877.png" alt="image-20251003154717877" class="img-fluid"/></p> <p>也就是说， \(\tau\) 内的时间满足上面的非马尔可夫inference过程，其他时间是一个星形的概率图结构。由于marginal 要和 DDPM 一样，所以训练不变。</p> <p>在生成时，只用到了 \(\tau\) 内的时间，其他时间就不管。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Gaussiansplatting</title><link href="https://cekxm.github.io/blog/2025/gaussianSplatting/" rel="alternate" type="text/html" title="Gaussiansplatting"/><published>2025-12-25T12:57:46+00:00</published><updated>2025-12-25T12:57:46+00:00</updated><id>https://cekxm.github.io/blog/2025/gaussianSplatting</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/gaussianSplatting/"><![CDATA[<p>参考资料：</p> <p>[1] https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362/</p> <p>[2] grok</p> <p>代码：</p> <p>[1] <a href="https://github.com/yzslab/gaussian-splatting-lightning">GitHub - yzslab/gaussian-splatting-lightning: A 3D Gaussian Splatting framework with various derived algorithms and an interactive web viewer</a></p> <p>[2] <a href="https://github.com/nerfstudio-project/gsplat">GitHub - nerfstudio-project/gsplat: CUDA accelerated rasterization of gaussian splatting</a>: gsplat is an open-source library for CUDA accelerated rasterization of gaussians with python bindings.</p> <h1 id="3d-gaussian-splatting">3D Gaussian Splatting</h1> <p>本文是grok 关于gaussian splatting 反向传播的回答，但是第一部分描述了前向过程。即</p> <p>从高斯参数 → 投影 → alpha 混合 → 渲染图像</p> <p>3D Gaussian Splatting (3DGS) 的反向传播（back propagation, BP）是其训练核心，用于优化高斯点云的参数（如位置 \(\mu\), 协方差 \(\Sigma\), 不透明度 \(\alpha\), 球谐系数 SH）。与 NeRF 的 MLP 优化不同，3DGS 使用<strong>显式参数</strong>（点云属性），通过可微分渲染（differentiable rendering）计算梯度，结合 PyTorch 的 autograd 实现 BP。以下我详细解释 3DGS 的反向传播机制，结合 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库的代码（基于你的使用场景），从数学到实现，尽量图文并茂（文字描述模拟图形），并回答如何在代码中观察 BP。</p> <hr/> <h4 id="1-反向传播的总体流程">1. <strong>反向传播的总体流程</strong></h4> <p>3DGS 的训练目标是通过多视图图像（e.g., lego 数据集的 PNG + transforms.json）监督，优化高斯参数，最小化渲染图像与真实图像的损失（如 L1 + SSIM）。BP 过程涉及：</p> <ol> <li><strong>前向传播</strong>：从高斯参数 → 投影 → alpha 混合 → 渲染图像。</li> <li><strong>损失计算</strong>：渲染图像与 GT（ground truth）图像比较。</li> <li><strong>反向传播</strong>：通过 autograd 计算梯度，更新高斯参数。</li> <li><strong>密度控制</strong>：非 BP 部分，定期调整点云（克隆/分裂/剔除）。</li> </ol> <p><strong>图示（文字模拟）</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯参数: μ, Σ, α, SH] → [投影: 2D means/scales] → [alpha 混合: RGB] → [Loss: L1+SSIM]
    ↑ (梯度流回)            ↑ (梯度流回)             ↑ (梯度流回)         ← [GT 图像]
</code></pre></div></div> <hr/> <h4 id="2-数学原理可微分渲染与梯度计算">2. <strong>数学原理：可微分渲染与梯度计算</strong></h4> <p>3DGS 的渲染管道是可微分的，梯度通过链式法则从损失函数流回高斯参数。以下拆解关键步骤：</p> <h5 id="21-前向传播渲染"><strong>2.1 前向传播（渲染）</strong></h5> <ul> <li><strong>输入</strong>：N 个高斯，参数为： <ul> <li>\(\mu_i \in \mathbb{R}^3\)：中心位置。</li> <li>\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)：协方差矩阵（分解为缩放 \(S_i\) 和旋转 \(R_i\)）。</li> <li>\(\alpha_i \in [0,1]\)：不透明度（sigmoid 输出）。</li> <li>SH 系数：视图相关颜色 \(\mathbf{c}_i(\mathbf{d})\)（球谐函数）。</li> </ul> </li> <li><strong>投影（Splatting）</strong>： <ul> <li>每个高斯投影到 2D 图像平面： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(P\): 相机投影矩阵（内参）。</li> <li>\(V\): 视图矩阵（外参）。</li> <li>\(J\): 投影的雅可比矩阵（透视效应）。</li> <li>\(W\): 视图旋转部分。</li> </ul> </li> <li>2D 高斯权重：\(w_i(u,v) = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D}))\)。</li> </ul> </li> <li><strong>Alpha 混合</strong>： <ul> <li>像素颜色： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(T_i\): 透射率，模拟光线衰减。</li> <li>按深度排序（front-to-back）。</li> </ul> </li> </ul> </li> </ul> <h5 id="22-损失函数"><strong>2.2 损失函数</strong></h5> <ul> <li>比较渲染图像 \(C(\mathbf{p})\) 与 GT 图像 \(\hat{C}(\mathbf{p})\)： \(\mathcal{L} = \lambda_1 \cdot \|\hat{C} - C\|_1 + \lambda_2 \cdot (1 - \text{SSIM}(\hat{C}, C))\) <ul> <li>典型：\(\lambda_1 = 1, \lambda_2 = 0.2\)（见 <code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）。</li> </ul> </li> <li>损失是标量，驱动梯度计算。</li> </ul> <h2 id="投影和alpha混合的可微性">投影和alpha混合的可微性</h2> <hr/> <p>是的，在 <strong>3D Gaussian Splatting (3DGS)</strong> 中，<strong>投影（Projection/Splatting）</strong> 和 <strong>Alpha 混合（Alpha Blending）</strong> 两个步骤都是<strong>可微分的</strong>（differentiable）。这是 3DGS 能够通过反向传播（back propagation）优化高斯参数（如位置 \(\mu\)、协方差 \(\Sigma\)、不透明度 \(\alpha\)、球谐系数 SH）的关键原因。以下详细解释两者的可微性，结合数学推导、代码实现（基于 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库），并以图文并茂的方式说明（文字模拟图形），回答你的问题。</p> <hr/> <h3 id="1-投影projectionsplatting的可微性">1. <strong>投影（Projection/Splatting）的可微性</strong></h3> <p><strong>投影</strong>步骤将 3D 高斯点云投影到 2D 图像平面，生成每个高斯的 2D 均值 \(\mu_{2D}\)、协方差 \(\Sigma_{2D}\) 和权重 \(w_i\)，用于后续渲染。这个过程是可微的，因为所有变换（矩阵运算、指数函数等）都支持梯度计算。</p> <h4 id="11-数学原理"><strong>1.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>3D 高斯参数：\(\mu_i \in \mathbb{R}^3\)（中心位置）、\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)（协方差，分解为旋转 \(R_i\) 和缩放 \(S_i\)）、\(\alpha_i \in [0,1]\)（不透明度）。</li> <li>相机参数：视图矩阵 \(V\)（外参，4x4）、投影矩阵 \(P\)（内参，透视投影）。</li> </ul> </li> <li><strong>投影公式</strong>： <ol> <li><strong>均值投影</strong>： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) <ul> <li>\(V \cdot \mu_i\)：将 3D 位置从世界坐标系变换到相机坐标系。</li> <li>\(P\)：透视投影（e.g., \(P = \begin{bmatrix} f_x/Z &amp; 0 &amp; c_x \\ 0 &amp; f_y/Z &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)），输出 2D 像素坐标。</li> <li><strong>可微性</strong>：矩阵乘法是线性操作，透视除法 (\(/Z\)) 是可微的（雅可比矩阵 \(J\) 提供导数）。</li> </ul> </li> <li><strong>协方差投影</strong>： \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(W\)：视图矩阵 \(V\) 的 3x3 旋转部分。</li> <li>\(J\)：投影雅可比矩阵，考虑透视畸变： \(J = \begin{bmatrix} \frac{f_x}{Z} &amp; 0 &amp; -\frac{f_x X}{Z^2} \\ 0 &amp; \frac{f_y}{Z} &amp; -\frac{f_y Y}{Z^2} \end{bmatrix}\)</li> <li>\(\Sigma_i = R_i S_i S_i^T R_i^T\)：3D 协方差分解。</li> <li><strong>可微性</strong>：矩阵乘法、转置和 \(J\) 的计算（涉及除法）都可微，梯度通过链式法则流回 \(\Sigma_i\)（即 \(S_i, R_i\)）。</li> </ul> </li> <li><strong>高斯权重</strong>： \(w_i(u,v) = \alpha_i \cdot \exp\left(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\right)\) <ul> <li>\(\alpha_i\)：sigmoid 输出，可微。</li> <li>指数函数和矩阵逆（\(\Sigma_{i,2D}^{-1}\)）可微（指数导数为自身，矩阵逆导数基于线性代数）。</li> </ul> </li> </ol> </li> <li><strong>梯度流</strong>： <ul> <li>损失 \(\mathcal{L}\) 对像素颜色 \(C(\mathbf{p})\) 的梯度 \(\frac{\partial \mathcal{L}}{\partial C}\) 流回 \(w_i\)，再通过 \(w_i\) 流回 \(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i\)，最终到 \(\mu_i, \Sigma_i, \alpha_i\)。</li> </ul> </li> </ul> <h4 id="12-代码实现yzslab"><strong>1.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means3D</span> <span class="o">=</span> <span class="n">means</span>  <span class="c1"># [N, 3]
</span>    <span class="n">view_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">view</span>  <span class="c1"># [4, 4]
</span>    <span class="n">proj_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">proj</span>  <span class="c1"># [3, 4]
</span>    <span class="n">means2D</span> <span class="o">=</span> <span class="n">proj_matrix</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span> <span class="o">@</span> <span class="n">means3D</span><span class="p">)</span>  <span class="c1"># 矩阵乘法
</span>    <span class="n">J</span> <span class="o">=</span> <span class="nf">compute_jacobian</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">.</span><span class="n">focal</span><span class="p">)</span>  <span class="c1"># 雅可比
</span>    <span class="n">cov3D</span> <span class="o">=</span> <span class="nf">scales_to_cov</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">)</span>  <span class="c1"># [N, 3, 3]
</span>    <span class="n">cov2D</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">cov3D</span> <span class="o">@</span> <span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">].</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">J</span><span class="p">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">torch.matmul</code>（矩阵乘法）可微。</li> <li><code class="language-plaintext highlighter-rouge">compute_jacobian</code> 计算 \(J\)（涉及除法 <code class="language-plaintext highlighter-rouge">/Z</code>），PyTorch autograd 自动处理。</li> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 CUDA 实现的，可微分内核，基于 C++ 和 PyBind11（见 gsplat 源码）。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">viewer.py</code> 或 <code class="language-plaintext highlighter-rouge">render.py</code> 加断点，打印 <code class="language-plaintext highlighter-rouge">means2D.grad</code> 检查梯度。</li> </ul> <h4 id="13-图示文字模拟"><strong>1.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3D 高斯 [μ, Σ, α] → [矩阵乘法: V, P] → [2D 均值 μ_2D] → [雅可比 J 变换] → [2D 协方差 Σ_2D]
                                                    ↓
                                                  [高斯权重 w_i: exp(-x^T Σ_2D^-1 x)]
梯度流: Loss ← w_i ← μ_2D, Σ_2D ← μ, Σ, α
</code></pre></div></div> <hr/> <h3 id="2-alpha-混合alpha-blending的可微性">2. <strong>Alpha 混合（Alpha Blending）的可微性</strong></h3> <p><strong>Alpha 混合</strong>将投影后的 2D 高斯按深度排序，累积颜色形成像素值。这个过程也是可微的，因为涉及的排序、乘法和累积操作支持梯度计算。</p> <h4 id="21-数学原理"><strong>2.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>每个高斯的 2D 参数：\(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i, \mathbf{c}_i\)（颜色，SH 插值）。</li> <li>像素坐标 \(\mathbf{p} = (u,v)\)。</li> </ul> </li> <li><strong>混合公式</strong>： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(w_i = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\)：高斯权重。</li> <li>\(T_i\)：透射率，模拟光线穿过前 i-1 个高斯的衰减。</li> <li>按深度 \(z_i\)（相机坐标 Z）排序（front-to-back）。</li> </ul> </li> <li><strong>可微性</strong>： <ul> <li><strong>颜色</strong>：\(\frac{\partial C}{\partial \mathbf{c}_i} = w_i \cdot T_i\)，线性操作，可微。</li> <li><strong>权重</strong>：\(w_i\) 依赖 \(\alpha_i, \mu_{i,2D}, \Sigma_{i,2D}\)，已证明可微（见投影）。</li> <li><strong>透射率</strong>：\(T_i = \prod_{j=1}^{i-1} (1 - w_j)\)，乘法链可微，导数： \(\frac{\partial T_i}{\partial w_k} = -T_i \cdot \frac{1}{1 - w_k} \quad (k &lt; i)\)</li> <li><strong>排序</strong>：深度排序在渲染中是离散操作（不可微），但 3DGS 用 <strong>tile-based rasterization</strong>（分块光栅化），梯度只流经数值计算（不涉及排序本身），通过 CUDA 实现（如 gsplat 的 radix sort）。</li> </ul> </li> </ul> <h4 id="22-代码实现yzslab"><strong>2.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span>  <span class="c1"># 排序+混合
</span>    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 C++/CUDA 实现，内部计算 alpha 混合的梯度。</li> <li>排序在 tile 内（16x16 像素块），梯度流经 \(w_i, T_i\)，不依赖排序顺序。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">training_step()</code> 打印 <code class="language-plaintext highlighter-rouge">rgb.grad</code>： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rendered</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">renderer</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">camera</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">].</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 像素梯度
</span></code></pre></div> </div> </li> </ul> <h4 id="23-图示文字模拟"><strong>2.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯: c_i, w_i] → [排序: z_i] → [T_i = Π(1-w_j)] → [C = Σ(c_i * w_i * T_i)] → Loss
梯度流: Loss ← C ← c_i, w_i ← α_i, μ_i, Σ_i
</code></pre></div></div> <h2 id="排序">排序</h2> <h3 id="1-排序的频率多久进行一次">1. <strong>排序的频率：多久进行一次？</strong></h3> <p>在 3DGS 中，<strong>排序</strong>是光栅化（rasterization）的一部分，具体用于按深度（z 值）对高斯点进行排序（front-to-back），以正确执行 alpha 混合。排序的频率直接取决于<strong>渲染的调用频率</strong>，因为每次渲染都需要对高斯点进行排序。</p> <h4 id="11-排序的时机"><strong>1.1 排序的时机</strong></h4> <ul> <li><strong>渲染触发</strong>： <ul> <li>排序发生在每次<strong>前向传播</strong>（forward pass）中，即每次调用 <code class="language-plaintext highlighter-rouge">render()</code> 函数时（包括训练、验证和渲染阶段）。</li> <li>在训练中，<code class="language-plaintext highlighter-rouge">render()</code> 由 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用，每处理一个批次（batch）数据（一张或多张 lego 图像）就渲染一次，因此排序也执行一次。</li> <li><strong>频率</strong>：<strong>每训练一步（step）</strong>，即每个 batch 都会进行一次排序。</li> </ul> </li> <li><strong>训练频率</strong>： <ul> <li>你的训练日志显示 300 个 epoch，30,000 步（<code class="language-plaintext highlighter-rouge">epoch=300-step=30000.ckpt</code>），每 epoch 100 个 batch（lego 数据集 <code class="language-plaintext highlighter-rouge">train/</code> 有 100 张图像，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 默认 batch_size=1）。</li> <li><strong>排序总次数</strong>：30,000 次（每 step 一次）。</li> <li><strong>时间开销</strong>：排序由 CUDA 加速（gsplat 使用 radix sort），单次排序 ~0.1-1 毫秒（RTX 3090，~100k 高斯），占渲染时间 &lt;10%。</li> </ul> </li> <li><strong>验证/渲染阶段</strong>： <ul> <li>验证（validation）：每 <code class="language-plaintext highlighter-rouge">val_interval</code>（默认 1000 步，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）渲染验证图像，触发排序。</li> <li>手动渲染：运行 <code class="language-plaintext highlighter-rouge">python render.py --ckpt_path outputs/lego_test/checkpoints/epoch=300-step=30000.ckpt</code>，每次生成一张图像或视频帧都排序一次（e.g., 100 帧视频 → 100 次排序）。</li> </ul> </li> <li><strong>Viewer</strong>：运行 <code class="language-plaintext highlighter-rouge">python viewer.py --ply_path outputs/lego_test/checkpoints/epoch=300-step=30000-xyz_rgb.ply</code>，交互式查看时，每帧实时渲染，排序随帧率（如 60 FPS → 每秒 60 次）。</li> </ul> <h4 id="12-代码中的排序"><strong>1.2 代码中的排序</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">16</span>
    <span class="p">)</span>  <span class="c1"># 内部排序
</span>    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>排序位置</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code>（gsplat 库，<code class="language-plaintext highlighter-rouge">gsplat/_torch_impl.py</code> 和 <code class="language-plaintext highlighter-rouge">cuda/rasterize.cu</code>）在每个 tile（16x16 像素块）内按深度 \(z_i\)（从 3D 均值 \(\mu_i\) 投影得到）排序。</li> <li>代码伪逻辑（CUDA）： <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// cuda/rasterize.cu</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">tile</span><span class="o">:</span>
    <span class="n">compute_depths</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">);</span>  <span class="c1">// z_i = (V * μ_i).z</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">depths</span><span class="p">);</span>     <span class="c1">// 按 z_i 排序</span>
    <span class="n">accumulate_colors</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">colors</span><span class="p">);</span> <span class="c1">// alpha 混合</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>频率</strong>：每个 batch 的 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用 <code class="language-plaintext highlighter-rouge">render()</code>，触发一次排序（~100 次/epoch）。</li> </ul> <h4 id="13-频率总结"><strong>1.3 频率总结</strong></h4> <ul> <li><strong>训练</strong>：每 step（batch）排序一次，30,000 步 → 30,000 次排序。</li> <li><strong>验证</strong>：每 1000 步（可调，<code class="language-plaintext highlighter-rouge">val_interval</code>），~30 次。</li> <li><strong>渲染/Viewer</strong>：每帧一次（视频 100 帧 → 100 次，viewer 60 FPS → 60 次/秒）。</li> <li><strong>配置调整</strong>：<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 的 <code class="language-plaintext highlighter-rouge">batch_size</code> 或 <code class="language-plaintext highlighter-rouge">val_interval</code> 影响频率，但排序本身不可跳过（alpha 混合依赖正确顺序）。</li> </ul> <hr/> <h3 id="2-排序的可微性排序是可微的吗">2. <strong>排序的可微性：排序是可微的吗？</strong></h3> <p><strong>简答</strong>：排序（sorting）是<strong>不可微的</strong>（non-differentiable），因为它是一个离散操作（改变高斯索引顺序），无法定义连续的导数。但在 3DGS 中，排序不影响反向传播的可微性，因为梯度流绕过了排序步骤，只依赖数值计算（权重 \(w_i\) 和透射率 \(T_i\)）。这与投影和 alpha 混合的可微性不同。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">SFM MVS Rendering</title><link href="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/" rel="alternate" type="text/html" title="SFM MVS Rendering"/><published>2025-12-25T12:25:10+00:00</published><updated>2025-12-25T12:25:10+00:00</updated><id>https://cekxm.github.io/blog/2025/sfm-mvs-rendering</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/"><![CDATA[<h2 id="资源">资源</h2> <ol> <li>https://www.youtube.com/watch?v=diBxFGgqAT0</li> <li>https://mashaan14.github.io/YouTube-channel/nerf/2025_01_25_sfm</li> </ol> <p>这个资源解释了 SFM，two view Geometry. 结合camera_and_stereo.pdf 进行理解。</p> <h2 id="多视角几何mvs新视角生成nvs对比">多视角几何（MVS），新视角生成（NVS）对比</h2> <p><img src="/images/2025-12-25-sfm-mvs-rendering/d3dcd8d3-35cb-403b-998d-64256b21ba06.png" alt="drawings-02 001" style="zoom:50%;"/></p> <p>简而言之：<strong>(MVS)SfM</strong> 是传统的“几何尺子”；<strong>VGGT</strong> 是现代的“几何大模型”；而 <strong>NVS</strong> 是“虚拟照相机”。</p> <h3 id="mvssfm-vs-vggt-vs-nvs-nerfgs-综合对比表">(MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>(MVS) SfM (如 COLMAP)</strong></th> <th><strong>VGGT (Visual Geometry Grounded Transformer)</strong></th> <th><strong>NVS (NeRF / 3D GS)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>核心任务目标</strong></td> <td><strong>三维重建</strong>：求解精确的相机姿态和场景几何（点云/深度）。</td> <td><strong>统一几何推断</strong>：一站式、秒级预测相机、点图、深度图和追踪。</td> <td><strong>新视角合成</strong>：在未拍摄过的角度生成照片级逼真的图像。</td> </tr> <tr> <td><strong>底层表示</strong></td> <td><strong>离散几何</strong>：稀疏或稠密的 3D 点云、深度图。</td> <td><strong>稠密几何图</strong>：Point Maps (\(H \times W \times 3\)) 和 Depth Maps。</td> <td><strong>光场表示</strong>：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。</td> </tr> <tr> <td><strong>运行机制</strong></td> <td><strong>优化驱动</strong>：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。</td> <td><strong>前馈推理 (Feed-forward)</strong>：一次性通过 Transformer 模型直接“看”出几何。</td> <td><strong>视图对齐优化</strong>：通过渲染结果与原图的颜色误差来反向训练。</td> </tr> <tr> <td><strong>对相机的需求</strong></td> <td><strong>未知或已知</strong>：通常用于解算未知相机参数。</td> <td><strong>无需预设</strong>：直接预测相机的 9 维参数（内外参）。</td> <td><strong>必须已知</strong>：通常依赖 SfM (COLMAP) 提供位姿初始化。</td> </tr> <tr> <td><strong>几何精确度</strong></td> <td><strong>高（度量级）</strong>：数学推导严谨，但易受弱纹理、模糊影响。</td> <td><strong>高且鲁棒</strong>：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。</td> <td><strong>中/低</strong>：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。</td> </tr> <tr> <td><strong>渲染视觉效果</strong></td> <td><strong>差</strong>：只有离散点，无法生成连续、真实的图像。</td> <td><strong>中</strong>：提供稠密几何，但主要用于几何任务而非美学渲染。</td> <td><strong>极高</strong>：支持照片级渲染、阴影、反射和透明效果。</td> </tr> <tr> <td><strong>处理速度</strong></td> <td><strong>极慢</strong>：通常需要数分钟到数小时。</td> <td><strong>极快</strong>：全流程通常在 <strong>1 秒以内</strong>。</td> <td><strong>训练慢/渲染快</strong>：NeRF 训练慢，GS 渲染极快，但都需要初始化。</td> </tr> </tbody> </table> <hr/> <h2 id="colmap">COLMAP</h2> <p><img src="/images/2025-12-25-sfm-mvs-rendering/35285f8e-982b-4ad8-96f7-1c4c5d09b5c7.png" alt="image"/></p> <p>在 COLMAP 的三维重建流水线中，<strong>BA（Bundle Adjustment，捆绑调整）</strong> 是最核心的优化环节。它的基本原理是通过<strong>非线性最小二乘法</strong>，同时优化相机的姿态、内参以及三维点的坐标，从而使重建结果达到最高的几何一致性。</p> <hr/> <h3 id="1-ba-的数学原理">1. BA 的数学原理</h3> <p>BA 的本质是一个<strong>重投影误差（Reprojection Error）</strong>最小化问题。</p> <h3 id="重投影误差公式">重投影误差公式</h3> <p>假设我们有 \(n\) 个三维点 \(X_j\) 和 \(m\) 张图像。对于每一张图像 \(i\) 和它观察到的点 \(j\)，其观测到的像素坐标为 \(x_{ij}\)。而根据当前估计的相机姿态 \(R_i, t_i\)、内参 \(K_i\) 和点 \(X_j\) 计算出的<strong>理论投影坐标</strong>为 \(\hat{x}_{ij} = \pi(K_i, R_i, t_i, X_j)\)。</p> <p>BA 的目标函数可以表示为：</p> \[\min_{K_i, R_i, t_i, X_j} \sum_{i=1}^m \sum_{j=1}^n \rho \left( \| x_{ij} - \pi(K_i, R_i, t_i, X_j) \|^2 \right)\] <ul> <li><strong>\(\pi\)</strong>：投影函数（将 3D 点映射到 2D 像素平面）。</li> <li><strong>\(\| x_{ij} - \hat{x}_{ij} \|^2\)</strong>：重投影误差。</li> <li><strong>\(\rho\)</strong>：鲁棒核函数（如 Huber loss），用于减少外点（错误匹配）对优化的干扰。</li> </ul> <h3 id="优化算法">优化算法</h3> <p>由于投影方程是非线性的，COLMAP 调用 <strong>Ceres Solver</strong> 库，使用 <strong>Levenberg-Marquardt (LM)</strong> 算法进行迭代求解。</p> <hr/> <h3 id="2-ba-在-colmap-中的作用">2. BA 在 COLMAP 中的作用</h3> <p>BA 贯穿于 COLMAP 的增量式重建（Incremental SfM）全过程，主要起到以下作用：</p> <ul> <li><strong>消除累积误差（去漂移）</strong>：在逐张添加图像的过程中，误差会不断累积。BA 能够通过全局约束，将这些微小的误差平摊，防止模型“弯曲”或“变形”。</li> <li><strong>精细化参数</strong>：初始的相机姿态通常由 PnP 算法得到，三维点坐标由三角化得到。BA 能够将这些“毛坯”数据进一步精细化，提高点云的精度。</li> <li><strong>自标定（Self-Calibration）</strong>：如果相机内参未知或不准，BA 可以通过优化过程自动校正焦距（Focal Length）和畸变参数。</li> <li><strong>剔除外点</strong>：在 BA 优化后，重投影误差依然很大的点会被视为无效点（Outliers）并被剔除，从而保证重建的鲁棒性。</li> </ul> <hr/> <h3 id="3-colmap-中的两种-ba-策略">3. COLMAP 中的两种 BA 策略</h3> <p>为了平衡计算速度和精度，COLMAP 将 BA 分为两个级别：</p> <table> <thead> <tr> <th><strong>类型</strong></th> <th><strong>触发时机</strong></th> <th><strong>优化范围</strong></th> <th><strong>目的</strong></th> </tr> </thead> <tbody> <tr> <td><strong>局部 BA (Local BA)</strong></td> <td>每注册一张新图像后</td> <td>当前图像及其邻近图像、可见的三维点</td> <td>确保新加入的图像能够稳定地融合进当前模型。</td> </tr> <tr> <td><strong>全局 BA (Global BA)</strong></td> <td>模型增长达到一定比例时</td> <td><strong>所有</strong>已注册的图像、<strong>所有</strong>三维点</td> <td>消除全局漂移，确保整个场景的几何结构严丝合缝。</td> </tr> </tbody> </table> <h3 id="4-估计的-k-r-t-x-是作为-ba-的初值吗">4. 估计的 \(K, R, t, X\) 是作为 BA 的初值吗？</h3> <p><strong>是的。</strong> BA 本质上是一个<strong>非线性优化</strong>过程，非线性优化必须依赖一个合理的<strong>初始值</strong>（Initial Guess）才能收敛到全局最优解，否则很容易陷入局部极小值。</p> <p>在 COLMAP 的增量式重建流程中，这些初值的来源如下：</p> <ul> <li><strong>相机姿态 (\(R, t\))</strong>：通过 <strong>PnP (Perspective-n-Point)</strong> 算法获得。当一张新图像被注册到现有模型时，系统利用已有的 3D 点和该图像中的 2D 特征匹配点，计算出该图像的位姿。</li> <li><strong>三维点坐标 (\(X\))</strong>：通过 <strong>三角化 (Triangulation)</strong> 获得。利用多张图像的相机中心和匹配特征射线的交点来估计点的空间位置。</li> <li><strong>内参 (\(K\))</strong>：通常来源于图像的 <strong>EXIF 信息</strong>（如焦距）或预设的相机模型。在 BA 过程中，这些参数可以被进一步精细化。</li> </ul> <p><strong>一句话总结：</strong> PnP 和三角化为 BA 提供了“毛坯”模型，而 BA 负责“精加工”。</p> <hr/> <h3 id="5-ba-是逐一优化还是同时优化">5. BA 是逐一优化还是同时优化？</h3> <p>BA 是<strong>同时优化（Joint Optimization）</strong>所有参数的。这意味着在同一个优化循环中，相机参数和三维点坐标是同时更新的。</p> <h4 id="为什么要同时优化">为什么要同时优化？</h4> <p>如果采用“逐一优化”（先固定点优化相机，再固定相机优化点），模型收敛速度会极慢，且容易陷入死循环或无法达到全局最优，这种方法被称为“坐标下降法”。</p> <h4 id="使用的数学技巧舒尔补-schur-complement">使用的数学技巧：舒尔补 (Schur Complement)</h4> <p>由于 BA 涉及成千上万个三维点和数百个相机位姿，直接求逆巨大的海森矩阵（Hessian Matrix）在计算上是不可行的。为了实现高效的<strong>同时优化</strong>，COLMAP（通过其背后的 <strong>Ceres Solver</strong>）使用了一个核心数学技巧：<strong>利用稀疏结构的舒尔补（Schur Complement）</strong>。</p> <h4 id="核心步骤">核心步骤：</h4> <ol> <li> <p>稀疏性分析：</p> <p>在 BA 中，一个 3D 点的投影只与观察到它的相机有关，与其他点无关；同样，一个相机的残差只与它看到的点有关。这导致其对应的正态方程（Normal Equations）具有极其显著的块稀疏结构。</p> \[\begin{bmatrix} B &amp; E \\ E^T &amp; C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v \\ w \end{bmatrix}\] <p>其中 \(B\) 是相机参数块，\(C\) 是三维点参数块（它是对角块矩阵，求逆非常快）。</p> </li> <li> <p>边缘化（Marginalization）：</p> <p>利用舒尔补将三维点块 \(C\) 消去，得到一个只包含相机参数的缩减相机系统（Reduced Camera System）：</p> \[(B - EC^{-1}E^T) \Delta x_c = v - EC^{-1}w\] <p>这个矩阵的维度大大降低（仅取决于相机的数量），求解出相机的增量 \(\Delta x_c\) 后，再通过代回（Back-substitution）快速算出所有点的增量 \(\Delta x_p\)。</p> </li> </ol> <h4 id="其他关键技术">其他关键技术：</h4> <ul> <li><strong>Levenberg-Marquardt (LM) 算法</strong>：一种介于高斯-牛顿法和梯度下降法之间的信赖域方法，保证了在远离最优解时也能稳健收敛。</li> <li><strong>鲁棒核函数 (Robust Loss)</strong>：如 Huber 或 Cauchy 损失，用来自动降低那些误匹配点（Outliers）在优化中的权重，防止它们带偏整个模型。</li> </ul> <p>在 COLMAP 的流水线中，参与 BA 的图像数量并不是一个固定值，它取决于<strong>你所处的重建阶段</strong>以及<strong>场景的规模</strong>。</p> <p>COLMAP 作为一种<strong>增量式 SfM（Incremental SfM）</strong>工具，它通过两种不同规模的 BA 来平衡精度和效率：</p> <h3 id="6-局部-ba-local-bundle-adjustment">6. 局部 BA (Local Bundle Adjustment)</h3> <ul> <li><strong>图像数量：通常为 10 到 30 张左右。</strong></li> <li><strong>原理：</strong> 每当 COLMAP 成功注册（添加）一张新图像并进行三角化后，它不会立即对全场进行优化（太慢了），而是执行一次局部 BA。</li> <li><strong>范围：</strong> 仅包含<strong>当前图像及其在共视图中最近的邻居图像</strong>。</li> <li><strong>作用：</strong> 确保新加入的图像能够正确地锚定在现有模型上，防止局部结构发生剧烈畸变。</li> </ul> <h3 id="7-全局-ba-global-bundle-adjustment">7. 全局 BA (Global Bundle Adjustment)</h3> <ul> <li><strong>图像数量：所有已注册的图像（从几十张到上万张不等）。</strong></li> <li><strong>原理：</strong> 当模型增长到一定程度（例如图像数量增加了 10%，或者达到了特定的步长），COLMAP 会触发全局 BA。</li> <li><strong>范围：</strong> 优化当前已成功进入模型的所有相机位姿和所有 3D 点。</li> <li><strong>规模示例：</strong> <ul> <li><strong>小型物体：</strong> 50 - 200 张图像。</li> <li><strong>中型场景（如建筑）：</strong> 500 - 2,000 张图像。</li> <li><strong>大型城市/测绘：</strong> 5,000 - 10,000+ 张图像。</li> </ul> </li> <li><strong>作用：</strong> 消除长时间增量重建积累的“漂移”误差，确保模型整体的闭环精度。</li> </ul> <hr/> <h3 id="8-影响参与-ba-图像数量的瓶颈">8. 影响参与 BA 图像数量的瓶颈</h3> <p>虽然理论上可以有成千上万张图像参与 BA，但实际操作中受以下因素限制：</p> <ol> <li><strong>内存（RAM）与显存：</strong> <ul> <li>BA 需要构建庞大的 Jacobi 矩阵。虽然有“舒尔补”技巧减小计算量，但当图像超过 <strong>2,000-3,000 张</strong>时，对内存的需求会显著增加。</li> <li>COLMAP 默认使用 <strong>Ceres Solver</strong>，如果内存不足，BA 可能会失败或运行极其缓慢。</li> </ul> </li> <li><strong>计算时间：</strong> <ul> <li>全局 BA 是 SfM 中最耗时的部分。对于上万张图的场景，一次全局 BA 可能需要几小时甚至更久。</li> </ul> </li> <li><strong>连接性（Connectivity）：</strong> <ul> <li>如果图像之间没有共同的特征点（即没有“边”连接），这些图像就不会在同一个 BA 块中被优化。COLMAP 会将它们拆分成不同的子模型（Sub-models）。</li> </ul> </li> </ol> <h2 id="研究方向">研究方向</h2> <p>在 2025 年的时间节点上，<strong>Novel View Synthesis (NVS)</strong> 在学术热度和资本市场显然更“火”，但 <strong>Multi-View Stereo (MVS)</strong> 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。</p> <p>这两者并非孤立竞争，而是呈现出一种<strong>深度融合</strong>的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：</p> <hr/> <h3 id="1-热门程度novel-view-synthesis-nvs-占据-c-位">1. 热门程度：Novel View Synthesis (NVS) 占据 C 位</h3> <p><strong>核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D</strong></p> <ul> <li><strong>学术热度：</strong> 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 <strong>3DGS (3D 高斯溅射)</strong> 和 <strong>生成式新视角合成</strong> 的论文数量呈爆炸式增长。</li> <li><strong>AIGC 助力：</strong> 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 <strong>Generative NVS</strong>）成了最热门的方向。</li> <li><strong>用户感知度：</strong> NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。</li> </ul> <h3 id="2-发展前景mvs-正在向几何大模型进化">2. 发展前景：MVS 正在向“几何大模型”进化</h3> <p><strong>核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry</strong></p> <ul> <li><strong>从“工具”到“大脑”：</strong> 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 <strong>VGGT</strong> 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个<strong>前馈网络（Feed-forward）</strong>。</li> <li><strong>解决“不可能任务”：</strong> 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。</li> <li><strong>工业刚需：</strong> 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的<strong>精确绝对坐标（Metric Geometry）</strong>。</li> </ul> <hr/> <h3 id="3-2025-年的关键趋势两者边界的模糊融合">3. 2025 年的关键趋势：两者边界的模糊（融合）</h3> <p>如果你在考虑职业发展或研究方向，<strong>“几何感知的 NVS” (Geometry-aware NVS)</strong> 是 2025 年最具前景的方向。</p> <ol> <li><strong>MVS 为前，NVS 为后：</strong> 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。</li> <li><strong>可推广性 (Generalizability)：</strong> 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 <strong>LRM (Large Reconstruction Models)</strong>，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。</li> <li><strong>动态场景：</strong> 静态场景的重建基本解决，2025 年的蓝海是<strong>动态 4D 重建</strong>（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。</li> </ol> <hr/> <h3 id="总结建议">总结建议</h3> <ul> <li><strong>如果你追求“视觉震撼”和“快速产出”：</strong> 选择 <strong>Novel View Synthesis (尤其是 3DGS)</strong>。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。</li> <li><strong>如果你追求“底层技术”和“稳健性”：</strong> 选择 <strong>MVS/几何大模型</strong>。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。</li> <li><strong>最具潜力的路径：</strong> 研究<strong>如何将 MVS 的几何约束引入 NVS</strong>（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。</li> </ul> <p><strong>结论：</strong> <strong>NVS 更“热门”（Hotter）</strong>，但 <strong>MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）</strong>。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">冲激信号的尺度变换</title><link href="https://cekxm.github.io/blog/2025/impulse/" rel="alternate" type="text/html" title="冲激信号的尺度变换"/><published>2025-01-26T02:01:00+00:00</published><updated>2025-01-26T02:01:00+00:00</updated><id>https://cekxm.github.io/blog/2025/impulse</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/impulse/"><![CDATA[<h3 id="t0-处冲激信号的尺度变换">\(t=0\) 处冲激信号的尺度变换</h3> \[\delta(at)=\frac{1}{|a|}\delta(t)\] <p>也就是说，对单位冲激信号进行尺度变换，会改变它的强度。</p> <p>这可以从冲激信号的极限法定义来理解。单位冲激信号可以由面积是1的矩形通过缩小宽度而生成。</p> <p>对该矩形进行尺度变换，会改变它的面积。</p> <p><img src="/images/2025-01-26-impulse/image-20220222005046096.png" alt="image-20220222005046096" style="zoom: 33%;"/></p> <h3 id="tt_0-处冲激信号的尺度变换">\(t=t_0\) 处冲激信号的尺度变换</h3> <p>若冲激信号为 \(\delta(t-t_0)\)，对它进行尺度缩放，有 \(\delta(at-t_0)=\delta(a(t-\frac{t_0}{a}))=\frac{1}{|a|}\delta(t-\frac{t_0}{a}))\) 可见，冲激信号的位置改变了，强度也发生了改变。</p> <p>同样的，可以从冲激信号的极限法定义来理解。</p> <p><img src="/images/2025-01-26-impulse/image-20220222005140714.png" alt="image-20220222005140714" style="zoom: 33%;"/></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">信号与系统课程的数学知识</title><link href="https://cekxm.github.io/blog/2025/math/" rel="alternate" type="text/html" title="信号与系统课程的数学知识"/><published>2025-01-26T00:01:00+00:00</published><updated>2025-01-26T00:01:00+00:00</updated><id>https://cekxm.github.io/blog/2025/math</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/math/"><![CDATA[<h3 id="基础">基础</h3> <p>本课程中，\(j\) 表示虚数单位，也就是高数中常用的 \(i\)。</p> <h4 id="欧拉公式">欧拉公式</h4> \[e^{j\varphi}=\cos \varphi +j \sin \varphi\] <p>由欧拉公式，可得 \(e^{jwt}=\cos(wt) +j \sin(wt)\)</p> \[\sin (wt) = \frac{1}{2j}(e^{jwt}-e^{-jwt})\] \[\cos (wt) = \frac{1}{2}(e^{jwt}+e^{-jwt})\] <blockquote> <p>例：展开 \(e^{(3-2j)t}\) \(e^{(3-2j)t}=e^{3t}e^{-j2t}=e^{3t}(\cos (2t) -j \sin (2t))\)</p> </blockquote> <h4 id="复数">复数</h4> <h5 id="笛卡尔坐标形式和极坐标形式">笛卡尔坐标形式和极坐标形式</h5> \[z = x+jy=re^{j\varphi}\] <p>其中，\(r\) 为模，\(\varphi\) 为相位（或辐角）。</p> <p><img src="/images/2025-01-26-math/480px-Complex_number_illustration_modarg.svg.png" alt="480px-Complex_number_illustration_modarg.svg" style="zoom:50%;"/></p> <h5 id="相位">相位</h5> <p>相位 \(\varphi\in[0,2\pi)\)</p> <blockquote> <p>例</p> </blockquote> <p><img src="/images/2025-01-26-math/Argand3-1.jpg" alt="Argand3-1" style="zoom:50%;"/></p> <p>相位的计算方法是 \(\varphi = atan2(y,x)\) 也就是<img src="/images/2025-01-26-math/argument.svg" alt="argument" style="zoom: 67%;"/></p> <p>但很多文献中，不太严格地写为 \(\varphi =\arctan (\frac{y}{x})\)</p> <h5 id="极坐标形式下的乘法除法">极坐标形式下的乘法、除法</h5> <p>乘法：模相乘，相位相加 \(r_1 e^{j\varphi_1 }\cdot r_2 e^{j\varphi_2 }=r_1 r_2 e^{j(\varphi_1+\varphi_2) }\) 除法：模相除，相位相减 \(\frac{r_1 e^{j\varphi_1 }}{r_2 e^{j\varphi_2 }}=\frac{r_1 }{r_2 }e^{j(\varphi_1-\varphi_2) }\)</p> <blockquote> <p>例</p> </blockquote> <p><img src="/images/2025-01-26-math/finding-reciprocal-of-complex-number.png" alt="finding-reciprocal-of-complex-number" style="zoom: 50%;"/></p> <h4 id="分部积分">分部积分</h4> \[\int f(t)g'(t)dt=f(t)g(t)-\int f'(t)g(t)dt\] <blockquote> <p>例：证明冲激偶的性质 \(\int_{-\infty}^{\infty}\delta'(t)f(t)dt=-f'(0)\)</p> <p>证： \(\int_{-\infty}^{\infty}\delta'(t)f(t)dt=\delta(t)f(t)|_{-\infty}^{\infty}-\int_{-\infty}^{\infty}\delta(t)f'(t)dt\) 第一项为0，第二项根据冲激信号的抽样性质，等于 \(-f'(0)\)。</p> </blockquote> <h4 id="三角函数">三角函数</h4> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>诱导公式</td> <td>\(\sin (\theta+\pi)=-\sin \theta\), \(\cos (\theta+\pi)=-\cos \theta\)</td> </tr> <tr> <td> </td> <td>\(\sin (\theta+\pi/2)=\cos \theta\), \(\cos (\theta+\pi/2)=-\sin \theta\)</td> </tr> <tr> <td>和差化积</td> <td>\(\sin\alpha + \sin\beta=2\sin(\frac{\alpha+\beta}{2})\cos(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\sin\alpha- \sin\beta=2\cos(\frac{\alpha+\beta}{2})\sin(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha + \cos\beta=2\cos(\frac{\alpha+\beta}{2})\cos(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha - \cos\beta=-2\sin(\frac{\alpha+\beta}{2})\sin(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td>积化和差</td> <td>\(\sin\alpha \cos\beta=\frac{1}{2}[\sin(\alpha+\beta)+\sin(\alpha-\beta)]\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha \cos\beta=\frac{1}{2}[\cos(\alpha+\beta)+\cos(\alpha-\beta)]\)</td> </tr> <tr> <td> </td> <td>\(\sin\alpha \sin\beta=\frac{1}{2}[\cos(\alpha+\beta)-\cos(\alpha-\beta)]\)</td> </tr> <tr> <td>两角和与差</td> <td> </td> </tr> </tbody> </table> <h3 id="冲激信号">冲激信号</h3> <h4 id="单位冲激信号-deltat">单位冲激信号 \(\delta(t)\)</h4> <p><strong>定义</strong>： \(\left\{ \begin{array}{**lr**} \int_{-\infty}^\infty \delta(t)dt=1 \\ \delta(t)=0, t\ne 0 \end{array} \right.\)</p> <blockquote> <p>例：求 \(\int_{-1}^5 \delta(t)dt\)</p> <p>由于 \(\delta(t)\) 在 \(t=0\) 以外的地方为 0，只要积分区间包含 \(t=0\)，则积分值等于 1. 所以答案为 1.</p> <p>也就是说，可以把积分区间缩小到 \(t=0\) 附近一个无穷小区间：</p> </blockquote> \[\int_{0_-}^{0_+} \delta(t)dt=1\] <p><strong>性质</strong>：</p> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>抽样性</td> <td>\(f(t)\delta(t)=f(0)\delta(t)\)</td> </tr> <tr> <td>抽样性</td> <td>\(\int_{-\infty}^\infty f(t)\delta(t)dt=f(0)\)</td> </tr> <tr> <td>抽样性</td> <td>\(f(t)\delta(t-t_0)=f(0)\delta(t-t_0)\)</td> </tr> <tr> <td>抽样性</td> <td>\(\int_{-\infty}^\infty f(t)\delta(t-t_0)dt=f(t_0)\)</td> </tr> <tr> <td>尺度变换</td> <td>\(\delta(at)=\frac{1}{\lvert a \rvert}\delta(t)\)</td> </tr> <tr> <td>尺度变换</td> <td>\(\delta(at-t_0)=\frac{1}{\lvert a \rvert}\delta(t-\frac{t_0}{a})\)</td> </tr> <tr> <td>偶函数</td> <td>\(\delta(-t)=\delta(t)\)</td> </tr> <tr> <td>积分</td> <td>\(\int_{-\infty}^t \delta(t)dt=u(t)\)</td> </tr> <tr> <td> </td> <td>一般的，\(\int_{-\infty}^t \delta(t-t_0)dt=u(t-t_0)\)</td> </tr> </tbody> </table> <h4 id="冲激偶信号-deltat">冲激偶信号 \(\delta'(t)\)</h4> <h3 id="卷积">卷积</h3> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>交换律</td> <td>\(f_1(t)\star f_2(t)=f_2(t)\star f_1(t)\)</td> </tr> <tr> <td>分配律</td> <td>\(f_1(t)\star[f_2(t)+f_3(t)]=f_1(t)\star f_2(t)+f_1(t)\star f_3(t)\)</td> </tr> <tr> <td>结合律</td> <td>\([f_1(t)\star f_2(t)] \star f_3(t)=f_1(t)\star [f_2(t) \star f_3(t)]\)</td> </tr> <tr> <td>微积分性，令\(g(t)=f(t)\star h(t)\)</td> <td>\(g'(t)=f(t)\star h'(t)=f'(t)\star h(t)\)</td> </tr> <tr> <td> </td> <td>\(g^{(-1)}(t)=f(t)\star h^{(-1)}(t)=f^{(-1)}(t)\star h(t)\)</td> </tr> <tr> <td> </td> <td>\(g(t)=f(t)^{(1)}\star h^{(-1)}(t)=f^{(-1)}(t)\star h^{(1)}(t)\)</td> </tr> <tr> <td>延时</td> <td> </td> </tr> <tr> <td>与冲激信号卷积</td> <td> </td> </tr> <tr> <td>与阶跃信号卷积</td> <td> </td> </tr> </tbody> </table> <h3 id="傅里叶变换">傅里叶变换</h3> <table> <thead> <tr> <th>时域</th> <th>傅里叶变换</th> </tr> </thead> <tbody> <tr> <td>\(\delta(t)\)</td> <td>\(1\)</td> </tr> <tr> <td>\(\delta'(t)\)</td> <td>\(j\omega\)</td> </tr> <tr> <td>\(u(t)\)</td> <td>\(\frac{1}{j\omega}+\pi\delta(\omega)\)</td> </tr> <tr> <td>\(sgn(t)\)</td> <td>\(\frac{2}{j\omega}\)</td> </tr> <tr> <td>\(1\)</td> <td>\(2\pi\delta(\omega)\)</td> </tr> <tr> <td>\(e^{j\omega_0 t}\)</td> <td>\(2\pi\delta(\omega-\omega_0 )\)</td> </tr> <tr> <td>\(\cos (\omega_0t)\)</td> <td>\(\pi[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]\)</td> </tr> <tr> <td>\(\sin (\omega_0t)\)</td> <td>\(\pi j[\delta(\omega+\omega_0)-\delta(\omega-\omega_0)]\)</td> </tr> <tr> <td>\(G_\tau(t)\)</td> <td>\(\tau Sa(\frac{\omega \tau}{2})\)</td> </tr> <tr> <td>\(Sa(\omega_0 t)\)</td> <td>\(\frac{\pi}{\omega_0 } G_{2\omega_0 }(\omega)\)</td> </tr> <tr> <td>\(e^{-at}u(t)\)</td> <td>\(\frac{1}{a+j\omega}\)</td> </tr> <tr> <td>\(e^{-a\lvert t \rvert}\)</td> <td>\(\frac{2a}{a^2+\omega^2}\)</td> </tr> <tr> <td>待补充</td> <td> </td> </tr> </tbody> </table> <h3 id="拉普拉斯变换">拉普拉斯变换</h3>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">矩阵</title><link href="https://cekxm.github.io/blog/2024/Matrix/" rel="alternate" type="text/html" title="矩阵"/><published>2024-12-14T12:01:00+00:00</published><updated>2024-12-14T12:01:00+00:00</updated><id>https://cekxm.github.io/blog/2024/Matrix</id><content type="html" xml:base="https://cekxm.github.io/blog/2024/Matrix/"><![CDATA[<h2 id="特征值和特征根">特征值和特征根</h2> <p>详细的求解步骤和示例：</p> <p><strong>1. 定义</strong></p> <p>对于一个 n 阶方阵 A，如果存在一个非零向量 v 和一个标量 λ，使得：</p> <p>Av = λv</p> <p>那么 λ 称为矩阵 A 的一个特征值（或特征根），v 称为 A 对应于特征值 λ 的一个特征向量。</p> <p><strong>2. 求解特征值</strong></p> <ul> <li> <p>将上述等式变形为：</p> <p>(A - λE)v = 0</p> <p>其中 E 是 n 阶单位矩阵。</p> </li> <li> <p>要使上式有非零解 v，系数矩阵 (A - λE) 的行列式必须为零：</p> \[|A - λE| = 0\] </li> <li> <p>这个方程称为矩阵 A 的特征方程。解这个方程，就可以得到矩阵 A 的所有特征值 λ。特征方程是一个关于 λ 的 n 次多项式方程。</p> </li> </ul> <p><strong>3. 求解特征向量</strong></p> <p>对于每个求出的特征值 λ，将其代入方程 (A - λE)v = 0，解这个齐次线性方程组，得到的非零解 v 就是对应于特征值 λ 的特征向量。</p> <p><strong>4. 求解步骤总结</strong></p> <ul> <li> <p>写出特征方程</p> \[|A-\lambda E|=0\] </li> <li> <p>解特征方程，求出所有特征值 \(λ_1, λ_2, ..., λ_n\)。</p> </li> <li> <p>对于每个特征值 λ<sub>i</sub>，解齐次线性方程组 (A - λ<sub>i</sub>E)v = 0，求出对应的特征向量 v<sub>i</sub>。</p> </li> </ul> <h2 id="余子式代数余子式">余子式，代数余子式</h2> <p>在<a href="https://zh.wikipedia.org/wiki/线性代数">线性代数</a>中，一个矩阵<strong>A</strong>的<strong>子式</strong>是指将<strong>A</strong>的某些行与列的交点组成的<a href="https://zh.wikipedia.org/wiki/方块矩阵">方阵</a>的<a href="https://zh.wikipedia.org/wiki/行列式">行列式</a>；而<strong>A</strong>的<strong>余子式</strong>（又称<strong>余因式</strong>或<strong>余因子展开式</strong>，英语：minor）是指将<strong>A</strong>的某些行与列去掉之后所余下的方阵的行列式，其相应的方阵有时被称为<strong>余子阵</strong>。</p> <p>将方阵<strong>A</strong>的一行与一列去掉之后所得到的余子式可用来获得相应的<strong>代数余子式</strong>（英语：cofactor），后者在可以通过降低多阶矩阵的阶数来简化矩阵计算，并能和<a href="https://zh.wikipedia.org/wiki/转置矩阵">转置矩阵</a>的概念一并用于<a href="https://zh.wikipedia.org/wiki/逆矩阵">逆矩阵</a>计算。</p> <p>不过应当注意的是，余子式和代数余子式两个概念的区别。在数值上，二者的区别在于，余子式只计算去掉某行某列之后剩余行列式的值，而代数余子式则需要考虑去掉的这一个元素对最后值正负所产生的影响。</p> <p>余子式：\({\textstyle M_{i,j}=\det {\bigl (}\left(\mathbf {A} _{p,q}\right)_{p\neq i,q\neq j}{\bigr )}}\)</p> <p>代数余子式： \({\displaystyle C_{ij}=(-1)^{i+j}M_{ij}}\)</p> \[{\displaystyle {\begin{aligned}\det(\mathbf {A} )&amp;=a_{1j}C_{1j}+a_{2j}C_{2j}+a_{3j}C_{3j}+\cdots +a_{nj}C_{nj}\\[2pt]&amp;=\sum _{i=1}^{n}a_{ij}C_{ij}\\[2pt]&amp;=\sum _{i=1}^{n}a_{ij}(-1)^{i+j}M_{ij}\end{aligned}}}\] <h2 id="伴随矩阵">伴随矩阵</h2> <h3 id="定义">定义</h3> <p>余子矩阵 \({\displaystyle \mathbf {C} =\left((-1)^{i+j}\mathbf {M} _{ij}\right)_{1\leq i,j\leq n}}\)</p> <p>伴随矩阵</p> \[{\displaystyle \operatorname {adj} (\mathbf {A} )=\mathbf {C} ^{\mathsf {T}}=\left((-1)^{i+j}\mathbf {M} _{ji}\right)_{1\leq i,j\leq n}}\] <h3 id="性质">性质</h3> \[{\displaystyle \mathbf {A} \operatorname {adj} (\mathbf {A} )=\operatorname {adj} (\mathbf {A} )\mathbf {A} =\det(\mathbf {A} )\mathbf {I} }\] \[{\displaystyle {\begin{aligned}\operatorname {adj} (\mathbf {A} )&amp;=\det(\mathbf {A} )\mathbf {A} ^{-1},\\\mathbf {A} ^{-1}&amp;=\det(\mathbf {A} )^{-1}\operatorname {adj} (\mathbf {A} ).\end{aligned}}}\] <h2 id="矩阵对角化">矩阵对角化</h2> <p>矩阵对角化是线性代数中的一个重要概念，它指的是将一个方阵通过相似变换转化为对角矩阵的过程。并非所有矩阵都可以对角化，只有满足特定条件的矩阵才能进行对角化。</p> <p><strong>矩阵可以对角化的条件主要有以下几种：</strong></p> <p><strong>1. n阶矩阵有n个线性无关的特征向量：</strong></p> <p>这是矩阵可对角化的<strong>充分必要条件</strong>。也就是说，一个n阶方阵 <em>A</em> 可以对角化，当且仅当它有n个线性无关的特征向量。</p> <ul> <li><strong>特征向量：</strong> 对于一个矩阵 <em>A</em>，如果存在非零向量 <em>v</em> 和标量 λ，使得 <em>Av</em> = λ<em>v</em>，那么 <em>v</em> 就称为 <em>A</em> 的一个特征向量，λ 称为对应的特征值。</li> <li><strong>线性无关：</strong> 一组向量是线性无关的，意味着其中任何一个向量都不能表示成其他向量的线性组合。</li> </ul> <p>如果一个n阶矩阵有n个不同的特征值，那么它一定有n个线性无关的特征向量，因此可以对角化。但反之不然，即使特征值有重复，只要能找到足够多的线性无关的特征向量，矩阵仍然可以对角化。</p> <p><strong>2. 每个特征值的几何重数等于其代数重数：</strong></p> <p>这也是矩阵可对角化的<strong>充分必要条件</strong>。</p> <ul> <li><strong>代数重数：</strong> 一个特征值作为特征多项式的根的重数。例如，如果特征多项式为 (λ-2)²(λ-3)，那么特征值 2 的代数重数为 2，特征值 3 的代数重数为 1。</li> <li><strong>几何重数：</strong> 一个特征值对应的线性无关的特征向量的个数，也等于矩阵 (A - λI) 的零空间的维数，其中 I 是单位矩阵。</li> </ul> <p>如果某个特征值的几何重数小于其代数重数，则矩阵不能对角化。</p> <h3 id="伴随矩阵和矩阵对角化">伴随矩阵和矩阵对角化</h3> <p>伴随矩阵和矩阵对角化是线性代数中两个重要的概念，它们之间没有直接的必然联系，但在某些情况下，通过伴随矩阵可以辅助判断或简化对角化的过程。</p> <h2 id="矩阵满秩">矩阵满秩</h2> <p><strong>满秩矩阵：</strong> 一个 <em>n</em> × <em>n</em> 的方阵，如果其秩为 <em>n</em>，则称该矩阵为满秩矩阵。满秩矩阵等价于：</p> <ul> <li>矩阵可逆。</li> <li>矩阵的行列式不为零。</li> <li>矩阵的 <em>n</em> 个行向量（或列向量）线性无关。</li> <li>矩阵没有零特征值。</li> </ul> <p><strong>重复特征值与满秩的关系：</strong></p> <ul> <li><strong>零特征值是关键：</strong> 矩阵不满秩的<strong>充分必要条件</strong>是它有<strong>至少一个</strong>零特征值。无论其他特征值是否重复，只要存在零特征值，矩阵就不是满秩的。</li> <li><strong>非零特征值的重复不影响满秩性：</strong> 如果一个矩阵的所有特征值都非零，即使某些特征值有重复，该矩阵仍然是满秩的。例如，一个 3×3 的矩阵，其特征值为 2，2，3，虽然 2 重复出现两次，但由于没有零特征值，该矩阵仍然是满秩的。</li> </ul> <p>判断一个矩阵是否满秩，<strong>最直接有效的方法是计算其行列式</strong>。如果行列式不为零，则矩阵满秩；反之，则不满秩。另一种方法是计算矩阵的秩，如果秩等于矩阵的阶数，则矩阵满秩。</p> <p>虽然特征值可以提供一些信息，但仅仅知道特征值是否有重复<strong>并不能直接判断</strong>矩阵是否满秩。只有当确定<strong>是否存在零特征值</strong>时，才能通过特征值判断矩阵是否满秩。</p> <h2 id="约当标准型"><a href="https://en.wikipedia.org/wiki/Jordan_normal_form">约当标准型</a></h2> <p><img src="/images/2024-12-15-Matrix/Jordan_canonical_form.svg.png" alt="By Michael Hardy - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=125410521" style="zoom:50%;"/></p> <table> <thead> <tr> <th>特性</th> <th>矩阵对角化</th> <th>约旦标准形</th> </tr> </thead> <tbody> <tr> <td>目标</td> <td>转化为对角矩阵</td> <td>转化为约旦标准形（准对角矩阵）</td> </tr> <tr> <td>条件</td> <td>有n个线性无关的特征向量</td> <td>任何方阵都可以</td> </tr> <tr> <td>形式</td> <td>对角矩阵</td> <td>由约旦块组成的准对角矩阵</td> </tr> <tr> <td>应用</td> <td>简化计算，分析矩阵性质</td> <td>处理不可对角化的矩阵，更深入地分析矩阵结构</td> </tr> <tr> <td>关系</td> <td>是约旦标准形的特例</td> <td>是对角化的推广</td> </tr> </tbody> </table> <h3 id="谱映射定理-spectral-mapping-theorem">谱映射定理 Spectral mapping theorem</h3> <p>Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the <a href="https://en.wikipedia.org/wiki/Functional_calculus">polynomial functional calculus</a>: Let <em>A</em> be an <em>n</em> × <em>n</em> matrix with eigenvalues \(\lambda_1,..., \lambda_n\), then for any polynomial <em>p</em>, <em>p</em>(<em>A</em>) has eigenvalues \(p(\lambda_1), ...,p(\lambda_n)\).</p> <h2 id="矩阵指数">矩阵指数</h2> <h3 id="计算方法">计算方法</h3> <h4 id="可对角化">可对角化</h4> <p>若矩阵可对角化</p> <p>\(A = UDU^{−1}\) 且 <em>D</em> 是对角矩阵，则</p> \[e^A = Ue^{D}U^{−1}\] <h4 id="零幂的情况">零幂的情况</h4> <p>A matrix <em>N</em> is <a href="https://en.wikipedia.org/wiki/Nilpotent_matrix">nilpotent</a> if \(N^q = 0\) for some integer <em>q</em>. In this case, the matrix exponential \(e^N\) can be computed directly from the series expansion, as the series terminates after a finite number of terms:</p> \[{\displaystyle e^{N}=I+N+{\frac {1}{2}}N^{2}+{\frac {1}{6}}N^{3}+\cdots +{\frac {1}{(q-1)!}}N^{q-1}~.}\] <h4 id="jordan-canonical-form-约当标准型"><strong>Jordan canonical form</strong> 约当标准型</h4> <p>Suppose that \(X = PJP^{−1}\) where <em>J</em> is the Jordan form of <em>X</em>. Then</p> \[{\displaystyle e^{X}=Pe^{J}P^{-1}}\] <p>并且，由于</p> \[{\displaystyle {\begin{aligned}J&amp;=J_{a_{1}}(\lambda _{1})\oplus J_{a_{2}}(\lambda _{2})\oplus \cdots \oplus J_{a_{n}}(\lambda _{n}),\\e^{J}&amp;=\exp {\big (}J_{a_{1}}(\lambda _{1})\oplus J_{a_{2}}(\lambda _{2})\oplus \cdots \oplus J_{a_{n}}(\lambda _{n}){\big )}\\&amp;=\exp {\big (}J_{a_{1}}(\lambda _{1}){\big )}\oplus \exp {\big (}J_{a_{2}}(\lambda _{2}){\big )}\oplus \cdots \oplus \exp {\big (}J_{a_{n}}(\lambda _{n}){\big )}.\end{aligned}}}\] <p>因此，我们只需要指导如何计算一个约当块的矩阵指数。由于每一个约当块具有如下形式</p> \[{\displaystyle {\begin{aligned}&amp;&amp;J_{a}(\lambda )&amp;=\lambda I+N\\&amp;\Rightarrow &amp;e^{J_{a}(\lambda )}&amp;=e^{\lambda I+N}=e^{\lambda }e^{N}.\end{aligned}}}\] <p>其中 <em>N</em> 是一种特殊的零幂矩阵。 <em>J</em> 的矩阵指数为</p> \[{\displaystyle e^{J}=e^{\lambda _{1}}e^{N_{a_{1}}}\oplus e^{\lambda _{2}}e^{N_{a_{2}}}\oplus \cdots \oplus e^{\lambda _{n}}e^{N_{a_{n}}}}\]]]></content><author><name></name></author><category term="sample-posts"/><category term="research"/><category term="math"/><summary type="html"><![CDATA[Matrix]]></summary></entry><entry><title type="html">Typora and Jekyll</title><link href="https://cekxm.github.io/blog/2024/images/" rel="alternate" type="text/html" title="Typora and Jekyll"/><published>2024-11-26T21:01:00+00:00</published><updated>2024-11-26T21:01:00+00:00</updated><id>https://cekxm.github.io/blog/2024/images</id><content type="html" xml:base="https://cekxm.github.io/blog/2024/images/"><![CDATA[<h2 id="typora-本地编写">Typora 本地编写</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layout: post
title: Typora and Jekyll
date: 2024-11-26 21:01:00
description: this is what included images could look like
tags: formatting images
categories: sample-posts
thumbnail: assets/img/9.jpg
typora-root-url: ../
</code></pre></div></div> <p>这边一个神奇的front matter entry 是 typora-root-url。设置了它，Typora 插入图片时，会使用全局路径。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![image-20241127101541293](/images/2024-11-26-images/image-20241127101541293.png)
</code></pre></div></div> <p>结合 Typora 设置如下。</p> <p><img src="/images/2024-11-26-images/image-20241127101541293.png" alt="image-20241127101541293"/></p> <p>这样，Jekyll 文件结构为</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/
   _posts
   assets
   images
</code></pre></div></div> <p>图片会保存在 images 中，并且按照文章的名字单独存在一个文件夹。</p> <h2 id="和原来编写的文章的兼容性">和原来编写的文章的兼容性</h2> <p>原来的图片一般和文章保存在同一个目录</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/
  post1.md
  post1/img.png
</code></pre></div></div> <p>只要把 typora-root-url 去掉，则 Typora 默认支持相对路径。完美兼容。</p> <h2 id="jekyll-支持的图片插入方法">Jekyll 支持的图片插入方法</h2> <div class="row mt-3"> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div> </div> </div> <div class="caption"> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</code></pre></div> </div> </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/11-480.webp 480w,/assets/img/11-800.webp 800w,/assets/img/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
</code></pre></div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div> </div> </div> <h2 id="drafts">Drafts</h2> <p>Jekyll 判断blog是否要发布会看两个时间，一个是文件名的时间前缀，另一个是 front matter 的 date 时间。</p> <ul> <li>如果文件名不包含时间，则认为是draft</li> <li>如果文件名时间或 front matter 的 date 时间在系统时间之后，则不会发布</li> <li>网页上的时间是按照 front matter 的 date 时间</li> </ul> <p><strong>因此推荐的做法是</strong></p> <ul> <li>文件名的时间按撰写的时间</li> <li>front matter 的 date 时间可以先写一个未来的时间，等要正式发布时，才改为过去的时间</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>
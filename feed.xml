<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cekxm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cekxm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-26T00:29:50+00:00</updated><id>https://cekxm.github.io/feed.xml</id><title type="html">计算机视觉</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DINO</title><link href="https://cekxm.github.io/blog/2025/dino/" rel="alternate" type="text/html" title="DINO"/><published>2025-12-26T00:00:00+00:00</published><updated>2025-12-26T00:00:00+00:00</updated><id>https://cekxm.github.io/blog/2025/dino</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/dino/"><![CDATA[<h2 id="处理任意输入大小图片">处理任意输入大小图片</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">skimage.color</span> <span class="kn">import</span> <span class="n">hsv2rgb</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="c1"># --- 1. 配置参数 ---
# 图像文件路径 (已设置为您的文件)
</span><span class="n">IMAGE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fruits.jpg</span><span class="sh">'</span> 
<span class="c1"># DINOv2 模型 ID (Base 版本，公开且无需权限)
</span><span class="n">MODEL_ID</span> <span class="o">=</span> <span class="sh">"</span><span class="s">facebook/dinov2-base</span><span class="sh">"</span> 
<span class="n">DEVICE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="c1"># DINOv2-base 默认 Patch size 为 14x14
</span><span class="n">PATCH_SIZE</span> <span class="o">=</span> <span class="mi">14</span> 
<span class="c1"># 设置一个最小的安全尺寸，防止原图太小
</span><span class="n">MIN_SIZE</span> <span class="o">=</span> <span class="mi">224</span> 

<span class="k">def</span> <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    加载 DINOv2 模型，提取 Patch 特征，使用 PCA 降维并可视化。
    图像尺寸会调整到最接近原始尺寸且是 Patch Size 的整数倍。
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">错误: 图像文件未找到于 </span><span class="sh">'</span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="sh">'</span><span class="s">。请检查路径并重试。</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 2. 初始化模型和处理器 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">正在加载 DINOv2 模型: </span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s"> 到 </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># 评估模式
</span>    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型加载失败。请检查模型 ID 或网络连接。</span><span class="se">\n</span><span class="s">错误信息: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 3. 图像处理与特征提取 (重点修改部分) ---
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">原始图像尺寸 (W x H): </span><span class="si">{</span><span class="n">W_orig</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_orig</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># a. 计算目标输入尺寸 (必须是 PATCH_SIZE 的整数倍)
</span>    <span class="c1"># 取最接近原始尺寸且小于等于原始尺寸的 PATCH_SIZE 倍数
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">H_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    
    <span class="c1"># 确保尺寸不小于最小安全尺寸
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">W_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型目标输入尺寸 (W x H): </span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># b. 预处理
</span>    <span class="c1"># 显式传递 size 和 crop_size 参数，控制预处理器的缩放行为
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">processor</span><span class="p">(</span>
        <span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> 
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 缩放或调整到目标尺寸
</span>        <span class="n">crop_size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 确保不进行中心裁剪
</span>        <span class="n">do_center_crop</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 明确禁用中心裁剪
</span>    <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 实际输入模型张量的尺寸
</span>    <span class="n">h_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">w_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    
    <span class="c1"># 重新计算 Patch 网格尺寸 (H, W)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">h_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    
    <span class="c1"># c. 提取特征
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># **inputs 解包字典作为命名参数
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> 
        <span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="c1"># d. 移除 CLS Token
</span>    <span class="k">if</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">已移除 CLS Token。剩余 Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- 4. PCA 降维 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">正在进行 PCA 降维...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="c1"># 归一化到 [0, 1] 范围
</span>    <span class="n">pca_min</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pca_max</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">pca_max</span> <span class="o">-</span> <span class="n">pca_min</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-8</span> 
    <span class="n">pca_features_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pca_features</span> <span class="o">-</span> <span class="n">pca_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># --- 5. 可视化映射 ---
</span>
    <span class="c1"># a. 重塑为网格形状 (H, W, 3)
</span>    <span class="n">pca_grid</span> <span class="o">=</span> <span class="n">pca_features_norm</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># b. 映射到 HSV 颜色空间 
</span>    <span class="n">hsv_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pca_grid</span><span class="p">)</span>
    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Hue (色调)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>              <span class="c1"># Saturation (饱和度)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Value (亮度)
</span>
    <span class="c1"># 转换为 RGB 颜色
</span>    <span class="n">rgb_vis</span> <span class="o">=</span> <span class="nf">hsv2rgb</span><span class="p">(</span><span class="n">hsv_image</span><span class="p">)</span>
    
    <span class="c1"># c. 缩放可视化结果到原始图像大小
</span>    <span class="n">rgb_vis_upscaled</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span>
        <span class="n">rgb_vis</span><span class="p">,</span> 
        <span class="p">(</span><span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span><span class="p">),</span> <span class="c1"># 使用原图尺寸进行缩放
</span>        <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">INTER_NEAREST</span> <span class="c1"># 最近邻插值保持 Patch 块状效果
</span>    <span class="p">)</span>

    <span class="c1"># --- 6. 显示和保存结果 ---
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">原始图像 (Original Image: fruits.jpg)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rgb_vis_upscaled</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">DINOv2 特征 PCA 可视化 (</span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s">x</span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="s"> 输入)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">output_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dinov2_fruits_pca_visualization.png</span><span class="sh">"</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">output_filename</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">可视化结果已保存为 </span><span class="si">{</span><span class="n">output_filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">IMAGE_PATH</span><span class="p">,</span> <span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">PATCH_SIZE</span><span class="p">,</span> <span class="n">MIN_SIZE</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="CV"/><summary type="html"><![CDATA[DINO]]></summary></entry><entry><title type="html">Matplotlib输出中文</title><link href="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/" rel="alternate" type="text/html" title="Matplotlib输出中文"/><published>2025-12-25T14:06:17+00:00</published><updated>2025-12-25T14:06:17+00:00</updated><id>https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/matplotlib%E8%BE%93%E5%87%BA%E4%B8%AD%E6%96%87/"><![CDATA[<h4 id="步骤一在-ubuntu-系统上安装中文字体并清理缓存">步骤一：在 Ubuntu 系统上安装中文字体并清理缓存</h4> <p>您需要在 Ubuntu 24 上安装一个包含中文支持的字体包。</p> <ol> <li> <p><strong>安装中文字体（推荐：文泉驿）：</strong> 打开您的终端，执行以下命令安装常用的开源中文字体包：</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt update
sudo apt install fonts-wqy-zenhei
</code></pre></div> </div> </li> <li> <p><strong>清理 Matplotlib 缓存：</strong> Matplotlib 会缓存系统中的字体信息。如果您不清理缓存，即使安装了新字体，它也可能无法立即识别。</p> <p>Bash</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 找到缓存目录的位置
python -c "import matplotlib; print(matplotlib.get_cachedir())"
   
# 通常该目录是 ~/.cache/matplotlib，清理它
rm -rf ~/.cache/matplotlib
   
# （重要）清理后，请确保重新启动您的 Cursor 终端或整个 IDE，以使新的环境变量和字体生效。
</code></pre></div> </div> </li> </ol> <h4 id="步骤二修改-python-代码配置-matplotlib-字体">步骤二：修改 Python 代码配置 Matplotlib 字体</h4> <p>在您的 Python 脚本开头，导入 Matplotlib 之后，添加字体配置代码，指定 Matplotlib 优先使用支持中文的字体。</p> <p><strong>请用下面这段代码替换您脚本开头的所有 <code class="language-plaintext highlighter-rouge">import</code> 语句以及它们之后的代码（在 <code class="language-plaintext highlighter-rouge">visualize_dinov2_features_variable_res</code> 函数定义之前）。</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># =========================================================
# Matplotlib 中文字体配置（解决 UserWarning 问题）
# =========================================================
</span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># 尝试设置 Matplotlib 的字体
</span>    <span class="c1"># 使用一个字体列表，优先尝试安装的文泉驿字体，确保兼容性
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">sans-serif</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">font.sans-serif</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">WenQuanYi Zen Hei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Microsoft YaHei</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DejaVu Sans</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 解决负号显示问题 (可选，但推荐)
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">axes.unicode_minus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matplotlib 已配置中文字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">配置中文字体失败: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">。将使用默认字体。</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># =========================================================
</span></code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Mamba</title><link href="https://cekxm.github.io/blog/2025/mamba/" rel="alternate" type="text/html" title="Mamba"/><published>2025-12-25T14:03:54+00:00</published><updated>2025-12-25T14:03:54+00:00</updated><id>https://cekxm.github.io/blog/2025/mamba</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/mamba/"><![CDATA[<h2 id="mamba">Mamba</h2> <p>Mamba 处理的是序列的建模问题，由输入 \(x(t)\) 得到输出 \(y(t)\)，所以它更像是一个特征提取。它的优势是对于序列长度 \(L\) 来说，复杂度线性。</p> <p>S4 是 structured state space sequence model. 结构化指的是A 是 HiPPO 矩阵（是这样吗？）。但是它的参数依然是时不变的。</p> <p>Mamba 是S6，加了 selective scan，使得参数时变，依赖于输入 \(x\)。</p> <p>如果有一定的基础，可以直接看这个对比。其中 \(B,L,D,N\) 分别是 batch，序列长度，输入（输出）的特征维度，内部特征的维度。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111607190.png" alt="image-20250904111607190" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904112651698.png" alt="image-20250904112651698" class="img-fluid"/></p> <p><img src="/images/2025-12-25-mamba/image-20250904113042955.png" alt="image-20250904113042955" class="img-fluid"/></p> <p>它会独立的把 \(x\) 的每一通道到输出 \(y\) 的每一维，中间通过一个更维的隐藏状态 \(h\)。如上图，\(x\) 的每一通道是独立计算的，因此 \(\bar{A}\)，\(\bar{B}\) 的尺寸为 \((B,L,D,N)\)。但是\(x\) 的每一通道的参数和整个 \(x\) 有关系。 \(\bar{A}\)，\(\bar{B}\) 的离散化见下图公式（4），结合上面的表格，在计算中，尺寸有变化，因此在代码中，有大量的 einsum 操作。</p> <p>由 \(A\)，\(B\) 求 \(\bar{A}\)，\(\bar{B}\) 的过程是离散化。这个和信号与系统的知识有关。它内在的过程还是连续的，但是取值的时间是离散的，所以是去算一个微分方程+初值在 \(\Delta\) 时间后的状态。</p> <p><img src="/images/2025-12-25-mamba/image-20250904111925419.png" alt="image-20250904111925419" class="img-fluid"/></p> <h3 id="矩阵-a">矩阵 \(A\)</h3> <p>在 Mamba 模型中，矩阵 A 是对角阵（diagonal matrix），而非 HiPPO 阵。HiPPO 是一种用于初始化矩阵 A 的策略，主要出现在早期的 S4 模型中，但 Mamba 采用了对角结构并使用不同的初始化方式，以实现更高效的计算。</p> <p>在 Mamba 中，虽然矩阵 A A A 被简化为对角矩阵，但其对角元素的初始化方式仍然受到 HiPPO 矩阵的启发，具体体现在以下几个方面：</p> <ol> <li>特征值的分布 <ul> <li>HiPPO 矩阵的特征值通常被设计为负实部，以确保系统的稳定性。在 Mamba 中，对角矩阵 A A A 的对角元素（即其特征值）被初始化为负值，模仿 HiPPO 矩阵的稳定性特性。这确保了 Mamba 在处理长序列时不会出现数值不稳定的问题。</li> <li>具体来说，Mamba 的对角元素通常被初始化为负的、对数分布的值（如 −1,−2,−4,…-1, -2, -4, \ldots−1,−2,−4,… 或类似的分布），这与 HiPPO 矩阵的特征值分布有相似的动机，即通过控制特征值的范围来平衡短期和长期记忆。</li> </ul> </li> <li>动态生成对角元素 <ul> <li>Mamba 的对角矩阵 A A A 的对角元素是由网络参数动态生成的，而不是固定的。这些参数在训练开始时会根据 HiPPO 的思想进行初始化。例如，Mamba 可能通过对数尺度（log-scale）初始化对角元素，以模拟 HiPPO 矩阵在不同时间尺度上的记忆能力。</li> <li>这种初始化方式使得 Mamba 能够在训练初期就具备捕捉长程依赖的能力，而无需像 S4 那样依赖稠密的 HiPPO 矩阵。</li> </ul> </li> <li>高效性与简化 <ul> <li>HiPPO 矩阵通常是稠密的，计算成本较高。Mamba 通过将 A A A 限制为对角矩阵，极大地降低了计算复杂度（从 O(N2) O(N^2) O(N2) 降到 O(N) O(N) O(N)，其中 N N N 是状态维度）。</li> <li>尽管结构上简化了，Mamba 仍然通过借鉴 HiPPO 的初始化策略，保留了其在序列建模中的核心优势，如对长序列的记忆能力和稳定性。</li> </ul> </li> </ol> <h3 id="gpu-sram--gpu-hbm">GPU SRAM + GPU HBM</h3> <p>这一点也是 mamba 效率的关键。</p> <p>参见1 <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a> Hardware-aware Algorithm</p> <p>以及原论文 sec3.3.2</p> <h2 id="参考文献">参考文献</h2> <h3 id="博客">博客</h3> <ol> <li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">A Visual Guide to Mamba and State Space Models</a></li> </ol> <h3 id="代码">代码</h3> <h4 id="自然语言处理">自然语言处理</h4> <ol> <li> <p><a href="https://github.com/johnma2006/mamba-minimal/tree/master">GitHub - johnma2006/mamba-minimal: Simple, minimal implementation of the Mamba SSM in one file of PyTorch.</a></p> <p>这个代码给出了 mamba 的简化结构代码，没有训练，但是有调用预训练模型（<a href="https://huggingface.co/state-spaces/mamba-370m/tree/main">需要从 huggingface 上下载，大小1.5G</a>）。</p> <p>可以理解它的底层代码。</p> </li> <li> <p><a href="https://readmedium.com/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049">Building Mamba from Scratch: A Comprehensive Code Walkthrough</a></p> </li> </ol> <p>这个代码是整套的，包含底层结构和训练，使用 enwiki8 数据。可以运行，我把它放到了 colab 上。</p> <p><img src="/images/2025-12-25-mamba/image-20250904095945491.png" alt="image-20250904095945491" class="img-fluid"/></p> <p>刚开始的 loss 和初始化比较有关系。（昨天因为这个原因，在 M4 上暂停了，回头再跑一次）。</p> <h4 id="图像">图像</h4> <p><a href="https://github.com/pprp/Vision-Mamba-CIFAR10">GitHub - pprp/Vision-Mamba-CIFAR10</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Flow</title><link href="https://cekxm.github.io/blog/2025/flow/" rel="alternate" type="text/html" title="Flow"/><published>2025-12-25T13:44:12+00:00</published><updated>2025-12-25T13:44:12+00:00</updated><id>https://cekxm.github.io/blog/2025/flow</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/flow/"><![CDATA[<h1 id="flow-matching">Flow matching</h1> <h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] <a href="https://medium.com/@nikolaus.correll/flow-matching-for-generative-models-from-scratch-8264bad4e0ba">Flow Matching For Generative Models From Scratch | by Nikolaus Correll | Toward Humanoids | Medium</a></p> <p>[2] <a href="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/">A Visual Dive into Conditional Flow Matching | ICLR Blogposts 2025</a></p> <h3 id="toy-example">toy example</h3> <p><a href="https://github.com/Whalefishin/Latent_Flow_Matching_MNIST">GitHub - Whalefishin/Latent_Flow_Matching_MNIST: A minimal example for training a flow matching model in a pretrained VAE’s latent space to generate MNIST digits.</a></p> <p><a href="https://github.com/lebellig/discrete-fm">GitHub - lebellig/discrete-fm: Educational implementation of the Discrete Flow Matching paper</a></p> <h2 id="理论">理论</h2> <h3 id="continuous-normalizing-flows">Continuous Normalizing Flows</h3> <p>以下主要参考 [2]</p> <p><img src="/images/2025-12-25-flow/image-20250911165058417.png" alt="image-20250911165058417" class="img-fluid"/></p> <p>首先要理解这个图。</p> <ul> <li> <p>概率路径 \(p_t\) 指 \(t\) 时刻 \(x\) 的概率分布 \(p_t(x)\)，一般令 \(p_0(t)\) 是一个标准的高斯分布，\(p_1(t)\) 是我们要建模的、未知的分布。概率路径就是从0时刻到1时刻，概率分布的变化过程。</p> </li> <li> <p>速度场 \(u(x,t)\) 比较好理解</p> </li> <li> <p>flow 就是流，类似水流，水流中 \(x\) 位置的一个水滴（假设它有独特的标记），经过 \(t\) 时候，会跟随水流移动到 \(f(x,t)\) 位置。 \(f(x,t)\) 可以看成一个映射，\(f^u: \mathbb{R}^d \times[0, 1] \to \mathbb{R}^d\)</p> </li> <li> <p>和 flow 有关系的另一个变量是 \(x(t)\) ,也是表示原处于 \(x_0\) 位置的点随时间t的变化，见下面这个常微分方程（ODE），它很好理解，速度是位置的导数。</p> </li> </ul> \[\begin{cases} x(0) = x_0 \\ \partial_t x(t) = u(x(t), t) \quad \forall t \in [0, 1] \end{cases}\] <p>在文献[2]中，\(\partial_t\) 就是指 \(\frac{\partial}{\partial t}\)。上式也叫 initial value problem（初值问题）。flow \(f^u(x,t)\) defined as the solution at time \(t\) to the initial value problem driven by \(u\) with initial condition \(x(0)=x\).</p> <p>上面这个图，三个要素形成三角关系。</p> <ul> <li>flow 和速度场之间是 ODE 的关系，知道速度场，可以求解 flow。\(f^u(x,t)\) 是它的解，\(x\) 实际上是初值。</li> <li>概率路径和速度场之间满足连续性方程</li> </ul> \[\begin{equation}\label{eq:continuity_eq} \partial_t p_t + \nabla\cdot u_t p_t = 0 \end{equation}\] <p>其中 \(\nabla\cdot\) 表示散度。这个方程保证了概率质量的守恒：概率密度在空间中的变化（通过时间导数 \(\frac{\partial p_t(x)}{\partial t}\)）由概率流量的散度决定。</p> <ul> <li>概率路径和flow之间满足 <em>change-of-variable formula</em>，也称为 pushforward \(p_t = f^u(\cdot, t)\# p_0\)。就是说通过这个流，把 \(p_0\) 经时间t，变为 \(p_t\)。</li> </ul> <p>以下是AI关于change-of-variable formula的介绍。</p> <p><strong>Change-of-variable formula</strong> 描述了概率密度如何随这种映射变化。对于一个可逆的映射 \(\phi_t: x_0 \mapsto x_t\) （可逆映射在这边就是flow，它是一个映射，并且是可逆的），概率密度 \(p_t(x_t)\) 与初始密度 \(p_0(x_0)\) 的关系为： \(p_t(x_t) = p_0(x_0) \cdot \left| \det\left( \frac{\partial \phi_t^{-1}}{\partial x_t} \right) \right| = p_0(\phi_t^{-1}(x_t)) \cdot \left| \det\left( \frac{\partial x_0}{\partial x_t} \right) \right|\) 其中：</p> <ul> <li>\(\phi_t^{-1}\) 是逆映射，从 \(x_t\) 映射回 \(x_0\)。</li> <li>\(\det\left( \frac{\partial x_0}{\partial x_t} \right)\) 是逆映射的雅可比行列式的绝对值。</li> </ul> <p>在流匹配中，雅可比行列式反映了流映射如何缩放空间，从而影响概率密度。</p> <blockquote> <p>我之前有疑问，从 \(x_0\) 到 \(x_t\) 可能经过了很长的距离变化，怎么这两个位置之间的密度能建立联系？答案可能是因为流是连续的，\(x_0\) 附近的质量都会流到 \(x_t\) 附近，这个变化由雅可比行列式来决定。</p> </blockquote> <h3 id="conditional-flow-matching">Conditional Flow Matching</h3> <p>可能，flow matching 就是指 conditional flow matching.</p> <p>CFM 核心思想是选择一个条件变量 \(z\)，以及一个条件概率路径 \(p(x\mid t,z)\)，满足两点</p> <ol> <li>由 \(p(x\mid t,z)\)​ 推导出的全局概率路径 \(p(x\mid t)\)​ 可以把 \(p_0\)​ 转化为 \(p_{data}\)​。即要求 \(p(x\mid t,z)\) 在 t=0,t=1 的边际概率 \(p(x\mid t=0)\)，\(p(x\mid t=1)\)符合</li> </ol> \[\begin{align*} \forall x \space E_z [ p(x \vert z, t=0) ] = p_0(x) , \\ \forall x \space E_z [ p(x \vert z, t=1) ] = p_{data}(x). \end{align*}\] <ol> <li>\(p(x\mid t,z)\) 对应的条件速度场 \(u^{cond}(x,t,z)\) （回忆一下，二者的关系是连续性方程）具有一个解析的形式，这是因为要使用一个神经网络来回归条件速度场。</li> </ol> <p>文中用下图进行概括：</p> <p><img src="/images/2025-12-25-flow/image-20250911233204539.png" alt="image-20250911233204539" class="img-fluid"/></p> <ul> <li> <p>首先，要做出 choice 1</p> </li> <li> <p>然后做出 choice 2（必须满足要求1）</p> </li> <li> <p>\(p(x\mid t,z)\) 能够确定 \(u^{cond}(x,t,z)\) ，它们之间满足连续性方程</p> </li> <li> <p>\(p(x\mid t,z)\) 边际化 \(z\) 可以获得 \(p(x\mid t)\)</p> </li> <li> <p>\(p(x\mid t)\) 和速度场 \(u(x,t)\) 之间满足连续性方程</p> </li> <li> <p>\(u(x,t)\)可由 \(u^{cond}(x,t,z)\)​ 显式的表达，这个关系就是文中的Theorem 1:</p> </li> </ul> \[\begin{align} \forall t, x, \, \, u(x,t) &amp;= E_{z\mid x, t} {u^{cond} }(x,t,z) \end{align}\] <ul> <li>实际计算中，并不是通过这个表达式去获得 \(u(x,t)\)。而是使用一个神经网络来回归条件速度场。那回归条件速度场有什么用呢？这个解释是文中的 Theorem 2. 即使用下面这个Loss来回归条件速度场，</li> </ul> \[\begin{aligned} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \overset{\mathrm{def}}{=} E_{ \substack{t \sim \mathcal{U}([0, 1]) \\ z \sim p_z \\ x \sim p( \cdot \mid t, z) } }{\lVert u_\theta^{CFM}(x,t) - \underbrace{u^{cond}(x,t,z)}_{\substack{ \text{chosen to be} \\ \text{explictly defined}, \\ \text{cheap to compute}, \\ \text{e.g., } x_1 - x_0}} \rVert^2} \enspace, \end{aligned}\] <p>等价于回归不可知的速度场</p> \[\begin{align*} \mathcal{L}^{\mathrm{CFM}}(\theta) &amp; \underset{(\text{proof below})}{=} E_{\substack{ t \sim \mathcal{U}([0, 1]) \\ x \sim p_t} } \Vert{u_\theta^{CFM}(x,t) - \underbrace{u(x,t)}_{\substack{\text{implicitly defined,} \\ \text{hard/expensive} \\ \text{to compute}}}}\Vert^2 + \underbrace{C}_{\text{indep. of } \theta} \end{align*}\] <p>Theorem 2 的证明使用了Theorem 1.</p> <p>因此，我们用神经网络去逼近条件速度 \(u^{cond}(x,t,z)\)，最终学习得到的是经过点 \((x,t)\) 的所有轨迹的平均速度，这个平均速度也就是 \(u(x,t)\)。</p> <h1 id="mean-flow">Mean flow</h1> <h2 id="mean-flow-论文中的-flow-matching-定义">mean flow 论文中的 flow matching 定义</h2> <p>flow 的 0 时刻为 \(p_{data}(x)\)，1 时刻为 \(p_{prior}(\epsilon)\)。flow 是把数据映射为先验（一版是高斯噪声），和之前定义的映射反过来了。给定\(x\sim p_{prior}(\epsilon)\)， \(x\sim p_{data}(x)\)，定义 flow path: \(z_t=a_t x+b_t \epsilon\)，其中 \(a_t\) , \(b_t\) 是 predifined schedules.</p> <blockquote> <p>可能，只要满足 \(z_0=\epsilon\), \(z_1=x\) 就行。</p> </blockquote> <p>比较常用的是，\(a_t=1-t\), \(b_t=t\)。由于速度 \(v_t = z'_t=a'_t x+b'_t \epsilon\)，因此 \(v_t = \epsilon - x\)。</p> <p>给定速度场 \(v(z_t,t)\)，通过求解 ODE 来进行数据采样 \(z_t\): \(\frac{d}{dt} z_t=v(z_t,t)\) starting from \(z_1=\epsilon\)。注意，这个初值问题的初值是 \(z_1\)。所以是从 \(t=1\) 倒推的。解可以写成 \(z_r=z_t - \int_r^t v(z_r,r)dr\) 实现时，是用数值解，比如欧拉法 \(z_{t_{i+1}}=z_{t_{i}}+(t_{i+1}-t_i)v(z_{t_{i}},t_i)\) 注意，这两个式子没有矛盾。倒推时，\(t_{i+1}-t_i&lt;0\)，速度取的时间是 \(t_i\)。</p> <h2 id="mean-flows">mean flows</h2> <p>定义平均速度 \(u\)： \(u(z_t,r,t) \triangleq \frac{1}{t-r}\int_r^tv(z_r,r)dr\) 用一个神经网络 \(u_\theta(z_t,r,t)\) 来预测平均速度 \(u\)。在训练时，就需要它的真值。经过推导可得 \(u(z_t,r,t) = v(z_t,t)-(t-r)\frac{d}{dt}u(z_t,r,t)\)</p> \[\frac{d}{dt}u(z_t,r,t) =v(z_t,t)\partial_z u +\partial_t u\] <p>可以看出，要知道真值，需要真值对时间的微分，这没法获得。所以实际使用的真值是 \(u_{tgt} = v(z_t,t)-(t-r)(v(z_t,t)\partial_z u_\theta +\partial_t u_\theta)\) 即在计算微分时，用参数化的 \(\partial u_\theta\) 来代替 \(\partial u\)。并且回顾一下神经网络的训练，真值用于计算 loss，它本身一般是一个固定值。而在这边真值和 \(u_\theta\) 有关系，所以需要额外设定它不参与微分计算，否则就会 “double backpropagation”。 \(\mathcal{L}(\theta)=E\parallel u_\theta(z_t,r,t) - sg(u_{tgt})\parallel_2^2\) <img src="/images/2025-12-25-flow/image-20251012141549566.png" alt="image-20251012141549566" class="img-fluid"/></p> <h2 id="mean-flows-with-guidance">Mean Flows with Guidance</h2> <p>classifier-free guidance</p> <p>推导看得不是很懂，但是训练过程，如下</p> <p><img src="/images/2025-12-25-flow/image-20251013095354856.png" alt="image-20251013095354856" class="img-fluid"/></p> <p>有区别的地方在于式（19），\(v_t\) 是 \(\epsilon -x\)，\(u_\theta^{cfg}(z_t,t,t)\)注意后面的两个时间都是 t，它是一个速度。</p> <p>上面 \(\omega\) 表示引导的强度。对照原版的CFG，原版的CFG还在训练时是没有 \(\omega\) 的，只在采样时使用，在训练时，有一步骤是以一定概率不给class，即 \(c=\empty\)。此处，同样有这步。</p> <h1 id="2025-cvpr-reversing-flow-for-image-restoration">2025 CVPR Reversing Flow for Image Restoration</h1> <p>论文强调了<strong>不确定范围（uncertainty scope）</strong>的概念（如图1所示）：从HQ到LQ的退化过程遵循数据处理不等式（Data Processing Inequality, DPI），即HQ与中间图像之间的互信息（mutual information）逐渐减少。随着退化加深，LQ图像的“不确定范围”扩大——多个HQ图像可能退化到相似的LQ图像（例如，不同清晰图像添加雾霾后变得相似）。图1直观描绘了这一过程：灰色区域表示从中间状态的不确定范围，随着从LQ向HQ逆转，不确定范围缩小，互信息增加。</p> <p>现有生成模型（如扩散模型或分数匹配模型）通常将退化过程建模为<strong>随机变换（stochastic transformation）</strong>，从高斯噪声开始逆向生成HQ图像。这引入了不必要的复杂性和计算开销（如数百步采样），因为<strong>LQ图像已提供结构信息，无需从纯噪声重构</strong>。论文指出，这种随机性导致训练和推理效率低下，且忽略了退化过程的确定性本质。</p> <p>ResFlow的核心动机：将退化过程重新定义为<strong>确定性路径（deterministic path）</strong>，使用连续归一化流（Continuous Normalizing Flows）实现可逆映射，从而高效逆转退化，仅需少于4步采样。</p> <blockquote> <p>这是论文的第一个评论，扩散模型属于随机前向过程，论文认为是不好的。flow matching 是确定性前向过程。但这个评论和从高斯噪声开始采样不是同一个问题。</p> <p>论文认为，从高斯噪声开始采样没有必要，因为LQ图像已提供结构信息。</p> <p>有一些扩散模型的方法从LQ图像开始恢复，比如论文第二页罗列的一些。但是论文说这些方法依然是随机前向过程。However, these approaches still treat the degradation process as a progressively diffusing stochastic forward process, which seems unnecessary and introduces additional complexity and inefficiency. Given that the degraded image is already known, the degradation process could be redefined as a deterministic forward process.</p> </blockquote> <table> <thead> <tr> <th>方法</th> <th>t=0 分布（起始/目标分布）</th> <th>t=T 分布（结束/噪声分布）</th> <th>关键特点</th> </tr> </thead> <tbody> <tr> <td><strong>DDRM [38]</strong></td> <td>清晰图像分布（HQ/clean image distribution）。恢复过程的最终输出 x_0 是估计的 HQ 图像。</td> <td>近似噪声分布（approximated noise distribution，通常高斯噪声 N(0, I)）。x_T 是 Markov 链的起始，条件于退化观测 y = H x + z（H 是退化算子，z 是已知噪声）。</td> <td>基于预训练 DDPM，解决线性逆问题（如去模糊、超分辨率）。前向从 HQ 扩散到噪声；逆向从噪声恢复 HQ，条件于 LQ（degraded image）。</td> </tr> <tr> <td><strong>IR-SDE [64]</strong></td> <td>清晰图像分布（HQ/high-quality image x(0)）。</td> <td>退化图像的噪声版本（LQ + fixed Gaussian noise, μ + ε，其中 μ 是 LQ，ε ~ N(0, σ²I)）。</td> <td>使用 mean-reverting SDE 建模退化过程，从 HQ 扩散到 noisy LQ。逆向从 noisy LQ 开始恢复 HQ。非 Markov 链，而是连续 SDE。</td> </tr> <tr> <td><strong>I2SB [55]</strong></td> <td>一个给定分布（通常清晰图像分布 HQ/clean data distribution）。</td> <td>另一个给定分布（退化图像分布 LQ/degraded data distribution）。</td> <td>构建 Schrödinger bridge（扩散桥），直接连接两个分布（clean 和 degraded）。非线性扩散过程，从 LQ 桥接到 HQ。边界对：t=0 为 clean，t=T 为 degraded（或反之）。</td> </tr> <tr> <td><strong>ResShift [112]</strong></td> <td>高分辨率图像分布（HR/high-resolution image distribution）。初始状态近似 HR。</td> <td>低分辨率图像分布（LR/low-resolution image distribution）。最终状态近似 LR。</td> <td>通过 shifting residual 在 HR 和 LR 之间构建 Markov 链。针对超分辨率（SR），减少步数；t=0 为 HR，t=T 为 LR。</td> </tr> <tr> <td><strong>RDDM [57]</strong></td> <td>目标图像分布（HQ/target image distribution）。</td> <td>纯噪声分布（pure noise, N(0, I)）用于生成；或噪声携带的输入图像（noise-carrying input, LQ + noise）用于恢复。</td> <td>双重扩散：residual diffusion（从 HQ 到 LQ 的定向扩散）和 noise diffusion（随机扰动）。统一生成和恢复；t=T 根据任务调整（纯噪声或 noisy LQ）。</td> </tr> <tr> <td><strong>Resfusion [89]</strong></td> <td>原图像分布（ground truth/original image distribution）。</td> <td>noisy degraded 图像分布（noisy degraded images, LQ + weighted residual noise）。</td> <td>将 residual term 引入前向过程，从 noisy LQ 开始逆过程。预测 resnoise（weighted residual + noise）；t=0 为 HQ，t=T 为 noisy LQ。统一训练/推理。</td> </tr> </tbody> </table> <p>因此论文，要 reverses the deterministic paths between HQ and LQ images for image restoration。即要使用flow matching 方法，同时从 LQ开始进行恢复。这样的问题是前面谈到的不确定范围。</p> <p>但直接用flow来建模HQ到LQ的退化是不可以的。这是因为退化导致互信息下降（信息处理不等式），而flow对应的ODE却会保持互信息不变。见命题1</p> <h2 id="命题1">命题1</h2> <p>flow 的 OED 保持互信息不变。 \(MI(z_{t_1}, r) = MI(z_{t_2}, r),\) 其中：</p> <p>\(z_t\) 是随时间 \(t\) 变化的随机过程（random process），由普通微分方程（ODE）定义：\(\frac{\partial z_t}{\partial t} = v(z_t, t)\)。 \(r\) 是任意参考随机变量（reference random variable），可以是 \(z_t\) 的任意状态（如 \(z_0\) 或其他 \(z_{t'}\)）。 \(MI(\cdot, \cdot)\) 表示互信息，定义为两个随机变量之间的共同信息量，数学上等价于： \(MI(X, Y) = H(X) + H(Y) - H(X, Y),\) 其中 \(H(X)\) 和 \(H(Y)\) 分别是 \(X\) 和 \(Y\) 的熵，\(H(X, Y)\) 是联合熵。</p> <h2 id="逆转流的基本形式reversing-flow-for-image-restoration">逆转流的基本形式（Reversing Flow for Image Restoration）</h2> <p>退化过程定义为随机过程{zt | 0 ≤ t ≤ 1}上的ODE： \(\frac{\partial z_t}{\partial t} = v(z_t, t); \quad 0 \leq t \leq 1,\) 其中v是速度场（velocity field），z_0对应HQ图像x_HQ，z_1对应LQ图像x_LQ。</p> <table> <tbody> <tr> <td>为使过程可逆，引入辅助过程{yt</td> <td>0 ≤ t ≤ 1}，增强状态：</td> </tr> </tbody> </table> \[z_t^T = [x_t^T; y_t^T], \quad z_0^T = [x_{HQ}^T; y_0^T], \quad z_1^T = [x_{LQ}^T; y_1^T].\] <p>yt编码“信息丢失”，与不确定范围耦合：当x_t接近x_LQ时，y_t与x_0的互信息增加，以保持整体MI(z_t, z_0)恒定</p> <p>。图2框架：zt由数据组件x_t（HQ到LQ）和辅助组件y_t（不确定范围缩小）组成。前向过程通过插值定义，逆向过程通过匹配速度场学习。神经网络v_θ估计速度： \(\frac{\partial [x_t^T; y_t^T]^T}{\partial t} = v_\theta(x_t, y_t, t).\)</p> <p>在实际实现中，\(y_0=0\), \(y_1\sim N(0,I)\).从 \(y_0\) 到 \(y_1\) 熵是增加的。</p> <h2 id="resflow-训练过程算法步骤">ResFlow 训练过程算法步骤</h2> <p>论文中的训练过程基于速度场匹配（velocity field matching），通过最小化Eq. (9)的损失函数实现。采用U-Net架构作为v_θ，Adam优化器，训练在256分辨率图像crops上进行。以下是算法步骤伪代码： text算法: ResFlow 训练过程</p> <p>输入: HQ-LQ图像对数据集 {(x0, x1)}，超参数 β=10, γ=1.75, 学习率 (详见Appendix C) 输出: 训练好的速度场网络 v_θ</p> <ol> <li> <p>初始化神经网络 v_θ (采用DDPM的U-Net架构，timestep t 通过adaptive layer normalization嵌入)</p> </li> <li> <p>对于每个训练epoch: a. 从数据集采样一个batch的HQ-LQ对 (x0, x1) b. 对于每个样本: i. 设置 y0 = 0 (零向量) ii. 采样 y1 ~ N(0, I) (标准高斯分布) iii. 定义退化调度: α^x_t = 1 - t, σ^x_t = t (对于数据组件 x) α^y_t = 1 - σ^y_t, σ^y_t = β / (1 - t + β) (对于辅助组件 y, 熵保持) iv. 计算路径点 (geodesics): x_t = α^x_t * x0 + σ^x_t * x1 y_t = α^y_t * y0 + σ^y_t * y1 z_t = [x_t; y_t] (增强状态) ˙z_t = [˙α^x_t * x0 + ˙σ^x_t * x1; ˙α^y_t * y0 + ˙σ^y_t * y1] (真实速度) v. 通过网络预测速度: v_θ(x_t, y_t, t) vi. 计算时间权重: λ(t) = [cos(π/2 * (t - 2)) + 1]^γ (强调t接近1) vii. 计算损失: L = ∫_0^1 λ(t) * ||v_θ(x_t, y_t, t) - ˙z_t||^2_2 dt (积分近似或蒙特卡罗采样t) c. 平均batch损失 d. 使用Adam优化器更新 θ (反向传播)</p> </li> <li> <p>重复步骤2直到收敛 (详见Appendix C的超参数，如学习率、batch size) 注意：损失优化确保凸传输成本非增，且无需模拟ODE（与传统流方法不同）。训练强调t接近1的困难样本，以平衡梯度。 ResFlow 采样过程算法步骤 论文中的采样（推理）过程基于逆向求解ODE Eq. (6)，从LQ图像开始，仅需4步采样（uniform time schedule）。以下是算法步骤伪代码： text算法: ResFlow 采样过程 (图像恢复)</p> </li> </ol> <p>输入: 低质量图像 x1 (LQ), 训练好的 v_θ, 步数 N=4 (默认) 输出: 恢复的高质量图像 ˆx0 (HQ)</p> <ol> <li> <p>采样辅助变量: y1 ~ N(0, I) (标准高斯分布)</p> </li> <li> <p>初始化增强状态: z1 = [x1; y1]</p> </li> <li> <p>设置时间步: t 从1到0，分N=4步 (uniform schedule, e.g., Δt = 1/N)</p> </li> <li> <p>对于每个时间步 i 从1到N: a. 当前 t = 1 - (i-1)/N b. 预测速度: v = v_θ(z_t 的 x 组件, z_t 的 y 组件, t) c. 更新状态: z_{t-Δt} = z_t - v * Δt (Euler方法或更高阶如Heun求解ODE dz/dt = v(z, t)) d. (可选) 替换中间 ˆy_t 为 ground-truth y_t = α^y_t * 0 + σ^y_t * y1 (基于Eq.(5)，但概念上丢弃 ˆy_t)</p> </li> <li> <p>从最终 z0 提取 ˆx0 (丢弃 ˆy0)</p> </li> <li> <p>输出 ˆx0 作为恢复图像 (全分辨率测试) 注意：采样是确定性的（无随机噪声注入），通过辅助y消除不确定性。论文实验显示此过程在&lt;4步内完成，适用于实时应用。</p> </li> </ol> <h1 id="pnp-flow-plug-and-play-image-restoration-with-flow-matching">PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING</h1> <p>pnp 方法的洞见是 <em>the proximal</em> step on the regularization term is effectively a denoising operation. 因此近端算子可以用BM3D或神经网络。</p> <p>本文的出发点是，近来生成模型提供了智能的框架来从数据直接学习 priors，可以超越人工设计或神经网络去噪器。</p> <h2 id="图像恢复的数学问题">图像恢复的数学问题</h2> <p>在论文的引言部分，图像恢复（image restoration）问题被表述为从退化观测（degraded observation）\(y\) 恢复未知图像 \(x\) 的逆问题（inverse problem），其中 \(y = Hx + \xi\) 这里，\(H\) 是一个（线性）退化算子（degradation operator），\(\xi\) 表示加性噪声（additive noise）模型。由于该问题是病态的（ill-posed）和高维的，求解具有挑战性。 论文假设图像 \(x\) 来自具有密度 \(p_X\) 的随机变量 \(X\)，观测 \(y\) 来自具有密度 \(p_Y\) 的随机变量 \(Y\)。然后，使用最大后验（maximum a posteriori, MAP）估计器求解具有最高后验概率的值： \(\arg\max_{x \in \mathbb{R}^d} \left[ \log p_{X\mid Y=y}(x) \right] = \arg\max_{x \in \mathbb{R}^d} \left[ \log p_{Y\mid X=x}(y) + \log p_X(x) \right],\) 其中右侧第一项是数据保真（fidelity to the data），第二项是图像的先验分布（prior distribution）。 由于 \(p_X\) 通常未知，且缺乏训练数据，论文转而考虑一个正则化优化问题（regularized optimization problem）： \(\arg\min_{x \in \mathbb{R}^d} \left\{ F(x) + R(x) \right\},\) 其中 \(F(x) := -\log p_{Y\mid X=x}(y)\) 表示数据保真项（data-fidelity term），\(R : \mathbb{R}^d \to \mathbb{R}\) 通常强制对解的一些假设（enforces some assumptions on the solution），以确保（唯一）最小化器的存在。例如，对于高斯噪声 \(\mathcal{N}(0, \sigma^2 I_d)\)，数据保真项对应 \(F(x) = \frac{1}{2\sigma^2} \\mid Hx - y\\mid ^2.\) 该优化问题可以通过近端分裂方法（proximal splitting methods）有效求解。</p> <h2 id="pnp方法">PNP方法</h2> <p><img src="/images/2025-12-25-flow/image-20251014191145906.png" alt="image-20251014191145906" class="img-fluid"/></p> <h2 id="pnp-meets-flow-matching"><em>P<strong>N</strong>P</em> <em>MEETS</em> FLOW <em>MATCHING</em></h2> <p>PnP-Flow定义时间相关去噪器： \(D_t := \mathrm{Id} + (1 - t) v^\theta_t,\)</p> <blockquote> <p>这个去噪器的前提是直线路径，所以训练这个去噪器使用OT coupling 或 reflow. \(\pi\) 是耦合（optimal transport耦合可产生直线路径）。</p> </blockquote> <blockquote> <p>对于最优传输（OT）耦合，有：</p> </blockquote> \[v_t(f(t, x)) = T(x) - x, \tag{5}\] <blockquote> <p>其中 \(T\) 是Monge映射。</p> </blockquote> <p>在理想情况下： \(D_t(x) = \mathbb{E}[X_1 \mid X_t = x],\) 其中 \(X_t = (1-t)X_0 + t X_1\)。对于直线路径，去噪损失为0（Proposition 1）。</p> <p><img src="/images/2025-12-25-flow/image-20251014220345497.png" alt="image-20251014220345497" class="img-fluid"/></p> <p>注意：</p> <ol> <li>PnP flow 中的flow 是预训练的。上述算法没有训练，而是迭代的图像恢复过程。</li> <li>其中 \(t_n\) 从小到大。其中 \(\tilde{z}^n\) 这一步有使用插值，使用插值的原因在下一小节。\(t_n\) 越小，插值时，加的噪声越大。</li> </ol> <p><img src="/images/2025-12-25-flow/image-20251014230941949.png" alt="image-20251014230941949" class="img-fluid"/></p> <p>注意 \(t=0\) 时，\(\tilde{z}^n=\epsilon\)，从噪声直接去噪，再指向 \(y\)。</p> <h2 id="为什么要插值">为什么要插值</h2> <p>在PnP-Flow算法中，插入插值步（interpolation step）的目的是确保去噪器 \(D_t\) 能够有效工作，这与Flow Matching（FM）模型的特性以及算法的迭代过程密切相关。以下是详细的解释：</p> <h3 id="原因与背景">原因与背景</h3> <p>PnP-Flow结合了PnP框架与FM模型，其中去噪器 \(D_t\) 是基于预训练的FM速度场 \(v^\theta_t\) 定义的： \(D_t = \mathrm{Id} + (1 - t) v^\theta_t. \tag{6}\) 理想情况下，\(D_t(x)\) 将路径上的点 \(X_t = (1-t)X_0 + t X_1\) 投影回目标分布 \(P_1\) 的样本 \(X_1\)，其中 \(X_0 \sim P_0\)（潜在分布），\(X_1 \sim P_1\)（数据分布），\((X_0, X_1) \sim \pi\)（耦合）。然而，传统PnP-FBS算法的迭代点（通过梯度步更新后）不一定位于FM路径 \(X_t\) 上，而 \(D_t\) 的设计假设输入点在该路径上。如果输入点偏离 \(X_t\) 的支持集，去噪效果会显著下降。</p> <h3 id="插值步的必要性">插值步的必要性</h3> <p>论文在第5页（Section 3.2）中指出，经典PnP-FBS直接在梯度步后应用去噪器，但由于 \(D_t\) 针对 \(X_t\) 优化，若梯度步输出的 \(z\) 不位于 \(X_t\) 支持集，效果不佳。因此，引入插值步将 \(z\) “投影”回FM路径。</p> <h3 id="技术细节与动机">技术细节与动机</h3> <ul> <li><strong>路径一致性</strong>：FM模型（尤其是OT-FM）产生直线路径 \(X_t = (1-t)X_0 + t X_1\)。插值步确保迭代点与此路径对齐，从而利用 \(D_t\) 的最佳性能（Proposition 1表明直线路径下去噪损失为0）。</li> <li><strong>避免退化</strong>：若不插值，\(D_t\) 可能将 \(z^n\) 映射到不相关区域，特别是在 \(t\) 接近1时，\(D_t\) 趋于恒等变换（\(D_1 = \mathrm{Id}\)），导致算法退化为仅依赖数据保真项。</li> <li><strong>噪声引入</strong>：\(\epsilon \sim P_0\) 的随机性模拟FM的潜在分布采样，防止 \(D_t\) 简单地将 \(\tilde{z}^n\) 映射回 \(z^n\)（若 \(\epsilon\) 与 \(z^n\) 耦合，效果会抵消，详见论文Remark 2）。</li> </ul> <h2 id="讨论">讨论</h2> <p>pnp 方法，需要知道 \(H\)，这个比较难办?</p> <h1 id="posterior-mean-rectified-flow">POSTERIOR-MEAN RECTIFIED FLOW</h1> <p>论文《Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration》（PMRF）关注照片真实图像恢复（Photo-Realistic Image Restoration, PIR）问题，即从退化测量（如噪声、模糊图像）中重建视觉上吸引人的图像。该领域算法通常通过失真度量（如PSNR、SSIM、LPIPS）和感知质量度量（如FID、KID、NIQE）评估，目标是实现最低失真而不牺牲感知质量（即重建图像看起来自然）。 现有方法的局限性：</p> <p>后验采样（Posterior Sampling）：许多扩散或流模型（如DPS、DDS）尝试从后验分布 \(p_{X\mid Y}\) 采样，理论上可实现完美感知指数（即重建分布 \(p_{\hat{X}} = p_X\)，其中 \(p_X\) 是真实图像分布）。然而，根据Blau &amp; Michaeli (2018)，其MSE（均方误差）是无约束最小MSE（MMSE）的两倍，即 \(\mathbb{E}[\\mid X - \hat{X}\\mid ^2] = 2 \times \mathrm{MMSE}\)。 GAN+失真损失：优化失真（如MSE）和感知（如GAN）损失的加权和，可遍历失真-感知权衡曲线（Distortion-Perception Tradeoff），但GAN优化困难，尤其当感知损失权重较大时，实际性能不如后验采样。</p> <p>论文的核心动机：针对完美感知指数约束下最小化MSE的最优估计器 \(\hat{X}_0\)，定义为： \(\hat{X}_0 = \arg\min_{p_{\hat{X}\mid Y}} \mathbb{E}[\\mid X - \hat{X}\\mid ^2] \quad \mathrm{s.t.} \quad p_{\hat{X}} = p_X.\) Freirich et al. (2021)证明，\(\hat{X}_0\) 可通过先预测后验均值 \(\hat{X}^* = \mathbb{E}[X\mid Y]\)（MMSE估计），然后将其最优传输（Optimal Transport）到真实分布 \(p_X\) 来构建。该MSE通常严格小于后验采样的MSE（如图1所示）。 受此启发，PMRF提出一个简单高效算法：使用整流流（Rectified Flow）近似最优传输地图，将后验均值预测传输到高质量图像。论文通过理论分析（如Proposition 1）和实验证明，PMRF在去噪、超分辨率、补全、着色和盲面部恢复等任务中优于基线方法。 训练过程 PMRF训练分为两个阶段（见Algorithm 1），假设 \(X\) 是真实图像随机向量，\(Y\) 是退化测量。</p> <p>阶段1：后验均值预测 训练模型 \(f_\omega\) 最小化MSE损失，近似后验均值 \(\mathbb{E}[X\mid Y]\)： \(\omega^* = \arg\min_{\omega} \mathbb{E}\left[ \\mid X - f_\omega(Y)\\mid ^2 \right].\)</p> <p>此阶段可使用现成高PSNR模型跳过。 实际中，\(f_\omega\) 可为CNN或Transformer架构，优化目标是重建接近真实图像的平滑预测。</p> <p>阶段2：整流流模型训练 训练向量场 \(v_\theta\) （Rectified Flow模型），最小化流匹配损失： \(\theta^* = \arg\min_{\theta} \int_0^1 \mathbb{E}\left[ \\mid (X - Z_0) - v_\theta(Z_t, t)\\mid ^2 \right] \, dt,\) 其中：</p> <p>\(Z_t = t X + (1-t) Z_0\)，为直线路径前向过程。 \(Z_0 = f_{\omega^*}(Y) + \sigma_s \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)，\(\sigma_s\) 是小噪声超参数（缓解源/目标分布维数不匹配引起的奇异性）。 \(t \sim \mathcal{U}[0,1]\)。 此损失训练 \(v_\theta\) 预测从后验均值到真实图像的直线方向，近似最优传输地图。</p> <p>训练中，\(v_\theta\) 通常为U-Net架构，输入包括 \(Z_t\) 和时间 \(t\)（通过位置编码嵌入）。 推理过程 推理时，给定退化测量 \(y\)，PMRF解决ODE生成重建图像 \(\hat{x}\)： \(\frac{d \hat{Z}_t}{dt} = v_{\theta^*}(\hat{Z}_t, t), \quad \hat{Z}_0 = f_{\omega^*}(y) + \sigma_s \epsilon,\) 其中 \(\epsilon \sim \mathcal{N}(0, I)\)。 使用Euler方法离散求解（K步）：</p> <p>采样 \(\epsilon \sim \mathcal{N}(0, I)\)。 初始化 \(\hat{x} = f_{\omega^*}(y) + \sigma_s \epsilon\)。 对于 \(i = 0\) 到 \(K-1\)： \(\hat{x} \leftarrow \hat{x} + \frac{1}{K} v_{\theta^*}\left( \hat{x}, \frac{i}{K} \right).\)</p> <p>返回 \(\hat{x}\)。</p> <p>此过程从后验均值开始，通过整流流逐步“校正”到真实分布，生成低失真、高感知质量图像。论文实验显示，PMRF在CelebA-Test盲面部恢复基准上达到SOTA（如表1所示）。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Diffusion</title><link href="https://cekxm.github.io/blog/2025/diffusion/" rel="alternate" type="text/html" title="Diffusion"/><published>2025-12-25T13:42:24+00:00</published><updated>2025-12-25T13:42:24+00:00</updated><id>https://cekxm.github.io/blog/2025/diffusion</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/diffusion/"><![CDATA[<h2 id="参考资料">参考资料</h2> <h3 id="论文和博客">论文和博客</h3> <p>[1] 2022 Understanding Diffusion Models A Unified Perspective.pdf</p> <p>[2] What are Diffusion Models Lilian Weng.pdf</p> <p>[3] <a href="https://drscotthawley.github.io/blog/posts/FlowModels.html">blog - Flow With What You Know</a></p> <h3 id="toy-example">toy example</h3> <ul> <li><a href="https://github.com/varun-ml/diffusion-models-tutorial?tab=readme-ov-file">GitHub - varun-ml/diffusion-models-tutorial: Experiment with diffusion models that you can run on your local jupyter instances</a></li> <li>Classifier-Free-Guidance (CFG) https://github.com/dome272/Diffusion-Models-pytorch</li> <li>Classifier-Free-Guidance (CFG) forked from 上面的链接，增加了一些功能和一个博客 <ul> <li>https://github.com/tcapelle/Diffusion-Models-pytorch</li> <li>https://wandb.ai/capecape/train_sd/reports/How-To-Train-a-Conditional-Diffusion-Model-From-Scratch–VmlldzoyNzIzNTQ1</li> </ul> </li> <li><a href="https://github.com/dome272/Diffusion-Models-pytorch">GitHub - dome272/Diffusion-Models-pytorch: Pytorch implementation of Diffusion Models (https://arxiv.org/pdf/2006.11239.pdf)</a></li> <li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch?tab=readme-ov-file">GitHub - lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch</a></li> </ul> <h2 id="variational-diffusion-models">Variational Diffusion Models</h2> <h3 id="markovian-hierarchical-variational-autoencoder">Markovian Hierarchical Variational Autoencoder</h3> <p>按照 [1] 的思路，首先介绍 Markovian Hierarchical Variational Autoencoder (MHVA)</p> <p><img src="/images/2025-12-25-diffusion/image-20250826145311374.png" alt="image-20250826145311374" class="img-fluid"/></p> <p>从左往右是编码，从右往左是解码。MHVA 规定，解码时，满足马尔可夫性，也就是生成 \(z_t\) 只依赖于 \(z_{t+1}\)。</p> <p>编码也是马尔可夫的，这个应该是默认的。因此有 \(p(x,z_{1:T})=p(z_T)p_\theta (x\mid z_1 )\prod_{t=2}^Tp_\theta (z_{t-1}\mid z_t) \tag{1}\)</p> \[q_\phi (z_{1:T}\mid x)=q_\phi(z_1\mid x)\prod_{t=2}^T q_\phi (z_{t}\mid z_{t-1}) \tag{2}\] <h3 id="variational-diffusion-models-可以认为是mhva再满足三个约束">Variational Diffusion Models 可以认为是MHVA再满足三个约束</h3> <ol> <li>隐藏变量和数据 x 维度一样</li> <li>编码器的结构不是学习得到的，而是设定为高斯</li> <li>设置编码器的高斯参数随时间而变化，使得T时刻 \(z_T\) 为标准高斯 \(N(0,I)\)。</li> </ol> <p><img src="/images/2025-12-25-diffusion/image-20250826151937355.png" alt="image-20250826151937355" class="img-fluid"/></p> <p>由于 z 和 x 维度一样，从这儿开始，如上图所示，统一用 x 表示。</p> <p>接下来，就是给出满足上述三个约束的 \(q(x_{t}\mid x_{t-1})\) 的参数了。然后推导得到约束3。这个推导参考文献[2]比较详细。这儿不再赘述。 \(q(x_{t}\mid x_{t-1})=N(x_t\mid \sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I )\) 在推导中，还有参数 \(\beta_t\), \(\bar{\alpha}_t\)。这几个参数可以相互转化。</p> <p>上式用<strong>参数化</strong>的技巧，就是 \(x_{t}=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} , \epsilon_{t-1}\sim N(0,I)\) 注意，参数 \(\alpha_t\) (or \(\beta_t\) or 其他变体) 的设置，可以按照预先规定的某种函数（scheduler），或者也可以学习得到。</p> <p>其次，经过推导，还有一个重要的性质 \(x_{t}=\sqrt{\bar{\alpha_t}}x_{0}+\sqrt{1-\bar{\alpha_t}}\epsilon , \epsilon\sim N(0,I)\) 也就是说，按编码的流程要采样得到 \(x_{t}\)，可以根据上式一步采样，不需要先采样 \(x_{1},x_{2}...x_{t-1}\)。这一点在训练中是有用的。</p> <h3 id="似然最大化elbo">似然最大化，ELBO</h3> <p>接下来，就是要通过最大似然来学习解码器 \(p_\theta (x_{t-1}\mid x_t)\) 了。具体来说，有满足某种分布的\(x\)数据集（这个 \(x\) 就是上面的 \(x_0\)），要最大化似然 \(\log p(x)=\log \int p(x_{0:T})dx_{1:T}\)</p> <blockquote> <p>这边有个疑惑，就是在后续推导中，有用到 \(q(x_{1:T}\mid x_0)\)。如果先进行编码，再进行解码，岂不是有两个不同的 \(x_1,x_2,...x_{T-1}\)。正确理解应该是，\(x_{1:T}\) 是被 marginalized out 的，所以考虑了所有可能，但每一种可能下，都只有一个值，不会有两个值。不管是p还是q，都是概率模型，所以任意值下都是可计算概率的。可以认为，先进行编码，得到了\(x_{1:T}\)，然后计算在当前模型参数下\(p_\theta (x_{t-1}\mid x_t)\)的似然，及其梯度。</p> </blockquote> <p>\(p_\theta (x_{t-1}\mid x_t)\)可以设为任意某种分布，只要知道\(z_{t-1},z_t\)（由编码生成）就可以计算概率\(p_\theta (x_{t-1}\mid x_t)\)。但是根据大段的推导，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</p> <p>推导参见参考文献。</p> <p>\(\log p(x)=\log \int p(x_{0:T})dx_{1:T}&gt;ELBO\)，ELBO由三项组成</p> <p><img src="/images/2025-12-25-diffusion/image-20250826210812707.png" alt="image-20250826210812707" class="img-fluid"/></p> <p>其中，</p> <ol> <li>第一项类似于VAE中的ELBO，可以通过蒙特卡洛估计来近似和优化。但在我看的代码中，这一项并没有使用。</li> <li>第二项没有需训练的参数，也是等于0.</li> <li>第三项可以推导出，\(p_\theta (x_{t-1}\mid x_t)\) 应尽量趋近后验概率 \(q(x_{t-1}\mid x_t,x_0)\)。</li> </ol> <p>关于第一项，在代码实现中没有被使用，AI是这样回答的。</p> <blockquote> <p><img src="/images/2025-12-25-diffusion/image-20250826211846887.png" alt="image-20250826211846887" class="img-fluid"/></p> </blockquote> <p>现在进一步分析上述第三项。可以推导得到，\(q(x_{t-1}\mid x_t,x_0)\) 是一个高斯分布。因此 \(p_\theta (x_{t-1}\mid x_t)\) 也应该是高斯分布，方差和\(q(x_{t-1}\mid x_t,x_0)\)的方差相同（因为方差和\(x_0\)无关，所以可以直接设为相等），均值要尽量接近<img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826222640680.png" alt="image-20250826222640680" style="zoom:50%;"/></p> <p>可以看出 \(\mu_q\) 和 \(x_0\) 有关，而\(p_\theta (x_{t-1}\mid x_t)\) 并没有\(x_0\)的信息，所以可以训练一个神经网络 \(\hat{x}_\theta (x_t,t)\)，从\(x_t\)来预测 \(x_0\)</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223028627.png" alt="image-20250826223028627" style="zoom:50%;"/></p> <p>由于两个高斯的KL距离是可以直接计算，再经过推导，最大化ELBO就等价于，在每个timestep，神经网络的输出 \(\hat{x}_\theta (x_t,t)\)要和\(x_0\)越符合，越好。即最优解为</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223326616.png" alt="image-20250826223326616" style="zoom: 50%;"/></p> <p>上式中的系数，也可以推导等于</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250826223502799.png" alt="image-20250826223502799" style="zoom:50%;"/></p> <h3 id="训练过程">训练过程</h3> <p>训练采用sgd，对于每一个\(x_0\)，只随机取一个\(t\)，采样得到 \(x_t\)（不用迭代，直接采样），然后计算上式，上式即为 loss。</p> <h3 id="推理过程">推理过程</h3> <p>推理过程则需要执行完整的解码过程。从一个随机向量 \(x_T\)出发，通过神经网络，计算得到 \(\hat{x}_\theta (x_t,t)\)，然后由上面 ，计算得到均值 \(\mu_\theta(x_t,t)\)；计算后验方差（\(p_\theta\) 的方差和后验概率 \(q(x_{t-1}\mid x_t,x_0)\) 的方差相等）；采样得到 \(x_{T-1}\)，如此迭代，最后得到 \(x_0\)。</p> <h3 id="简化的loss">简化的Loss</h3> <p>见[2] 中的一些补充</p> <p>Empirically, Ho et al. (2020) 提出不带上式中系数的Loss</p> <p><img class="img-fluid" src="/images/2025-12-25-diffusion/image-20250827094521508.png" alt="image-20250827094521508" style="zoom:50%;"/></p> <p>其中，\(\epsilon_t\) 是编码过程采样得到 \(x_t\) 的标准高斯噪声，\(\epsilon_\theta\) 是学习的神经网络。大概理解是这样：目标是要让解码的 \(x_{t-1}\) 分布均值接近编码的 \(x_{t-1}\) 分布均值，编码的 \(x_{t-1}\) 分布均值和 \(x_{t}\) 之间有关系，除了乘性系数外，差值为 \(\epsilon_t\) 。解码的 \(x_{t-1}\) 分布均值经过推导式子和编码时大概一样，区别是需要预测一个\(\epsilon_\theta\)。见下图 Algorithm2 Sampling step 4.</p> <p><img src="/images/2025-12-25-diffusion/image-20250827095354468.png" alt="image-20250827095354468" class="img-fluid"/></p> <h4 id="训练和推理">训练和推理</h4> <p><img src="/images/2025-12-25-diffusion/image-20250827143550828.png" alt="image-20250827143550828" class="img-fluid"/></p> <h3 id="学习扩散噪声参数">学习扩散噪声参数</h3> <p>上述，扩散噪声参数是按照scheduler确定的，在实现中，也可以联合学习得到。</p> <h2 id="noise-conditioned-score-networks-ncsn">noise-conditioned score networks (NCSN)</h2> <p>注意：本小节引用了AI的回答，其中 \(p\) 泛指概率，而不是特制反向采样中的概率分布。</p> <h3 id="分数-s_thetax_t-t">分数 \(s_\theta(x_t, t)\)</h3> <p>所谓“分数”，指的是概率密度函数的对数关于数据的梯度，即： \(s(x) = \nabla_x \log p(x)\) 其中 \(p(x)\) 是数据的概率密度函数，分数 \(s(x)\) 描述了数据在概率密度上的局部变化方向和幅度。通过训练一个模型 \(s_\theta(x)\) 来逼近真实的分数 \(\nabla_x \log p(x)\)，可以间接学习数据的概率分布，而无需直接估计 \(p(x)\) 本身。</p> <p>在扩散模型中，分数可以有两种相关定义，具体取决于上下文：</p> <ul> <li><strong>边缘分布的分数</strong>：\(\nabla_{x_t} \log p(x_t)\)，其中 \(p(x_t)\) 是时间步 \(t\) 的边缘分布，即： \(p(x_t) = \int p(x_t \mid x_0) p_{\text{data}}(x_0) \, dx_0\) 这个分数描述了 \(x_t\) 在整个数据分布上的概率密度梯度。然而，\(p(x_t)\) 通常难以直接计算，因为它涉及对所有可能的 \(x_0\) 积分。</li> <li><strong>条件分布的分数</strong>：\(\nabla_{x_t} \log p(x_t \mid x_0)\)，其中 \(p(x_t \mid x_0)\) 是前向过程中给定原始数据 \(x_0\) 的条件分布。根据扩散模型的前向过程： \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\) 这个分数的计算更直接，因为它是高斯分布的梯度： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\)</li> </ul> <p>在实际的扩散模型训练中（如 DDPM），通常使用 <strong>去噪分数匹配（Denoising Score Matching, DSM）</strong>，训练分数模型 \(s_\theta(x_t, t)\) 来逼近 <strong>条件分数</strong> \(\nabla_{x_t} \log p(x_t \mid x_0)\)，而不是边缘分数 \(\nabla_{x_t} \log p(x_t)\)。这是因为条件分数的表达式已知且易于计算，而边缘分数需要复杂的积分。</p> <h3 id="分数如何用在采样中">分数如何用在采样中</h3> <p>大体思路如下：根据扩散模型的推导，在反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计（下面第2点），而\(x_0\) 的估计和分数\(s_\theta(x_t, t)\)有关系（下面第1点）。</p> <ol> <li><strong>分数的可计算性</strong>：</li> </ol> <p>前向过程定义了 \(p(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)\)，其分数可以通过高斯分布的性质直接计算： \(\nabla_{x_t} \log p(x_t \mid x_0) = -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 这个分数是已知的，因此可以作为训练目标。</p> <p>训练好的分数模型 \(s_\theta(x_t, t)\) 可以用来估计 \(x_0\)（原始数据）的值： \(s_\theta(x_t, t) \approx -\frac{x_t - \sqrt{\bar{\alpha}_t} x_0}{1 - \bar{\alpha}_t}\) 重排后： \(x_0 \approx \frac{x_t + (1 - \bar{\alpha}_t) s_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\)</p> <ol> <li><strong>与反向过程的联系</strong>：</li> </ol> <p>反向过程 \(p_\theta(x_{t-1} \mid x_t)\) 的均值需要用到 \(x_0\) 的估计。分数模型 \(s_\theta(x_t, t)\) 提供了从 \(x_t\) 估计 \(x_0\) 的方法。</p> <p>均值可以通过高斯条件分布公式得到： \(\mu_q(x_t, x_0) = \sqrt{\alpha_t} x_{t-1} + \frac{(1 - \alpha_t) \sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_t}\) 将 \(s_\theta(x_t, t)\) 代入反向过程的均值表达式，DDPM 推导出简化的均值形式： \(\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} s_\theta(x_t, t) \right)\) 这个表达式直接用 \(x_t\) 和 \(s_\theta(x_t, t)\) 表示均值。</p> <p>以上公示由AI生成，用作思路理解。</p> <h3 id="和上面简化的loss中-epsilon-的关系">和上面简化的Loss中 \(\epsilon\) 的关系</h3> <p>\(\epsilon\) 是 \(x_t\) 和它分布均值之间的差。即 \(\epsilon =x_t - \sqrt{\bar{\alpha}_t} x_0\) \(\epsilon_\theta(x_t,t)\) 是它的估计。</p> <p>因此有 \(s_\theta(x_t, t) \approx -\frac{\epsilon_\theta(x_t,t)}{1 - \bar{\alpha}_t}\)</p> <h2 id="classifier-guided-diffusion">Classifier Guided Diffusion</h2> <p>分类器引导扩散中，有一个条件 \(y\)，使用了条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\) 来调整无条件扩散模型的采样过程，符合分数模型的核心思想。</p> <p><strong>分类器 \(p(y \mid x_t)\) 和无条件扩散模型 \(s_\theta(x_t, t)\) 都是预训练的</strong>，然后通过调整反向过程的均值。分类器 \(p_\phi(y \mid x_t, t)\) 是预训练的，在带噪声的图像 \(x_t \sim q(x_t \mid x_0)\) 上训练，以预测类别 \(y\)。这确保分类器梯度在不同噪声水平下可靠，为条件分数提供准确的引导。</p> <p>具体来说，论文通过以下方式从分数模型角度解释分类器引导扩散：</p> <ul> <li> <p>它将无条件分数 \(\nabla_{x_t} \log p(x_t)\)（由<strong>预训练</strong>的扩散模型提供）与分类器的梯度 \(\nabla_{x_t} \log p(y \mid x_t)\) 结合，构造条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。这一步通过贝叶斯准则可以推导。 \(\nabla_{x_t} \log p(x_t \mid y) = \nabla_{x_t} \log p(x_t) + \nabla_{x_t} \log p(y \mid x_t)\)</p> </li> <li> <p>反向过程的均值通过条件分数调整，确保采样生成符合特定类别 \(y\) 的样本。</p> </li> </ul> <p>替换为条件分数后，得到：</p> <p>\(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \nabla_{x_t} \log p(x_t \mid y) \right)\) \(\tilde{\mu}_t(x_t, y) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \left( s_\theta(x_t, t) + s \cdot \nabla_{x_t} \log p_\phi(y \mid x_t) \right) \right)\) 这里的 \(s_\theta(x_t, t) \approx \nabla_{x_t} \log p(x_t)\)，而 \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) 是分类器梯度，整体形成条件分数 \(\nabla_{x_t} \log p(x_t \mid y)\)。</p> <blockquote> <p>“To perform conditional sampling, we train a classifier \(p_\phi(y \mid x_t)\) on noisy images to predict the class label \(y\), and use its gradient \(\nabla_{x_t} \log p_\phi(y \mid x_t)\) to guide the diffusion process. Specifically, we modify the reverse process mean as follows: \(\tilde{\mu}_t(x_t, y) = \mu_t(x_t) + s \cdot \sigma_t^2 \cdot \nabla_{x_t} \log p_\phi(y \mid x_t)\) where \(\mu_t(x_t)\) is the original mean of the reverse process, \(\sigma_t^2\) is the variance, and \(s\) is a scale factor controlling the strength of the classifier guidance.”</p> </blockquote> <p>这边分类器梯度前面有正负不一致。先不管。</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p><strong>论文中的符号和其他文献不同，\(\lambda\) 是时间 \(t\)，但是采样时 \(\lambda\) 从小到大。\(z_\lambda\) 对应 \(x_t\)。</strong></p> <p>CFG 的核心是联合训练一个支持条件和无条件的扩散模型，然后在采样时通过线性组合分数函数来实现引导。论文将扩散模型表述在连续时间框架下，\(\lambda\) 被定义为前向过程的噪声水平，\(\lambda=\log \alpha_\lambda^2/\sigma_\lambda^2\) 即 log SNR。其中前向过程为 \(q(z_\lambda \mid x) = \mathcal{N}(\alpha_\lambda x, \sigma_\lambda^2 I)\)，\(\alpha_\lambda^2 = 1 / (1 + e^{-\lambda})\)，\(\sigma_\lambda^2 = 1 - \alpha_\lambda^2\)。</p> <h3 id="训练过程algorithm-1-in-the-paper">训练过程（Algorithm 1 in the paper）：</h3> <ol> <li> <p>使用单个神经网络参数化分数估计器 \(\epsilon_\theta(z_\lambda, c)\)（条件模型）和 \(\epsilon_\theta(z_\lambda)\)（无条件模型）。</p> </li> <li> <p>在训练时，以概率 \(p_{\text{uncond}}\)（超参数，通常为 0.1-0.2）随机将条件信息 \(c\) 设置为无条件标识符 \(\emptyset\)，从而联合训练两个模型。 训练目标是去噪分数匹配（Denoising Score Matching）：\(\mathbb{E}_{\epsilon, \lambda} [ \\mid \epsilon_\theta(z_\lambda) - \epsilon \\mid _2^2 ]\)，其中 \(z_\lambda = \alpha_\lambda x + \sigma_\lambda \epsilon\)，\(\epsilon \sim \mathcal{N}(0, I)\)。</p> </li> <li> <p>论文描述：“We jointly train the unconditional and conditional models simply by randomly setting \(c\) to the unconditional class identifier \(\emptyset\) with some probability \(p_{\text{uncond}}\), set as a hyperparameter.”</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110741671.png" alt="image-20250930110741671" class="img-fluid"/></p> </li> </ol> <h3 id="采样过程algorithm-2-in-the-paper">采样过程（Algorithm 2 in the paper）：</h3> <ol> <li>从纯噪声 \(z_{\lambda_T} \sim \mathcal{N}(0, I)\) 开始，逐步去噪。</li> <li>在每个时间步 \(t\)，计算引导分数：\(\tilde{\epsilon}_\theta(z_\lambda, c) = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\)，其中 \(w\) 是引导强度（guidance strength，通常 \(w &gt; 0\)）。</li> <li>使用这个引导分数来更新样本，例如在 DDPM 采样中替换原分数函数。 论文解释：“Form the classifier-free guided score at log SNR \(\lambda_t\): \(\tilde{\epsilon}_t = (1 + w) \epsilon_\theta(z_\lambda, c) - w \epsilon_\theta(z_\lambda)\).”</li> </ol> <p>直观上，\(w = 0\) 时退化为标准条件采样；\(w &gt; 0\) 时，增强条件分数并减弱无条件分数，从而提高样本质量（更符合条件 \(c\)），但降低多样性。</p> <p><img src="/images/2025-12-25-diffusion/image-20250930110925539.png" alt="image-20250930110925539" class="img-fluid"/></p> <p>该算法中，step 3的解释如下，见[2]</p> <p><img src="/images/2025-12-25-diffusion/image-20250930111119169.png" alt="image-20250930111119169" class="img-fluid"/></p> <p>step 4 中，\(\tilde{x}_t\) 是当前时间步的去噪数据估计，基于引导分数 \(\tilde{\epsilon}_t\)。它表示在条件 \(c\) 引导下，模型预测的原始数据 \(x\)。</p> <p>step 5中，计算均值 \(\tilde{\mu}_t\): \(\tilde{\mu}_t = \alpha_{\lambda_{t+1}} \tilde{x}_t + \sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\)</p> <p>意义：\(\tilde{\mu}_t\) 是条件反向分布 \(p(z_{\lambda_{t+1}} \mid z_{\lambda_t}, c) = \mathcal{N}(z_{\lambda_{t+1}}; \tilde{\mu}_t, \sigma_{\lambda_{t+1}}^2 I)\) 的均值，引导采样朝符合条件 \(c\) 的方向移动。 作为 \(z_t\) 和 \(\tilde{x}_t\) 的函数：公式明确显示 \(\tilde{\mu}_t\) 依赖于当前样本 \(z_t\) 和引导预测器 \(\tilde{x}_t\)。其中：</p> <p>\(\alpha_{\lambda_{t+1}} \tilde{x}_t\): 缩放后的去噪估计，代表信号部分。 \(\sigma_{\lambda_{t+1}} \frac{\alpha_{\lambda_t} \tilde{x}_t - z_t}{\sigma_{\lambda_t}}\): 噪声调整项，通过 \(\tilde{x}_t\) 和 \(z_t\) 的差值引入引导分数的影响。</p> <h3 id="简化采样">简化采样</h3> <p>实际编程中，没有按algorithm2这么复杂。还是按 DDPM 简化Loss的采样过程，不过把 \(\epsilon\) 替换成引导的 \(\tilde{\epsilon}\) 。</p> <h2 id="ddim">DDIM</h2> <p>应该可以肯定的是，DDIM和DDPM的训练过程是一样的，采样过程不一样。</p> <p>但在理论上，forward process（本论文称为 inference）不一样。DDIM引入了非马尔可夫forward process。</p> <h3 id="non-markovian-forward-processes">NON-MARKOVIAN FORWARD PROCESSES</h3> <p>前向过程是一个随机过程，用联合概率密度分布表示。由于在DDPM中，训练过程是在每一个 \(t\)，让神经网络根据 \(x_t\) 去预测分数\(\nabla_{x_t} \log p(x_t \mid x_0)\) （等价于预测 \(x_0\)，或 \(\epsilon(x_t,t)\)）。这个训练过程只和 \(q(x_t\mid x_0)\) 有关，和联合概率密度分布没有关系，实际上有无数联合概率密度分布，可以获得这个边际分布。DDPM定义了其中的具有马尔可夫性的一种。</p> <p>DDIM论文直接设计另一种联合概率分布。给定一个实数向量 \(\sigma \in R^T_{\ge 0}\)，定义联合概率密度分布：</p> <p><img src="/images/2025-12-25-diffusion/image-20251003144728914.png" alt="image-20251003144728914" class="img-fluid"/></p> <p>论文证明，上面的这个联合概率分布，满足 \(q_\sigma(x_t\mid x_0)=N(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) 也就是和DDPM中相同。所以训练过程和 DDPM 一样。</p> <p>这个联合概率分布的设计中，直接给出了后向概率分布 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)，这样在采样时，就可以直接按照这个概率分布进行采样。</p> <blockquote> <p>The magnitude of \(\sigma\) controls the how stochastic the forward process is; when \(\sigma=0\), we reach an extreme case where as long as we observe x0 and xt for some t, then \(x_{t-1}\) become known and fixed.</p> </blockquote> <p>在上面的讨论中，并没有给出潜在的前向过程，也即是如何从 \(x_0\) 逐步推理得到 \(x_T\)。论文说这个过程可以由贝叶斯公式推导得到 \(q_\sigma(x_{t}\mid x_{t-1},x_0)\)，它是一个高斯分布。由于有依赖 \(x_0\)，它不是一个马尔可夫过程。熟悉DDPM的话，可以知道，在训练时，实际上并不需要去执行这个前向过程。</p> <h3 id="sampling-from-generalized-generative-processes">SAMPLING FROM GENERALIZED GENERATIVE PROCESSES</h3> <p>前面说过，生成过程可以直接借助 \(q_\sigma(x_{t-1}\mid x_t,x_0)\)</p> <blockquote> <p>Intuitively, given a noisy observation \(x_t\), we first make a prediction of the corresponding \(x_0\), and then use it to obtain a sample \(x_{t-1}\) through the reverse conditional distribution \(q_\sigma(x_{t-1}\mid x_t,x_0)\), which we have defined.</p> </blockquote> <p>如果我们采用simple loss预测 \(\epsilon\)，则</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153043856.png" alt="image-20251003153043856" class="img-fluid"/></p> <p>上文中，\(f_\theta^t\) 就是对 \(x_0\) 的预测。把式（9）中\(f_\theta^t\)代入式(10)，可得</p> <p><img src="/images/2025-12-25-diffusion/image-20251003153420286.png" alt="image-20251003153420286" class="img-fluid"/></p> <p>上式就是采样过程。其中\(\epsilon_\theta^{(t)}\) 是神经网络的输出。某一个特定的 \(\alpha\)，则退回成DDPM。</p> <p>而 \(\alpha_t=0\)，即为 DDIM。</p> <h3 id="加速采样过程">加速采样过程</h3> <p>上述讨论已经定义了DDIM，但还没涉及到加速采样。加速采样，也有类似的理论，但是训练过程不变，因此也要满足 marginal 要和 DDPM 一样。</p> <p>假设我们已经选择了时间的子集 \(\tau\)，\(\tau_S=T\)；定义联合概率密度</p> <p><img src="/images/2025-12-25-diffusion/image-20251003154717877.png" alt="image-20251003154717877" class="img-fluid"/></p> <p>也就是说， \(\tau\) 内的时间满足上面的非马尔可夫inference过程，其他时间是一个星形的概率图结构。由于marginal 要和 DDPM 一样，所以训练不变。</p> <p>在生成时，只用到了 \(\tau\) 内的时间，其他时间就不管。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Gaussiansplatting</title><link href="https://cekxm.github.io/blog/2025/gaussianSplatting/" rel="alternate" type="text/html" title="Gaussiansplatting"/><published>2025-12-25T12:57:46+00:00</published><updated>2025-12-25T12:57:46+00:00</updated><id>https://cekxm.github.io/blog/2025/gaussianSplatting</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/gaussianSplatting/"><![CDATA[<p>参考资料：</p> <p>[1] https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362/</p> <p>[2] grok</p> <p>代码：</p> <p>[1] <a href="https://github.com/yzslab/gaussian-splatting-lightning">GitHub - yzslab/gaussian-splatting-lightning: A 3D Gaussian Splatting framework with various derived algorithms and an interactive web viewer</a></p> <p>[2] <a href="https://github.com/nerfstudio-project/gsplat">GitHub - nerfstudio-project/gsplat: CUDA accelerated rasterization of gaussian splatting</a>: gsplat is an open-source library for CUDA accelerated rasterization of gaussians with python bindings.</p> <h1 id="3d-gaussian-splatting">3D Gaussian Splatting</h1> <p>本文是grok 关于gaussian splatting 反向传播的回答，但是第一部分描述了前向过程。即</p> <p>从高斯参数 → 投影 → alpha 混合 → 渲染图像</p> <p>3D Gaussian Splatting (3DGS) 的反向传播（back propagation, BP）是其训练核心，用于优化高斯点云的参数（如位置 \(\mu\), 协方差 \(\Sigma\), 不透明度 \(\alpha\), 球谐系数 SH）。与 NeRF 的 MLP 优化不同，3DGS 使用<strong>显式参数</strong>（点云属性），通过可微分渲染（differentiable rendering）计算梯度，结合 PyTorch 的 autograd 实现 BP。以下我详细解释 3DGS 的反向传播机制，结合 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库的代码（基于你的使用场景），从数学到实现，尽量图文并茂（文字描述模拟图形），并回答如何在代码中观察 BP。</p> <hr/> <h4 id="1-反向传播的总体流程">1. <strong>反向传播的总体流程</strong></h4> <p>3DGS 的训练目标是通过多视图图像（e.g., lego 数据集的 PNG + transforms.json）监督，优化高斯参数，最小化渲染图像与真实图像的损失（如 L1 + SSIM）。BP 过程涉及：</p> <ol> <li><strong>前向传播</strong>：从高斯参数 → 投影 → alpha 混合 → 渲染图像。</li> <li><strong>损失计算</strong>：渲染图像与 GT（ground truth）图像比较。</li> <li><strong>反向传播</strong>：通过 autograd 计算梯度，更新高斯参数。</li> <li><strong>密度控制</strong>：非 BP 部分，定期调整点云（克隆/分裂/剔除）。</li> </ol> <p><strong>图示（文字模拟）</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯参数: μ, Σ, α, SH] → [投影: 2D means/scales] → [alpha 混合: RGB] → [Loss: L1+SSIM]
    ↑ (梯度流回)            ↑ (梯度流回)             ↑ (梯度流回)         ← [GT 图像]
</code></pre></div></div> <hr/> <h4 id="2-数学原理可微分渲染与梯度计算">2. <strong>数学原理：可微分渲染与梯度计算</strong></h4> <p>3DGS 的渲染管道是可微分的，梯度通过链式法则从损失函数流回高斯参数。以下拆解关键步骤：</p> <h5 id="21-前向传播渲染"><strong>2.1 前向传播（渲染）</strong></h5> <ul> <li><strong>输入</strong>：N 个高斯，参数为： <ul> <li>\(\mu_i \in \mathbb{R}^3\)：中心位置。</li> <li>\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)：协方差矩阵（分解为缩放 \(S_i\) 和旋转 \(R_i\)）。</li> <li>\(\alpha_i \in [0,1]\)：不透明度（sigmoid 输出）。</li> <li>SH 系数：视图相关颜色 \(\mathbf{c}_i(\mathbf{d})\)（球谐函数）。</li> </ul> </li> <li><strong>投影（Splatting）</strong>： <ul> <li>每个高斯投影到 2D 图像平面： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(P\): 相机投影矩阵（内参）。</li> <li>\(V\): 视图矩阵（外参）。</li> <li>\(J\): 投影的雅可比矩阵（透视效应）。</li> <li>\(W\): 视图旋转部分。</li> </ul> </li> <li>2D 高斯权重：\(w_i(u,v) = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D}))\)。</li> </ul> </li> <li><strong>Alpha 混合</strong>： <ul> <li>像素颜色： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(T_i\): 透射率，模拟光线衰减。</li> <li>按深度排序（front-to-back）。</li> </ul> </li> </ul> </li> </ul> <h5 id="22-损失函数"><strong>2.2 损失函数</strong></h5> <ul> <li>比较渲染图像 \(C(\mathbf{p})\) 与 GT 图像 \(\hat{C}(\mathbf{p})\)： \(\mathcal{L} = \lambda_1 \cdot \|\hat{C} - C\|_1 + \lambda_2 \cdot (1 - \text{SSIM}(\hat{C}, C))\) <ul> <li>典型：\(\lambda_1 = 1, \lambda_2 = 0.2\)（见 <code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）。</li> </ul> </li> <li>损失是标量，驱动梯度计算。</li> </ul> <h2 id="投影和alpha混合的可微性">投影和alpha混合的可微性</h2> <hr/> <p>是的，在 <strong>3D Gaussian Splatting (3DGS)</strong> 中，<strong>投影（Projection/Splatting）</strong> 和 <strong>Alpha 混合（Alpha Blending）</strong> 两个步骤都是<strong>可微分的</strong>（differentiable）。这是 3DGS 能够通过反向传播（back propagation）优化高斯参数（如位置 \(\mu\)、协方差 \(\Sigma\)、不透明度 \(\alpha\)、球谐系数 SH）的关键原因。以下详细解释两者的可微性，结合数学推导、代码实现（基于 <strong>yzslab/gaussian-splatting-lightning</strong> 仓库），并以图文并茂的方式说明（文字模拟图形），回答你的问题。</p> <hr/> <h3 id="1-投影projectionsplatting的可微性">1. <strong>投影（Projection/Splatting）的可微性</strong></h3> <p><strong>投影</strong>步骤将 3D 高斯点云投影到 2D 图像平面，生成每个高斯的 2D 均值 \(\mu_{2D}\)、协方差 \(\Sigma_{2D}\) 和权重 \(w_i\)，用于后续渲染。这个过程是可微的，因为所有变换（矩阵运算、指数函数等）都支持梯度计算。</p> <h4 id="11-数学原理"><strong>1.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>3D 高斯参数：\(\mu_i \in \mathbb{R}^3\)（中心位置）、\(\Sigma_i \in \mathbb{R}^{3 \times 3}\)（协方差，分解为旋转 \(R_i\) 和缩放 \(S_i\)）、\(\alpha_i \in [0,1]\)（不透明度）。</li> <li>相机参数：视图矩阵 \(V\)（外参，4x4）、投影矩阵 \(P\)（内参，透视投影）。</li> </ul> </li> <li><strong>投影公式</strong>： <ol> <li><strong>均值投影</strong>： \(\mu_{i,2D} = P \cdot V \cdot \mu_i\) <ul> <li>\(V \cdot \mu_i\)：将 3D 位置从世界坐标系变换到相机坐标系。</li> <li>\(P\)：透视投影（e.g., \(P = \begin{bmatrix} f_x/Z &amp; 0 &amp; c_x \\ 0 &amp; f_y/Z &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)），输出 2D 像素坐标。</li> <li><strong>可微性</strong>：矩阵乘法是线性操作，透视除法 (\(/Z\)) 是可微的（雅可比矩阵 \(J\) 提供导数）。</li> </ul> </li> <li><strong>协方差投影</strong>： \(\Sigma_{i,2D} = J \cdot W \cdot \Sigma_i \cdot W^T \cdot J^T\) <ul> <li>\(W\)：视图矩阵 \(V\) 的 3x3 旋转部分。</li> <li>\(J\)：投影雅可比矩阵，考虑透视畸变： \(J = \begin{bmatrix} \frac{f_x}{Z} &amp; 0 &amp; -\frac{f_x X}{Z^2} \\ 0 &amp; \frac{f_y}{Z} &amp; -\frac{f_y Y}{Z^2} \end{bmatrix}\)</li> <li>\(\Sigma_i = R_i S_i S_i^T R_i^T\)：3D 协方差分解。</li> <li><strong>可微性</strong>：矩阵乘法、转置和 \(J\) 的计算（涉及除法）都可微，梯度通过链式法则流回 \(\Sigma_i\)（即 \(S_i, R_i\)）。</li> </ul> </li> <li><strong>高斯权重</strong>： \(w_i(u,v) = \alpha_i \cdot \exp\left(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\right)\) <ul> <li>\(\alpha_i\)：sigmoid 输出，可微。</li> <li>指数函数和矩阵逆（\(\Sigma_{i,2D}^{-1}\)）可微（指数导数为自身，矩阵逆导数基于线性代数）。</li> </ul> </li> </ol> </li> <li><strong>梯度流</strong>： <ul> <li>损失 \(\mathcal{L}\) 对像素颜色 \(C(\mathbf{p})\) 的梯度 \(\frac{\partial \mathcal{L}}{\partial C}\) 流回 \(w_i\)，再通过 \(w_i\) 流回 \(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i\)，最终到 \(\mu_i, \Sigma_i, \alpha_i\)。</li> </ul> </li> </ul> <h4 id="12-代码实现yzslab"><strong>1.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means3D</span> <span class="o">=</span> <span class="n">means</span>  <span class="c1"># [N, 3]
</span>    <span class="n">view_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">view</span>  <span class="c1"># [4, 4]
</span>    <span class="n">proj_matrix</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">proj</span>  <span class="c1"># [3, 4]
</span>    <span class="n">means2D</span> <span class="o">=</span> <span class="n">proj_matrix</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span> <span class="o">@</span> <span class="n">means3D</span><span class="p">)</span>  <span class="c1"># 矩阵乘法
</span>    <span class="n">J</span> <span class="o">=</span> <span class="nf">compute_jacobian</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">.</span><span class="n">focal</span><span class="p">)</span>  <span class="c1"># 雅可比
</span>    <span class="n">cov3D</span> <span class="o">=</span> <span class="nf">scales_to_cov</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">rotations</span><span class="p">)</span>  <span class="c1"># [N, 3, 3]
</span>    <span class="n">cov2D</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="p">(</span><span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">cov3D</span> <span class="o">@</span> <span class="n">view_matrix</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">].</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">J</span><span class="p">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">torch.matmul</code>（矩阵乘法）可微。</li> <li><code class="language-plaintext highlighter-rouge">compute_jacobian</code> 计算 \(J\)（涉及除法 <code class="language-plaintext highlighter-rouge">/Z</code>），PyTorch autograd 自动处理。</li> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 CUDA 实现的，可微分内核，基于 C++ 和 PyBind11（见 gsplat 源码）。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">viewer.py</code> 或 <code class="language-plaintext highlighter-rouge">render.py</code> 加断点，打印 <code class="language-plaintext highlighter-rouge">means2D.grad</code> 检查梯度。</li> </ul> <h4 id="13-图示文字模拟"><strong>1.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3D 高斯 [μ, Σ, α] → [矩阵乘法: V, P] → [2D 均值 μ_2D] → [雅可比 J 变换] → [2D 协方差 Σ_2D]
                                                    ↓
                                                  [高斯权重 w_i: exp(-x^T Σ_2D^-1 x)]
梯度流: Loss ← w_i ← μ_2D, Σ_2D ← μ, Σ, α
</code></pre></div></div> <hr/> <h3 id="2-alpha-混合alpha-blending的可微性">2. <strong>Alpha 混合（Alpha Blending）的可微性</strong></h3> <p><strong>Alpha 混合</strong>将投影后的 2D 高斯按深度排序，累积颜色形成像素值。这个过程也是可微的，因为涉及的排序、乘法和累积操作支持梯度计算。</p> <h4 id="21-数学原理"><strong>2.1 数学原理</strong></h4> <ul> <li><strong>输入</strong>： <ul> <li>每个高斯的 2D 参数：\(\mu_{i,2D}, \Sigma_{i,2D}, \alpha_i, \mathbf{c}_i\)（颜色，SH 插值）。</li> <li>像素坐标 \(\mathbf{p} = (u,v)\)。</li> </ul> </li> <li><strong>混合公式</strong>： \(C(\mathbf{p}) = \sum_{i=1}^N \mathbf{c}_i \cdot w_i \cdot T_i, \quad T_i = \prod_{j=1}^{i-1} (1 - w_j)\) <ul> <li>\(w_i = \alpha_i \cdot \exp(-\frac{1}{2} (\mathbf{p} - \mu_{i,2D})^T \Sigma_{i,2D}^{-1} (\mathbf{p} - \mu_{i,2D})\)：高斯权重。</li> <li>\(T_i\)：透射率，模拟光线穿过前 i-1 个高斯的衰减。</li> <li>按深度 \(z_i\)（相机坐标 Z）排序（front-to-back）。</li> </ul> </li> <li><strong>可微性</strong>： <ul> <li><strong>颜色</strong>：\(\frac{\partial C}{\partial \mathbf{c}_i} = w_i \cdot T_i\)，线性操作，可微。</li> <li><strong>权重</strong>：\(w_i\) 依赖 \(\alpha_i, \mu_{i,2D}, \Sigma_{i,2D}\)，已证明可微（见投影）。</li> <li><strong>透射率</strong>：\(T_i = \prod_{j=1}^{i-1} (1 - w_j)\)，乘法链可微，导数： \(\frac{\partial T_i}{\partial w_k} = -T_i \cdot \frac{1}{1 - w_k} \quad (k &lt; i)\)</li> <li><strong>排序</strong>：深度排序在渲染中是离散操作（不可微），但 3DGS 用 <strong>tile-based rasterization</strong>（分块光栅化），梯度只流经数值计算（不涉及排序本身），通过 CUDA 实现（如 gsplat 的 radix sort）。</li> </ul> </li> </ul> <h4 id="22-代码实现yzslab"><strong>2.2 代码实现（yzslab）</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span>  <span class="c1"># 排序+混合
</span>    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>可微性</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code> 是 C++/CUDA 实现，内部计算 alpha 混合的梯度。</li> <li>排序在 tile 内（16x16 像素块），梯度流经 \(w_i, T_i\)，不依赖排序顺序。</li> </ul> </li> <li><strong>调试</strong>：在 <code class="language-plaintext highlighter-rouge">training_step()</code> 打印 <code class="language-plaintext highlighter-rouge">rgb.grad</code>： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rendered</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">renderer</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">camera</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">])</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rendered</span><span class="p">[</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">].</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 像素梯度
</span></code></pre></div> </div> </li> </ul> <h4 id="23-图示文字模拟"><strong>2.3 图示（文字模拟）</strong></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[高斯: c_i, w_i] → [排序: z_i] → [T_i = Π(1-w_j)] → [C = Σ(c_i * w_i * T_i)] → Loss
梯度流: Loss ← C ← c_i, w_i ← α_i, μ_i, Σ_i
</code></pre></div></div> <h2 id="排序">排序</h2> <h3 id="1-排序的频率多久进行一次">1. <strong>排序的频率：多久进行一次？</strong></h3> <p>在 3DGS 中，<strong>排序</strong>是光栅化（rasterization）的一部分，具体用于按深度（z 值）对高斯点进行排序（front-to-back），以正确执行 alpha 混合。排序的频率直接取决于<strong>渲染的调用频率</strong>，因为每次渲染都需要对高斯点进行排序。</p> <h4 id="11-排序的时机"><strong>1.1 排序的时机</strong></h4> <ul> <li><strong>渲染触发</strong>： <ul> <li>排序发生在每次<strong>前向传播</strong>（forward pass）中，即每次调用 <code class="language-plaintext highlighter-rouge">render()</code> 函数时（包括训练、验证和渲染阶段）。</li> <li>在训练中，<code class="language-plaintext highlighter-rouge">render()</code> 由 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用，每处理一个批次（batch）数据（一张或多张 lego 图像）就渲染一次，因此排序也执行一次。</li> <li><strong>频率</strong>：<strong>每训练一步（step）</strong>，即每个 batch 都会进行一次排序。</li> </ul> </li> <li><strong>训练频率</strong>： <ul> <li>你的训练日志显示 300 个 epoch，30,000 步（<code class="language-plaintext highlighter-rouge">epoch=300-step=30000.ckpt</code>），每 epoch 100 个 batch（lego 数据集 <code class="language-plaintext highlighter-rouge">train/</code> 有 100 张图像，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 默认 batch_size=1）。</li> <li><strong>排序总次数</strong>：30,000 次（每 step 一次）。</li> <li><strong>时间开销</strong>：排序由 CUDA 加速（gsplat 使用 radix sort），单次排序 ~0.1-1 毫秒（RTX 3090，~100k 高斯），占渲染时间 &lt;10%。</li> </ul> </li> <li><strong>验证/渲染阶段</strong>： <ul> <li>验证（validation）：每 <code class="language-plaintext highlighter-rouge">val_interval</code>（默认 1000 步，<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code>）渲染验证图像，触发排序。</li> <li>手动渲染：运行 <code class="language-plaintext highlighter-rouge">python render.py --ckpt_path outputs/lego_test/checkpoints/epoch=300-step=30000.ckpt</code>，每次生成一张图像或视频帧都排序一次（e.g., 100 帧视频 → 100 次排序）。</li> </ul> </li> <li><strong>Viewer</strong>：运行 <code class="language-plaintext highlighter-rouge">python viewer.py --ply_path outputs/lego_test/checkpoints/epoch=300-step=30000-xyz_rgb.ply</code>，交互式查看时，每帧实时渲染，排序随帧率（如 60 FPS → 每秒 60 次）。</li> </ul> <h4 id="12-代码中的排序"><strong>1.2 代码中的排序</strong></h4> <ul> <li><strong>文件</strong>：<code class="language-plaintext highlighter-rouge">src/render/gs_renderer.py</code>，调用 <code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians()</code>。</li> <li><strong>关键代码</strong>（简化）： <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gsplat</span>
<span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">gaussians</span><span class="p">,</span> <span class="n">camera</span><span class="p">):</span>
    <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span> <span class="o">=</span> <span class="nf">project_gaussians</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">means</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">scales</span><span class="p">,</span> <span class="n">camera</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">gsplat</span><span class="p">.</span><span class="nf">rasterize_gaussians</span><span class="p">(</span>
        <span class="n">means2D</span><span class="p">,</span> <span class="n">cov2D</span><span class="p">,</span> <span class="n">opacities</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">colors</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">16</span>
    <span class="p">)</span>  <span class="c1"># 内部排序
</span>    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">rgb</span><span class="sh">"</span><span class="p">:</span> <span class="n">rgb</span><span class="p">}</span>
</code></pre></div> </div> </li> <li><strong>排序位置</strong>： <ul> <li><code class="language-plaintext highlighter-rouge">gsplat.rasterize_gaussians</code>（gsplat 库，<code class="language-plaintext highlighter-rouge">gsplat/_torch_impl.py</code> 和 <code class="language-plaintext highlighter-rouge">cuda/rasterize.cu</code>）在每个 tile（16x16 像素块）内按深度 \(z_i\)（从 3D 均值 \(\mu_i\) 投影得到）排序。</li> <li>代码伪逻辑（CUDA）： <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// cuda/rasterize.cu</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">tile</span><span class="o">:</span>
    <span class="n">compute_depths</span><span class="p">(</span><span class="n">means3D</span><span class="p">,</span> <span class="n">camera</span><span class="p">);</span>  <span class="c1">// z_i = (V * μ_i).z</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">radix_sort</span><span class="p">(</span><span class="n">depths</span><span class="p">);</span>     <span class="c1">// 按 z_i 排序</span>
    <span class="n">accumulate_colors</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">colors</span><span class="p">);</span> <span class="c1">// alpha 混合</span>
</code></pre></div> </div> </li> </ul> </li> <li><strong>频率</strong>：每个 batch 的 <code class="language-plaintext highlighter-rouge">training_step()</code> 调用 <code class="language-plaintext highlighter-rouge">render()</code>，触发一次排序（~100 次/epoch）。</li> </ul> <h4 id="13-频率总结"><strong>1.3 频率总结</strong></h4> <ul> <li><strong>训练</strong>：每 step（batch）排序一次，30,000 步 → 30,000 次排序。</li> <li><strong>验证</strong>：每 1000 步（可调，<code class="language-plaintext highlighter-rouge">val_interval</code>），~30 次。</li> <li><strong>渲染/Viewer</strong>：每帧一次（视频 100 帧 → 100 次，viewer 60 FPS → 60 次/秒）。</li> <li><strong>配置调整</strong>：<code class="language-plaintext highlighter-rouge">configs/blender.yaml</code> 的 <code class="language-plaintext highlighter-rouge">batch_size</code> 或 <code class="language-plaintext highlighter-rouge">val_interval</code> 影响频率，但排序本身不可跳过（alpha 混合依赖正确顺序）。</li> </ul> <hr/> <h3 id="2-排序的可微性排序是可微的吗">2. <strong>排序的可微性：排序是可微的吗？</strong></h3> <p><strong>简答</strong>：排序（sorting）是<strong>不可微的</strong>（non-differentiable），因为它是一个离散操作（改变高斯索引顺序），无法定义连续的导数。但在 3DGS 中，排序不影响反向传播的可微性，因为梯度流绕过了排序步骤，只依赖数值计算（权重 \(w_i\) 和透射率 \(T_i\)）。这与投影和 alpha 混合的可微性不同。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">SFM MVS Rendering</title><link href="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/" rel="alternate" type="text/html" title="SFM MVS Rendering"/><published>2025-12-25T12:25:10+00:00</published><updated>2025-12-25T12:25:10+00:00</updated><id>https://cekxm.github.io/blog/2025/sfm-mvs-rendering</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/sfm-mvs-rendering/"><![CDATA[<h2 id="资源">资源</h2> <ol> <li>https://www.youtube.com/watch?v=diBxFGgqAT0</li> <li>https://mashaan14.github.io/YouTube-channel/nerf/2025_01_25_sfm</li> </ol> <p>这个资源解释了 SFM，two view Geometry. 结合camera_and_stereo.pdf 进行理解。</p> <h2 id="多视角几何mvs新视角生成nvs对比">多视角几何（MVS），新视角生成（NVS）对比</h2> <p><img src="/images/2025-12-25-sfm-mvs-rendering/d3dcd8d3-35cb-403b-998d-64256b21ba06.png" alt="drawings-02 001" style="zoom:50%;"/></p> <p>简而言之：<strong>(MVS)SfM</strong> 是传统的“几何尺子”；<strong>VGGT</strong> 是现代的“几何大模型”；而 <strong>NVS</strong> 是“虚拟照相机”。</p> <h3 id="mvssfm-vs-vggt-vs-nvs-nerfgs-综合对比表">(MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>(MVS) SfM (如 COLMAP)</strong></th> <th><strong>VGGT (Visual Geometry Grounded Transformer)</strong></th> <th><strong>NVS (NeRF / 3D GS)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>核心任务目标</strong></td> <td><strong>三维重建</strong>：求解精确的相机姿态和场景几何（点云/深度）。</td> <td><strong>统一几何推断</strong>：一站式、秒级预测相机、点图、深度图和追踪。</td> <td><strong>新视角合成</strong>：在未拍摄过的角度生成照片级逼真的图像。</td> </tr> <tr> <td><strong>底层表示</strong></td> <td><strong>离散几何</strong>：稀疏或稠密的 3D 点云、深度图。</td> <td><strong>稠密几何图</strong>：Point Maps (\(H \times W \times 3\)) 和 Depth Maps。</td> <td><strong>光场表示</strong>：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。</td> </tr> <tr> <td><strong>运行机制</strong></td> <td><strong>优化驱动</strong>：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。</td> <td><strong>前馈推理 (Feed-forward)</strong>：一次性通过 Transformer 模型直接“看”出几何。</td> <td><strong>视图对齐优化</strong>：通过渲染结果与原图的颜色误差来反向训练。</td> </tr> <tr> <td><strong>对相机的需求</strong></td> <td><strong>未知或已知</strong>：通常用于解算未知相机参数。</td> <td><strong>无需预设</strong>：直接预测相机的 9 维参数（内外参）。</td> <td><strong>必须已知</strong>：通常依赖 SfM (COLMAP) 提供位姿初始化。</td> </tr> <tr> <td><strong>几何精确度</strong></td> <td><strong>高（度量级）</strong>：数学推导严谨，但易受弱纹理、模糊影响。</td> <td><strong>高且鲁棒</strong>：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。</td> <td><strong>中/低</strong>：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。</td> </tr> <tr> <td><strong>渲染视觉效果</strong></td> <td><strong>差</strong>：只有离散点，无法生成连续、真实的图像。</td> <td><strong>中</strong>：提供稠密几何，但主要用于几何任务而非美学渲染。</td> <td><strong>极高</strong>：支持照片级渲染、阴影、反射和透明效果。</td> </tr> <tr> <td><strong>处理速度</strong></td> <td><strong>极慢</strong>：通常需要数分钟到数小时。</td> <td><strong>极快</strong>：全流程通常在 <strong>1 秒以内</strong>。</td> <td><strong>训练慢/渲染快</strong>：NeRF 训练慢，GS 渲染极快，但都需要初始化。</td> </tr> </tbody> </table> <hr/> <h2 id="colmap">COLMAP</h2> <p><img src="/images/2025-12-25-sfm-mvs-rendering/35285f8e-982b-4ad8-96f7-1c4c5d09b5c7.png" alt="image"/></p> <p>在 COLMAP 的三维重建流水线中，<strong>BA（Bundle Adjustment，捆绑调整）</strong> 是最核心的优化环节。它的基本原理是通过<strong>非线性最小二乘法</strong>，同时优化相机的姿态、内参以及三维点的坐标，从而使重建结果达到最高的几何一致性。</p> <hr/> <h3 id="1-ba-的数学原理">1. BA 的数学原理</h3> <p>BA 的本质是一个<strong>重投影误差（Reprojection Error）</strong>最小化问题。</p> <h3 id="重投影误差公式">重投影误差公式</h3> <p>假设我们有 \(n\) 个三维点 \(X_j\) 和 \(m\) 张图像。对于每一张图像 \(i\) 和它观察到的点 \(j\)，其观测到的像素坐标为 \(x_{ij}\)。而根据当前估计的相机姿态 \(R_i, t_i\)、内参 \(K_i\) 和点 \(X_j\) 计算出的<strong>理论投影坐标</strong>为 \(\hat{x}_{ij} = \pi(K_i, R_i, t_i, X_j)\)。</p> <p>BA 的目标函数可以表示为：</p> \[\min_{K_i, R_i, t_i, X_j} \sum_{i=1}^m \sum_{j=1}^n \rho \left( \| x_{ij} - \pi(K_i, R_i, t_i, X_j) \|^2 \right)\] <ul> <li><strong>\(\pi\)</strong>：投影函数（将 3D 点映射到 2D 像素平面）。</li> <li><strong>\(\| x_{ij} - \hat{x}_{ij} \|^2\)</strong>：重投影误差。</li> <li><strong>\(\rho\)</strong>：鲁棒核函数（如 Huber loss），用于减少外点（错误匹配）对优化的干扰。</li> </ul> <h3 id="优化算法">优化算法</h3> <p>由于投影方程是非线性的，COLMAP 调用 <strong>Ceres Solver</strong> 库，使用 <strong>Levenberg-Marquardt (LM)</strong> 算法进行迭代求解。</p> <hr/> <h3 id="2-ba-在-colmap-中的作用">2. BA 在 COLMAP 中的作用</h3> <p>BA 贯穿于 COLMAP 的增量式重建（Incremental SfM）全过程，主要起到以下作用：</p> <ul> <li><strong>消除累积误差（去漂移）</strong>：在逐张添加图像的过程中，误差会不断累积。BA 能够通过全局约束，将这些微小的误差平摊，防止模型“弯曲”或“变形”。</li> <li><strong>精细化参数</strong>：初始的相机姿态通常由 PnP 算法得到，三维点坐标由三角化得到。BA 能够将这些“毛坯”数据进一步精细化，提高点云的精度。</li> <li><strong>自标定（Self-Calibration）</strong>：如果相机内参未知或不准，BA 可以通过优化过程自动校正焦距（Focal Length）和畸变参数。</li> <li><strong>剔除外点</strong>：在 BA 优化后，重投影误差依然很大的点会被视为无效点（Outliers）并被剔除，从而保证重建的鲁棒性。</li> </ul> <hr/> <h3 id="3-colmap-中的两种-ba-策略">3. COLMAP 中的两种 BA 策略</h3> <p>为了平衡计算速度和精度，COLMAP 将 BA 分为两个级别：</p> <table> <thead> <tr> <th><strong>类型</strong></th> <th><strong>触发时机</strong></th> <th><strong>优化范围</strong></th> <th><strong>目的</strong></th> </tr> </thead> <tbody> <tr> <td><strong>局部 BA (Local BA)</strong></td> <td>每注册一张新图像后</td> <td>当前图像及其邻近图像、可见的三维点</td> <td>确保新加入的图像能够稳定地融合进当前模型。</td> </tr> <tr> <td><strong>全局 BA (Global BA)</strong></td> <td>模型增长达到一定比例时</td> <td><strong>所有</strong>已注册的图像、<strong>所有</strong>三维点</td> <td>消除全局漂移，确保整个场景的几何结构严丝合缝。</td> </tr> </tbody> </table> <h3 id="4-估计的-k-r-t-x-是作为-ba-的初值吗">4. 估计的 \(K, R, t, X\) 是作为 BA 的初值吗？</h3> <p><strong>是的。</strong> BA 本质上是一个<strong>非线性优化</strong>过程，非线性优化必须依赖一个合理的<strong>初始值</strong>（Initial Guess）才能收敛到全局最优解，否则很容易陷入局部极小值。</p> <p>在 COLMAP 的增量式重建流程中，这些初值的来源如下：</p> <ul> <li><strong>相机姿态 (\(R, t\))</strong>：通过 <strong>PnP (Perspective-n-Point)</strong> 算法获得。当一张新图像被注册到现有模型时，系统利用已有的 3D 点和该图像中的 2D 特征匹配点，计算出该图像的位姿。</li> <li><strong>三维点坐标 (\(X\))</strong>：通过 <strong>三角化 (Triangulation)</strong> 获得。利用多张图像的相机中心和匹配特征射线的交点来估计点的空间位置。</li> <li><strong>内参 (\(K\))</strong>：通常来源于图像的 <strong>EXIF 信息</strong>（如焦距）或预设的相机模型。在 BA 过程中，这些参数可以被进一步精细化。</li> </ul> <p><strong>一句话总结：</strong> PnP 和三角化为 BA 提供了“毛坯”模型，而 BA 负责“精加工”。</p> <hr/> <h3 id="5-ba-是逐一优化还是同时优化">5. BA 是逐一优化还是同时优化？</h3> <p>BA 是<strong>同时优化（Joint Optimization）</strong>所有参数的。这意味着在同一个优化循环中，相机参数和三维点坐标是同时更新的。</p> <h4 id="为什么要同时优化">为什么要同时优化？</h4> <p>如果采用“逐一优化”（先固定点优化相机，再固定相机优化点），模型收敛速度会极慢，且容易陷入死循环或无法达到全局最优，这种方法被称为“坐标下降法”。</p> <h4 id="使用的数学技巧舒尔补-schur-complement">使用的数学技巧：舒尔补 (Schur Complement)</h4> <p>由于 BA 涉及成千上万个三维点和数百个相机位姿，直接求逆巨大的海森矩阵（Hessian Matrix）在计算上是不可行的。为了实现高效的<strong>同时优化</strong>，COLMAP（通过其背后的 <strong>Ceres Solver</strong>）使用了一个核心数学技巧：<strong>利用稀疏结构的舒尔补（Schur Complement）</strong>。</p> <h4 id="核心步骤">核心步骤：</h4> <ol> <li> <p>稀疏性分析：</p> <p>在 BA 中，一个 3D 点的投影只与观察到它的相机有关，与其他点无关；同样，一个相机的残差只与它看到的点有关。这导致其对应的正态方程（Normal Equations）具有极其显著的块稀疏结构。</p> \[\begin{bmatrix} B &amp; E \\ E^T &amp; C \end{bmatrix} \begin{bmatrix} \Delta x_c \\ \Delta x_p \end{bmatrix} = \begin{bmatrix} v \\ w \end{bmatrix}\] <p>其中 \(B\) 是相机参数块，\(C\) 是三维点参数块（它是对角块矩阵，求逆非常快）。</p> </li> <li> <p>边缘化（Marginalization）：</p> <p>利用舒尔补将三维点块 \(C\) 消去，得到一个只包含相机参数的缩减相机系统（Reduced Camera System）：</p> \[(B - EC^{-1}E^T) \Delta x_c = v - EC^{-1}w\] <p>这个矩阵的维度大大降低（仅取决于相机的数量），求解出相机的增量 \(\Delta x_c\) 后，再通过代回（Back-substitution）快速算出所有点的增量 \(\Delta x_p\)。</p> </li> </ol> <h4 id="其他关键技术">其他关键技术：</h4> <ul> <li><strong>Levenberg-Marquardt (LM) 算法</strong>：一种介于高斯-牛顿法和梯度下降法之间的信赖域方法，保证了在远离最优解时也能稳健收敛。</li> <li><strong>鲁棒核函数 (Robust Loss)</strong>：如 Huber 或 Cauchy 损失，用来自动降低那些误匹配点（Outliers）在优化中的权重，防止它们带偏整个模型。</li> </ul> <p>在 COLMAP 的流水线中，参与 BA 的图像数量并不是一个固定值，它取决于<strong>你所处的重建阶段</strong>以及<strong>场景的规模</strong>。</p> <p>COLMAP 作为一种<strong>增量式 SfM（Incremental SfM）</strong>工具，它通过两种不同规模的 BA 来平衡精度和效率：</p> <h3 id="6-局部-ba-local-bundle-adjustment">6. 局部 BA (Local Bundle Adjustment)</h3> <ul> <li><strong>图像数量：通常为 10 到 30 张左右。</strong></li> <li><strong>原理：</strong> 每当 COLMAP 成功注册（添加）一张新图像并进行三角化后，它不会立即对全场进行优化（太慢了），而是执行一次局部 BA。</li> <li><strong>范围：</strong> 仅包含<strong>当前图像及其在共视图中最近的邻居图像</strong>。</li> <li><strong>作用：</strong> 确保新加入的图像能够正确地锚定在现有模型上，防止局部结构发生剧烈畸变。</li> </ul> <h3 id="7-全局-ba-global-bundle-adjustment">7. 全局 BA (Global Bundle Adjustment)</h3> <ul> <li><strong>图像数量：所有已注册的图像（从几十张到上万张不等）。</strong></li> <li><strong>原理：</strong> 当模型增长到一定程度（例如图像数量增加了 10%，或者达到了特定的步长），COLMAP 会触发全局 BA。</li> <li><strong>范围：</strong> 优化当前已成功进入模型的所有相机位姿和所有 3D 点。</li> <li><strong>规模示例：</strong> <ul> <li><strong>小型物体：</strong> 50 - 200 张图像。</li> <li><strong>中型场景（如建筑）：</strong> 500 - 2,000 张图像。</li> <li><strong>大型城市/测绘：</strong> 5,000 - 10,000+ 张图像。</li> </ul> </li> <li><strong>作用：</strong> 消除长时间增量重建积累的“漂移”误差，确保模型整体的闭环精度。</li> </ul> <hr/> <h3 id="8-影响参与-ba-图像数量的瓶颈">8. 影响参与 BA 图像数量的瓶颈</h3> <p>虽然理论上可以有成千上万张图像参与 BA，但实际操作中受以下因素限制：</p> <ol> <li><strong>内存（RAM）与显存：</strong> <ul> <li>BA 需要构建庞大的 Jacobi 矩阵。虽然有“舒尔补”技巧减小计算量，但当图像超过 <strong>2,000-3,000 张</strong>时，对内存的需求会显著增加。</li> <li>COLMAP 默认使用 <strong>Ceres Solver</strong>，如果内存不足，BA 可能会失败或运行极其缓慢。</li> </ul> </li> <li><strong>计算时间：</strong> <ul> <li>全局 BA 是 SfM 中最耗时的部分。对于上万张图的场景，一次全局 BA 可能需要几小时甚至更久。</li> </ul> </li> <li><strong>连接性（Connectivity）：</strong> <ul> <li>如果图像之间没有共同的特征点（即没有“边”连接），这些图像就不会在同一个 BA 块中被优化。COLMAP 会将它们拆分成不同的子模型（Sub-models）。</li> </ul> </li> </ol> <h2 id="研究方向">研究方向</h2> <p>在 2025 年的时间节点上，<strong>Novel View Synthesis (NVS)</strong> 在学术热度和资本市场显然更“火”，但 <strong>Multi-View Stereo (MVS)</strong> 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。</p> <p>这两者并非孤立竞争，而是呈现出一种<strong>深度融合</strong>的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：</p> <hr/> <h3 id="1-热门程度novel-view-synthesis-nvs-占据-c-位">1. 热门程度：Novel View Synthesis (NVS) 占据 C 位</h3> <p><strong>核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D</strong></p> <ul> <li><strong>学术热度：</strong> 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 <strong>3DGS (3D 高斯溅射)</strong> 和 <strong>生成式新视角合成</strong> 的论文数量呈爆炸式增长。</li> <li><strong>AIGC 助力：</strong> 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 <strong>Generative NVS</strong>）成了最热门的方向。</li> <li><strong>用户感知度：</strong> NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。</li> </ul> <h3 id="2-发展前景mvs-正在向几何大模型进化">2. 发展前景：MVS 正在向“几何大模型”进化</h3> <p><strong>核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry</strong></p> <ul> <li><strong>从“工具”到“大脑”：</strong> 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 <strong>VGGT</strong> 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个<strong>前馈网络（Feed-forward）</strong>。</li> <li><strong>解决“不可能任务”：</strong> 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。</li> <li><strong>工业刚需：</strong> 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的<strong>精确绝对坐标（Metric Geometry）</strong>。</li> </ul> <hr/> <h3 id="3-2025-年的关键趋势两者边界的模糊融合">3. 2025 年的关键趋势：两者边界的模糊（融合）</h3> <p>如果你在考虑职业发展或研究方向，<strong>“几何感知的 NVS” (Geometry-aware NVS)</strong> 是 2025 年最具前景的方向。</p> <ol> <li><strong>MVS 为前，NVS 为后：</strong> 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。</li> <li><strong>可推广性 (Generalizability)：</strong> 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 <strong>LRM (Large Reconstruction Models)</strong>，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。</li> <li><strong>动态场景：</strong> 静态场景的重建基本解决，2025 年的蓝海是<strong>动态 4D 重建</strong>（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。</li> </ol> <hr/> <h3 id="总结建议">总结建议</h3> <ul> <li><strong>如果你追求“视觉震撼”和“快速产出”：</strong> 选择 <strong>Novel View Synthesis (尤其是 3DGS)</strong>。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。</li> <li><strong>如果你追求“底层技术”和“稳健性”：</strong> 选择 <strong>MVS/几何大模型</strong>。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。</li> <li><strong>最具潜力的路径：</strong> 研究<strong>如何将 MVS 的几何约束引入 NVS</strong>（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。</li> </ul> <p><strong>结论：</strong> <strong>NVS 更“热门”（Hotter）</strong>，但 <strong>MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）</strong>。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">冲激信号的尺度变换</title><link href="https://cekxm.github.io/blog/2025/impulse/" rel="alternate" type="text/html" title="冲激信号的尺度变换"/><published>2025-01-26T02:01:00+00:00</published><updated>2025-01-26T02:01:00+00:00</updated><id>https://cekxm.github.io/blog/2025/impulse</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/impulse/"><![CDATA[<h3 id="t0-处冲激信号的尺度变换">\(t=0\) 处冲激信号的尺度变换</h3> \[\delta(at)=\frac{1}{|a|}\delta(t)\] <p>也就是说，对单位冲激信号进行尺度变换，会改变它的强度。</p> <p>这可以从冲激信号的极限法定义来理解。单位冲激信号可以由面积是1的矩形通过缩小宽度而生成。</p> <p>对该矩形进行尺度变换，会改变它的面积。</p> <p><img src="/images/2025-01-26-impulse/image-20220222005046096.png" alt="image-20220222005046096" style="zoom: 33%;"/></p> <h3 id="tt_0-处冲激信号的尺度变换">\(t=t_0\) 处冲激信号的尺度变换</h3> <p>若冲激信号为 \(\delta(t-t_0)\)，对它进行尺度缩放，有 \(\delta(at-t_0)=\delta(a(t-\frac{t_0}{a}))=\frac{1}{|a|}\delta(t-\frac{t_0}{a}))\) 可见，冲激信号的位置改变了，强度也发生了改变。</p> <p>同样的，可以从冲激信号的极限法定义来理解。</p> <p><img src="/images/2025-01-26-impulse/image-20220222005140714.png" alt="image-20220222005140714" style="zoom: 33%;"/></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">信号与系统课程的数学知识</title><link href="https://cekxm.github.io/blog/2025/math/" rel="alternate" type="text/html" title="信号与系统课程的数学知识"/><published>2025-01-26T00:01:00+00:00</published><updated>2025-01-26T00:01:00+00:00</updated><id>https://cekxm.github.io/blog/2025/math</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/math/"><![CDATA[<h3 id="基础">基础</h3> <p>本课程中，\(j\) 表示虚数单位，也就是高数中常用的 \(i\)。</p> <h4 id="欧拉公式">欧拉公式</h4> \[e^{j\varphi}=\cos \varphi +j \sin \varphi\] <p>由欧拉公式，可得 \(e^{jwt}=\cos(wt) +j \sin(wt)\)</p> \[\sin (wt) = \frac{1}{2j}(e^{jwt}-e^{-jwt})\] \[\cos (wt) = \frac{1}{2}(e^{jwt}+e^{-jwt})\] <blockquote> <p>例：展开 \(e^{(3-2j)t}\) \(e^{(3-2j)t}=e^{3t}e^{-j2t}=e^{3t}(\cos (2t) -j \sin (2t))\)</p> </blockquote> <h4 id="复数">复数</h4> <h5 id="笛卡尔坐标形式和极坐标形式">笛卡尔坐标形式和极坐标形式</h5> \[z = x+jy=re^{j\varphi}\] <p>其中，\(r\) 为模，\(\varphi\) 为相位（或辐角）。</p> <p><img src="/images/2025-01-26-math/480px-Complex_number_illustration_modarg.svg.png" alt="480px-Complex_number_illustration_modarg.svg" style="zoom:50%;"/></p> <h5 id="相位">相位</h5> <p>相位 \(\varphi\in[0,2\pi)\)</p> <blockquote> <p>例</p> </blockquote> <p><img src="/images/2025-01-26-math/Argand3-1.jpg" alt="Argand3-1" style="zoom:50%;"/></p> <p>相位的计算方法是 \(\varphi = atan2(y,x)\) 也就是<img src="/images/2025-01-26-math/argument.svg" alt="argument" style="zoom: 67%;"/></p> <p>但很多文献中，不太严格地写为 \(\varphi =\arctan (\frac{y}{x})\)</p> <h5 id="极坐标形式下的乘法除法">极坐标形式下的乘法、除法</h5> <p>乘法：模相乘，相位相加 \(r_1 e^{j\varphi_1 }\cdot r_2 e^{j\varphi_2 }=r_1 r_2 e^{j(\varphi_1+\varphi_2) }\) 除法：模相除，相位相减 \(\frac{r_1 e^{j\varphi_1 }}{r_2 e^{j\varphi_2 }}=\frac{r_1 }{r_2 }e^{j(\varphi_1-\varphi_2) }\)</p> <blockquote> <p>例</p> </blockquote> <p><img src="/images/2025-01-26-math/finding-reciprocal-of-complex-number.png" alt="finding-reciprocal-of-complex-number" style="zoom: 50%;"/></p> <h4 id="分部积分">分部积分</h4> \[\int f(t)g'(t)dt=f(t)g(t)-\int f'(t)g(t)dt\] <blockquote> <p>例：证明冲激偶的性质 \(\int_{-\infty}^{\infty}\delta'(t)f(t)dt=-f'(0)\)</p> <p>证： \(\int_{-\infty}^{\infty}\delta'(t)f(t)dt=\delta(t)f(t)|_{-\infty}^{\infty}-\int_{-\infty}^{\infty}\delta(t)f'(t)dt\) 第一项为0，第二项根据冲激信号的抽样性质，等于 \(-f'(0)\)。</p> </blockquote> <h4 id="三角函数">三角函数</h4> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>诱导公式</td> <td>\(\sin (\theta+\pi)=-\sin \theta\), \(\cos (\theta+\pi)=-\cos \theta\)</td> </tr> <tr> <td> </td> <td>\(\sin (\theta+\pi/2)=\cos \theta\), \(\cos (\theta+\pi/2)=-\sin \theta\)</td> </tr> <tr> <td>和差化积</td> <td>\(\sin\alpha + \sin\beta=2\sin(\frac{\alpha+\beta}{2})\cos(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\sin\alpha- \sin\beta=2\cos(\frac{\alpha+\beta}{2})\sin(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha + \cos\beta=2\cos(\frac{\alpha+\beta}{2})\cos(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha - \cos\beta=-2\sin(\frac{\alpha+\beta}{2})\sin(\frac{\alpha-\beta}{2})\)</td> </tr> <tr> <td>积化和差</td> <td>\(\sin\alpha \cos\beta=\frac{1}{2}[\sin(\alpha+\beta)+\sin(\alpha-\beta)]\)</td> </tr> <tr> <td> </td> <td>\(\cos\alpha \cos\beta=\frac{1}{2}[\cos(\alpha+\beta)+\cos(\alpha-\beta)]\)</td> </tr> <tr> <td> </td> <td>\(\sin\alpha \sin\beta=\frac{1}{2}[\cos(\alpha+\beta)-\cos(\alpha-\beta)]\)</td> </tr> <tr> <td>两角和与差</td> <td> </td> </tr> </tbody> </table> <h3 id="冲激信号">冲激信号</h3> <h4 id="单位冲激信号-deltat">单位冲激信号 \(\delta(t)\)</h4> <p><strong>定义</strong>： \(\left\{ \begin{array}{**lr**} \int_{-\infty}^\infty \delta(t)dt=1 \\ \delta(t)=0, t\ne 0 \end{array} \right.\)</p> <blockquote> <p>例：求 \(\int_{-1}^5 \delta(t)dt\)</p> <p>由于 \(\delta(t)\) 在 \(t=0\) 以外的地方为 0，只要积分区间包含 \(t=0\)，则积分值等于 1. 所以答案为 1.</p> <p>也就是说，可以把积分区间缩小到 \(t=0\) 附近一个无穷小区间：</p> </blockquote> \[\int_{0_-}^{0_+} \delta(t)dt=1\] <p><strong>性质</strong>：</p> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>抽样性</td> <td>\(f(t)\delta(t)=f(0)\delta(t)\)</td> </tr> <tr> <td>抽样性</td> <td>\(\int_{-\infty}^\infty f(t)\delta(t)dt=f(0)\)</td> </tr> <tr> <td>抽样性</td> <td>\(f(t)\delta(t-t_0)=f(0)\delta(t-t_0)\)</td> </tr> <tr> <td>抽样性</td> <td>\(\int_{-\infty}^\infty f(t)\delta(t-t_0)dt=f(t_0)\)</td> </tr> <tr> <td>尺度变换</td> <td>\(\delta(at)=\frac{1}{\lvert a \rvert}\delta(t)\)</td> </tr> <tr> <td>尺度变换</td> <td>\(\delta(at-t_0)=\frac{1}{\lvert a \rvert}\delta(t-\frac{t_0}{a})\)</td> </tr> <tr> <td>偶函数</td> <td>\(\delta(-t)=\delta(t)\)</td> </tr> <tr> <td>积分</td> <td>\(\int_{-\infty}^t \delta(t)dt=u(t)\)</td> </tr> <tr> <td> </td> <td>一般的，\(\int_{-\infty}^t \delta(t-t_0)dt=u(t-t_0)\)</td> </tr> </tbody> </table> <h4 id="冲激偶信号-deltat">冲激偶信号 \(\delta'(t)\)</h4> <h3 id="卷积">卷积</h3> <table> <thead> <tr> <th>条目</th> <th>公式</th> </tr> </thead> <tbody> <tr> <td>交换律</td> <td>\(f_1(t)\star f_2(t)=f_2(t)\star f_1(t)\)</td> </tr> <tr> <td>分配律</td> <td>\(f_1(t)\star[f_2(t)+f_3(t)]=f_1(t)\star f_2(t)+f_1(t)\star f_3(t)\)</td> </tr> <tr> <td>结合律</td> <td>\([f_1(t)\star f_2(t)] \star f_3(t)=f_1(t)\star [f_2(t) \star f_3(t)]\)</td> </tr> <tr> <td>微积分性，令\(g(t)=f(t)\star h(t)\)</td> <td>\(g'(t)=f(t)\star h'(t)=f'(t)\star h(t)\)</td> </tr> <tr> <td> </td> <td>\(g^{(-1)}(t)=f(t)\star h^{(-1)}(t)=f^{(-1)}(t)\star h(t)\)</td> </tr> <tr> <td> </td> <td>\(g(t)=f(t)^{(1)}\star h^{(-1)}(t)=f^{(-1)}(t)\star h^{(1)}(t)\)</td> </tr> <tr> <td>延时</td> <td> </td> </tr> <tr> <td>与冲激信号卷积</td> <td> </td> </tr> <tr> <td>与阶跃信号卷积</td> <td> </td> </tr> </tbody> </table> <h3 id="傅里叶变换">傅里叶变换</h3> <table> <thead> <tr> <th>时域</th> <th>傅里叶变换</th> </tr> </thead> <tbody> <tr> <td>\(\delta(t)\)</td> <td>\(1\)</td> </tr> <tr> <td>\(\delta'(t)\)</td> <td>\(j\omega\)</td> </tr> <tr> <td>\(u(t)\)</td> <td>\(\frac{1}{j\omega}+\pi\delta(\omega)\)</td> </tr> <tr> <td>\(sgn(t)\)</td> <td>\(\frac{2}{j\omega}\)</td> </tr> <tr> <td>\(1\)</td> <td>\(2\pi\delta(\omega)\)</td> </tr> <tr> <td>\(e^{j\omega_0 t}\)</td> <td>\(2\pi\delta(\omega-\omega_0 )\)</td> </tr> <tr> <td>\(\cos (\omega_0t)\)</td> <td>\(\pi[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]\)</td> </tr> <tr> <td>\(\sin (\omega_0t)\)</td> <td>\(\pi j[\delta(\omega+\omega_0)-\delta(\omega-\omega_0)]\)</td> </tr> <tr> <td>\(G_\tau(t)\)</td> <td>\(\tau Sa(\frac{\omega \tau}{2})\)</td> </tr> <tr> <td>\(Sa(\omega_0 t)\)</td> <td>\(\frac{\pi}{\omega_0 } G_{2\omega_0 }(\omega)\)</td> </tr> <tr> <td>\(e^{-at}u(t)\)</td> <td>\(\frac{1}{a+j\omega}\)</td> </tr> <tr> <td>\(e^{-a\lvert t \rvert}\)</td> <td>\(\frac{2a}{a^2+\omega^2}\)</td> </tr> <tr> <td>待补充</td> <td> </td> </tr> </tbody> </table> <h3 id="拉普拉斯变换">拉普拉斯变换</h3>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">Fisheye</title><link href="https://cekxm.github.io/blog/2024/fisheye/" rel="alternate" type="text/html" title="Fisheye"/><published>2024-12-30T15:44:47+00:00</published><updated>2024-12-30T15:44:47+00:00</updated><id>https://cekxm.github.io/blog/2024/fisheye</id><content type="html" xml:base="https://cekxm.github.io/blog/2024/fisheye/"><![CDATA[<hr/> <p>title: 鱼眼镜头去变形和校正 fisheye banner_img: /img/jmu.jpg banner_img_height: 50 math: true date: 2023-08-11 23:45:49 tags: —</p> <p>参考：</p> <p><a href="https://plaut.github.io/fisheye_tutorial/#the-fisheye-projection">Tutorial on Computer Vision with Fisheye Cameras</a></p> <p><a href="https://docs.opencv.org/4.8.0/db/d58/group__calib3d__fisheye.html">OpenCV Fisheye camera model</a></p> <h2 id="鱼眼相机投影模型">鱼眼相机投影模型</h2> <p>有多种被称为鱼眼的投影方式，包括等距、等固角、立体和正交投影。OpenCV 使用”等距鱼眼”相机模型。</p> <p>具有相同的入射角 \(\theta\) 的三维点（组成cone）被投影到鱼眼图像中距离主点一定距离 \(r\) 处（组成一个圆）。这个性质我猜对不同的投影方式都是成立的。区别在于 \(r\) 和 \(\theta\) 的关系。</p> <ul> <li>equidistant fisheye model：the distance between a pixel in the image and the principal point is directly proportional to the angle of incidence \(r=f\theta\).</li> <li>In the equisolid fisheye projection \(r=2f\sin \frac{\theta}{2}\),</li> <li>in the stereographic fisheye projection \(r=2f\tan \frac{\theta}{2}\)</li> <li>and in the orthographic fisheye projection \(r=f\sin \theta\).</li> </ul> <h3 id="等距鱼眼投影">等距鱼眼投影</h3> <p>OpenCV 描述</p> <p><img src="/images/2024-12-30-fisheye/image-20230812001447747.png" alt="image-20230812001447747" class="img-fluid"/></p> <p>可以看出</p> <ol> <li>先计算入射角 \(\theta\)</li> <li>若有鱼眼失真，计算失真后的 \(\theta_d\)</li> <li>鱼眼投影，投影到假象的单位距离焦平面</li> <li>根据相机内参，形成二维图像（相机内参可以理解成传感器成像时的性质）</li> </ol> <h2 id="opencv-函数">OpenCV 函数</h2> <h3 id="cvfisheyecalibrate">cv::fisheye::calibrate</h3> <p>使用黑白格子图，标定鱼眼相机，输出K（camera intrinsic matrix），D（向量，\(k_1,k_2,k_3,k_4\)）</p> <h3 id="cvfisheyeinitundistortrectifymap">cv.fisheye.initUndistortRectifyMap</h3> <p>cv.fisheye.initUndistortRectifyMap( K, D, R, P, size, m1type[, map1[, map2]] ) -&gt; map1, map2</p> <p>Computes undistortion and rectification maps for image transform by <em>remap</em>. 求解后的两个 map 在 remap 函数中使用。</p> <ul> <li> <p>K，D 是鱼眼相机的参数，可以通过 calibrate 函数获得。</p> </li> <li>R Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3 1-channel or 1x1 3-channel。矫正时在物体空间中的变换。这个不就是我们想要的吗？</li> <li>P New camera intrinsic matrix (3x3) or new projection matrix (3x4) 这个是相机内参，相机内参控制 focal length、skew、principal point。</li> </ul> <h3 id="cvfisheyeundistortimage">cv.fisheye.undistortImage</h3> <p>cv.fisheye.undistortImage( distorted, K, D[, undistorted[, Knew[, new_size]]] ) -&gt; undistorted</p> <p>这个函数是综合了 initUndistortRectifyMap 和 remap。但是少了一些控制参数，比如3维空间中的旋转。</p> <h2 id="去失真和校正">去失真和校正</h2> <ol> <li>拍摄黑白格子图，使用 calibrate 函数就K，D</li> <li>使用 initUndistortRectifyMap 计算映射</li> <li>调用 remap</li> </ol> <h3 id="控制焦距">控制焦距</h3> <p>通过 P 参数可以控制焦距，达到类似 zoom in/out 的效果。</p> <h3 id="外部旋转">外部旋转</h3> <p>通过 R 参数可以控制场景旋转，或者说控制相机旋转，以使得校正后的图像能看到鱼眼图像的某一个部分。</p> <p>一个应用是获得相机分别向左、向右旋转观察到的两张图，然后把这两张图 stitch，形成一张大图。</p> <p><a href="https://plaut.github.io/fisheye_tutorial/#multiple-rectifications">参见这儿。</a></p> <h2 id="数据集">数据集</h2> <p>WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving</p> <p><a href="https://github.com/valeoai/WoodScape">Github</a></p> <blockquote> <p>鱼眼相机通常用于在监控、增强现实和特别是汽车应用中获取大视野。尽管它们很常见，但很少有公开的数据集可用于对鱼眼图像上的计算机视觉算法进行详细评估。我们发布了第一个广泛的鱼眼汽车数据集，名为WoodScape，以纪念罗伯特·伍德（Robert Wood），他在1906年发明了鱼眼相机。WoodScape包括四个全景相机和九个任务，包括分割、深度估计、3D边界框检测和污染检测。对于超过10,000张图像，提供了40个类别的语义实例级别的注释，其他任务的注释提供了超过100,000张图像。通过WoodScape，我们希望鼓励社区在使用天真的矫正方法之前，为鱼眼相机调整计算机视觉模型。</p> </blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry></feed>
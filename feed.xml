<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cekxm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cekxm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-12T03:04:26+00:00</updated><id>https://cekxm.github.io/feed.xml</id><title type="html">计算机视觉</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Aistack</title><link href="https://cekxm.github.io/blog/2026/AIStack/" rel="alternate" type="text/html" title="Aistack"/><published>2026-02-12T02:56:54+00:00</published><updated>2026-02-12T02:56:54+00:00</updated><id>https://cekxm.github.io/blog/2026/AIStack</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/AIStack/"><![CDATA[<h2 id="ai-stack">AI Stack</h2> <p><strong>AI 正在从“会聊天的幻觉机器（LLM）”进化为“懂物理规律的系统化大脑（AI Stack）”</strong>。 以下是对该材料的核心总结： ——</p> <h3 id="1-为什么经典-llm已经不够用了">1. 为什么“经典 LLM”已经不够用了？</h3> <p>材料指出，传统的 LLM（如 GPT 系列）虽然强大，但存在本质缺陷：</p> <ul> <li><strong>预测的是字词（Tokens），而非现实：</strong> 它们在学习概率分布，而不是理解物理世界的运行逻辑。</li> <li><strong>缺乏“世界模型”：</strong> 它们无法进行长期规划、自主决策或处理复杂的因果关系。</li> <li> <h2 id="单体局限-真正的智能无法仅存在于一个模型中而应是一个系统"><strong>单体局限：</strong> 真正的智能无法仅存在于一个模型中，而应是一个系统。</h2> <h3 id="2-核心范式转移从语言模型到世界模型">2. 核心范式转移：从语言模型到世界模型</h3> <p>AI 架构正在经历从“文本预测”向“现实建模”的跨越：</p> </li> <li><strong>目标转变：</strong> 从<strong>预测 Token</strong> 转向 <strong>预测潜在语义（Latent Meaning）</strong>。</li> <li><strong>本质转变：</strong> 从<strong>语言建模（Language Modeling）</strong> 转向 <strong>世界建模（World Modeling）</strong>。</li> <li> <h2 id="形态转变-ai-不再是一个单纯的模型而是一个由多个组件构成的系统system"><strong>形态转变：</strong> AI 不再是一个单纯的模型，而是一个由多个组件构成的<strong>系统（System）</strong>。</h2> <h3 id="3-未来-ai-stack-的-6-大架构支柱">3. 未来 AI Stack 的 6 大架构支柱</h3> <p>材料提出了构建未来智能系统的六个核心家族：</p> <ol> <li><strong>世界模型（World Models）：</strong> 学习现实世界的运行规律。</li> <li><strong>JEPA (联合嵌入预测架构)：</strong> 学习事物的“意义”和特征，而非死磕像素。</li> <li><strong>重新定义的扩散模型（Diffusion Models）：</strong> 不再只画图，而是用于<strong>规划和动作生成</strong>。</li> <li><strong>推理模型（Reasoning Models）：</strong> 负责抽象思维、解释和系统编排。</li> <li><strong>状态空间模型（SSM）：</strong> 提供更高效的实时处理能力。</li> <li> <h2 id="智能体架构agent-architectures-通过系统间的交互实现复杂的任务目标"><strong>智能体架构（Agent Architectures）：</strong> 通过系统间的交互实现复杂的任务目标。</h2> <h3 id="4-统一的智能堆栈unified-ai-stack">4. 统一的智能堆栈（Unified AI Stack）</h3> <p>未来的 AI 智能将由以下模块协同产生，而不再是“一个对话框”：</p> <blockquote> <h2 id="感知-rightarrow-世界理解-rightarrow-预测-rightarrow-规划-rightarrow-推理-rightarrow-动作工具api智能体"><strong>感知 \(\rightarrow\) 世界理解 \(\rightarrow\) 预测 \(\rightarrow\) 规划 \(\rightarrow\) 推理 \(\rightarrow\) 动作（工具、API、智能体）</strong></h2> <h3 id="5-展望-2026ai-的终局">5. 展望 2026：AI 的终局</h3> </blockquote> </li> </ol> </li> <li><strong>单模型时代结束：</strong> 模块化、智能体化（Agentic）的系统是必然趋势。</li> <li><strong>形态演变：</strong> 到 2026 年，AI 的运作方式将<strong>更像生物大脑</strong>，而非简单的聊天机器人。</li> <li><strong>智能的本质：</strong> 智能是一个堆栈（Stack），而不仅仅是对话。 <h2 id="openclaw">OpenClaw</h2> <p>OpenClaw（曾用名 Clawdbot / Moltbot）在 2026 年被视为“具身智能（Embodied AI）”在数字操作系统层面的代表作。它的构架设计体现了从“聊天机器人”向“自主系统”转化的核心思路。 根据最新的技术文档和社区实践，OpenClaw 的构架可以被拆解为以下六层体系：</p> <h3 id="1-核心构架六层模型">1. 核心构架六层模型</h3> <p>OpenClaw 并不是一个简单的程序，而是一个高度模块化的<strong>智能体运行时（Agent Runtime）</strong>：</p> </li> <li><strong>网关层 (Gateway):</strong> 系统的“感官与通道”。作为一个长驻后台的 TypeScript 进程，它负责连接 WhatsApp、Telegram、Discord 等消息平台，处理消息的流入（Ingress）与流出（Egress）。</li> <li><strong>路由与会话层 (Routing &amp; Sessions):</strong> 负责多用户/多任务管理，决定哪一个智能体实例或会话历史应该响应当前的指令。</li> <li><strong>智能体运行时 (Agent Runtime):</strong> 系统的“中枢神经”。它接收上下文（系统提示词 + 历史记录 + 附件），调用底层模型，并处理模型的工具调用（Tool Call）请求。</li> <li><strong>大脑与模型解析器 (Brain &amp; Model Resolver):</strong> 模型中立（Model-agnostic）层。它可以根据任务复杂度自动路由：简单任务用本地 Llama 4，复杂推理用云端 Claude 4.5。</li> <li><strong>技能与工具集 (Skills &amp; Tools):</strong> 系统的“手脚”。包括网页爬虫、文件管理、Shell 执行等。它支持 <strong>MCP (Model Context Protocol)</strong> 协议，使其能无缝接入全球开发者贡献的工具服务器。</li> <li> <h2 id="交互界面-surfaces-用户接触点包括命令行cliweb-仪表盘及各类聊天-app"><strong>交互界面 (Surfaces):</strong> 用户接触点，包括命令行（CLI）、Web 仪表盘及各类聊天 App。</h2> <h3 id="2-关键技术组件">2. 关键技术组件</h3> <p>OpenClaw 的卓越稳定性源于其内部的几个核心组件： | <strong>组件名称</strong> | <strong>功能描述</strong> | <strong>核心价值</strong> | | ————————- | —————————————————- | —————————————————— | | <strong>Lane Queue (车道队列)</strong> | 强制所有执行动作进入串行队列。 | 防止多个动作竞争导致的系统死锁或逻辑冲突。 | | <strong>Docker Sandbox</strong> | 所有的文件读写和 Shell 脚本都在 Docker 容器中执行。 | <strong>安全隔离</strong>，即便 AI 误操作也不会格式化用户的宿主机。 | | <strong>双层记忆系统</strong> | JSONL 原始日志 + Markdown 提炼记忆 (<code class="language-plaintext highlighter-rouge">MEMORY.md</code>)。 | 兼顾了“事实审计”与“长程经验总结”的平衡。 | | <strong>Semantic Snapshots</strong> | 网页浏览时只解析“可访问性树（Accessibility Tree）”。 | 极大地降低了 Token 消耗，比直接识别截图更精准。 | ——</p> <h3 id="3-工作流程从感知到执行">3. 工作流程：从感知到执行</h3> <p>当你在 WhatsApp 上发一句“帮我把最近三封邮件汇总并存入 Notion”时，OpenClaw 内部会经历以下循环：</p> <ol> <li><strong>感知 (Perception):</strong> Gateway 捕获消息，将其归一化为标准的事件格式。</li> <li><strong>规划 (Planning):</strong> 智能体运行时结合 <strong>Memory</strong> 检索相关上下文，由 <strong>Brain</strong> 拆解为：<code class="language-plaintext highlighter-rouge">List Emails</code> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">Summarize</code> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">Write Notion</code>。</li> <li><strong>预测与决策 (Prediction):</strong> 通过 <strong>Prompt Builder</strong> 组装工具调用指令。</li> <li><strong>沙盒执行 (Action):</strong> 在 <strong>Sandbox</strong> 中安全运行脚本，并通过 <strong>MCP</strong> 协议与外部 API 通信。</li> <li><strong>反馈 (Feedback):</strong> 如果执行失败，Lane Queue 会触发重试逻辑或向用户报错。 <h2 id="mcp-协议">MCP 协议</h2> <p>MCP（Model Context Protocol，模型上下文协议）是 OpenClaw 这种 Agent 能够快速“进化”出各种能力的核心秘密。你可以把 MCP 想象成 AI 界的 <strong>“通用 USB 接口”</strong>。 在 OpenClaw 的架构中，它通过 <strong>MCP 客户端（Client）</strong> 的身份，与无数个独立的 <strong>MCP 服务器（Server）</strong> 通信。以下是它扩展插件的具体运作机制： ——</p> <h3 id="1-客户端-服务器架构能力的解耦">1. “客户端-服务器”架构：能力的解耦</h3> <p>OpenClaw 本身不需要知道如何查天气、如何读 SQL 数据库或如何搜索网页。</p> </li> </ol> </li> <li><strong>OpenClaw (Client):</strong> 负责与模型（如 Claude 4）对话。它告诉模型：“我有这些工具，你可以用。”</li> <li><strong>MCP Servers (Plugins):</strong> 这是一个个独立运行的微型程序。比如，有一个专门负责 Google 搜索的服务器，另一个负责管理本地 Notion 的服务器。</li> <li><strong>连接方式：</strong> OpenClaw 通过标准化的 JSON-RPC 协议与这些服务器通信。 <h3 id="2-插件扩展的三个核心维度">2. 插件扩展的三个核心维度</h3> <p>通过 MCP，OpenClaw 可以从服务器那里“继承”三种能力：</p> <ol> <li><strong>Resources（资源）:</strong> 允许模型<strong>读取数据</strong>。比如，一个 MCP 插件可以把你的本地 GitHub 仓库变成模型可以直接读取的资源。</li> <li><strong>Tools（工具）:</strong> 允许模型<strong>执行动作</strong>。比如，模型调用 <code class="language-plaintext highlighter-rouge">execute_python</code> 或 <code class="language-plaintext highlighter-rouge">create_linear_ticket</code> 这样的命令。</li> <li> <h2 id="prompts提示词模版-插件可以提供预设的指令模版告诉模型如何更好地处理特定任务如以资深架构师的身份审核这段代码"><strong>Prompts（提示词模版）:</strong> 插件可以提供预设的指令模版，告诉模型如何更好地处理特定任务（如“以资深架构师的身份审核这段代码”）。</h2> <h3 id="3-插件的发现与调用流程">3. 插件的“发现”与调用流程</h3> <p>当你在 OpenClaw 中配置了一个新的 MCP 插件（例如 <code class="language-plaintext highlighter-rouge">mcp-server-google-search</code>）后，会发生以下过程：</p> </li> </ol> </li> <li><strong>第一步：动态握手（Discovery）</strong> OpenClaw 启动时，会询问 MCP 服务器：“你能干什么？”服务器返回一份清单（JSON 格式），包含工具名、描述和参数定义。</li> <li><strong>第二步：能力注入</strong> OpenClaw 把这份清单“塞”进发给 LLM 的 System Prompt 中。现在模型知道：“原来我可以搜索 Google，只需要输出特殊的 <code class="language-plaintext highlighter-rouge">call_tool</code> 指令。”</li> <li><strong>第三步：安全执行（Execution）</strong> 如果模型决定搜索，它会发出请求。OpenClaw 接收到请求，转发给本地运行的 MCP 搜索服务器。服务器去执行搜索，再把结果返回给 OpenClaw，最后转给模型。 —— <h3 id="4-为什么这对开发者来说是杀手锏">4. 为什么这对开发者来说是“杀手锏”？</h3> </li> <li><strong>语言无关性：</strong> 你可以用 Python 写一个处理图像的插件，用 Rust 写一个读数据库的插件。只要它们符合 MCP 协议，OpenClaw 就能直接用。</li> <li><strong>生态复用：</strong> 既然 MCP 是 Anthropic 推动的标准，OpenClaw 可以直接复用社区里现成的数千个 MCP 插件（比如官方提供的文件系统服务器、SQL 服务器等），而不需要自己重写。</li> <li><strong>安全隔离：</strong> MCP 服务器通常运行在独立的进程中。这意味着即便一个复杂的数据库插件崩溃了，也不会导致 OpenClaw 的主进程崩溃。</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">World_Models</title><link href="https://cekxm.github.io/blog/2026/world_models/" rel="alternate" type="text/html" title="World_Models"/><published>2026-02-11T12:32:53+00:00</published><updated>2026-02-11T12:32:53+00:00</updated><id>https://cekxm.github.io/blog/2026/world_models</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/world_models/"><![CDATA[<h2 id="marble-world-labs">Marble, World Labs</h2> <p><a href="https://mp.weixin.qq.com/s/x63d39mcbJuT5dMCD_8Prw">李飞飞世界模型公司一年估值暴涨5倍！正洽谈新一轮5亿美元融资</a></p> <blockquote> <p>Marble可以根据文本或图像提示，直接生成可探索的3D世界。</p> <p>在传统流程中，3D内容通常从<strong>人工构建的多边形网格（polygon meshes）</strong>开始：</p> <p>场景由大量微小的三角形拼接而成，再交由渲染引擎处理，工程成本高、制作周期长，也很难规模化。</p> <p>Marble则采用了3D高斯溅射（3D Gaussian Splatting，3DGS）技术，用数以百万计的半透明点来表示场景结构，从而在保持较高渲染效率的同时，实现更细腻的视觉效果。</p> <p>更重要的是，Marble并不只追求“看起来真实”。它还会同时输出所谓的“碰撞网格（collider meshes）”——</p> <p>这是一种牺牲外观细节、换取计算速度的几何表示，专门用于物理仿真和机器人模拟。</p> <p>在交互层面，Marble还提供了<strong>Chisel</strong>工具：用户可以先用简单几何体快速“搭骨架”，再生成不同风格的细节版本，这是朝着可控、可编辑的世界构建迈出的一步。</p> <p>此外，World Labs近期还开放了World API，开发者可以在应用中通过文本、图像或视频，直接生成可探索的3D世界，把“世界模型”嵌入到具体产品之中。</p> <p>在战略层面，李飞飞本人也曾多次明确表态：</p> <p><strong>世界模型是实现空间智能的关键，是继大语言模型（LLM）之后，AI下一个十年的核心主线。</strong></p> <p>在她的框架中，真正具备空间智能的世界模型，至少需要三种能力：</p> <ul> <li>生成（Generative）：能创造遵守物理定律、空间一致的世界；</li> <li>多模态（Multimodal）：能处理从图像、视频到动作的多模态输入；</li> <li>交互（Interactive）：能预测世界随时间演变或互动的状态。</li> </ul> <p>也正因如此，世界模型被视为一种<strong>“基础设施型能力”</strong>。</p> <p>一旦成立，它的影响将不只局限在单一应用场景，而会向多个方向扩散：</p> <ul> <li><strong>AIGC：3D资产生成、游戏世界构建、影视制作与虚拟拍摄。</strong></li> <li><strong>具身智能：机器人、仿真环境、现实世界任务执行。</strong></li> </ul> <p>以具身智能为例，世界模型提供的是对环境的可预测表示，在此之上，才能叠加真正有效的决策与控制。</p> <p>包括DP（Diffusion policy）、RL等控制方法，以及对智能体行为进行系统性评估（如光轮智能)。 要理解 <strong>Chisel 工具中的简单几何体（Proxy Geometry）</strong> 与 <strong>3DGS（3D Gaussian Splatting）中的高斯球（Gaussians）</strong> 之间的关系，我们可以从“骨架”与“血肉”的比喻切入。 它们之间并非简单的替代关系，而是一种<strong>约束与填充、宏观与微观</strong>的耦合关系。 ——</p> <h3 id="1-空间上的约束关系容器与内容">1. 空间上的约束关系：容器与内容</h3> <p>在 Marble 的工作流中，Chisel 提供的简单几何体（如方块、圆柱、球体）充当了<strong>“空间容器”</strong>。</p> <ul> <li><strong>简单几何体（骨架）：</strong> 定义了场景的宏观布局和物理边界。它告诉 AI：“这里应该有一张桌子，高度是多少，长宽是多少。”</li> <li> <h2 id="3dgs-高斯球血肉-ai-会在这些几何体定义的空间范围内填充数以百万计的-3d-高斯点这些点带有颜色不透明度协方差形状等信息负责渲染出大理石的纹理木头的色泽或是光线的反射"><strong>3DGS 高斯球（血肉）：</strong> AI 会在这些几何体定义的空间范围内，填充数以百万计的 3D 高斯点。这些点带有颜色、不透明度、协方差（形状）等信息，负责渲染出大理石的纹理、木头的色泽或是光线的反射。</h2> <h3 id="2-数学上的参数关联">2. 数学上的参数关联</h3> <p>Chisel 操作的几何体实际上是在为生成模型提供<strong>先验条件（Prior）</strong>。它们的关系可以从以下三个维度理解： | <strong>维度</strong> | <strong>Chisel 简单几何体 (Proxies)</strong> | <strong>3DGS 高斯球 (Splatting)</strong> | | ————– | —————————————— | ———————————— | | <strong>密度控制</strong> | 几何体的体积决定了高斯点生成的<strong>范围</strong>。 | 在几何体内部或表面生成高斯点。 | | <strong>初始化引导</strong> | 几何体的表面法线可以引导高斯球的<strong>朝向</strong>。 | 确保高斯球的扁平方向贴合几何体表面。 | | <strong>物理对应</strong> | 直接转化为 <strong>Collider Mesh（碰撞网格）</strong>。 | 仅负责视觉呈现，不参与物理碰撞计算。 | ——</p> <h3 id="3-为什么不直接用高斯球建模">3. 为什么不直接用高斯球建模？</h3> <p>你可能会问：既然 3DGS 这么强大，为什么还要 Chisel 去搭几何体？</p> </li> <li><strong>可控性（Controllability）：</strong> 3DGS 本质上是一堆“云朵”，很难直接像推拉积木一样精确修改。通过 Chisel 修改几何体，AI 就能实时重新分布高斯球，实现“改动骨架，皮肤随之重塑”的效果。</li> <li> <h2 id="语义对齐-简单几何体通常带有语义标签比如墙门当你在-chisel-里移动一堵墙时系统知道要把与之关联的所有高斯点一起平移而不是漫无目的地移动点云"><strong>语义对齐：</strong> 简单几何体通常带有语义标签（比如“墙”、“门”）。当你在 Chisel 里移动一堵“墙”时，系统知道要把与之关联的所有高斯点一起平移，而不是漫无目的地移动点云。</h2> <h3 id="4-总结一种引导式生成">4. 总结：一种“引导式生成”</h3> <p><strong>简单来说，Chisel 的几何体是 3DGS 的“脚手架”。</strong> 在生成过程中，Marble 利用扩散模型（Diffusion Models）将文本提示词与 Chisel 的几何拓扑结构相结合。几何体提供了<strong>空间一致性</strong>（确保桌子不会飘在空中），而 3DGS 提供了<strong>视觉真实感</strong>。 这种“几何引导的高斯生成”正是李飞飞所强调的<strong>“空间智能”</strong>的具体体现——AI 不再是随机生成像素，而是在理解物理空间的基础上，往特定的“模具”里填注细节。</p> <h3 id="point-world">Point World</h3> <p><a href="https://www.youtube.com/watch?v=XPOsCwrYdk0&amp;t=120s">PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation</a> 《PointWorld》论文的核心实现是将<strong>状态（State）</strong>与<strong>动作（Action）</strong>统一在三维物理空间中，通过 3D 点流（3D Point Flows）来建模环境动力学。 以下是该论文具体实现的简要概述：</p> <h4 id="1-输入inputs">1. 输入（Inputs）</h4> <p>模型在推理时主要接收以下两类输入：</p> </li> <li><strong>静态场景点云 (Scene Points)</strong>：通过单张或多张标定的 RGB-D 图像构建而成。这些点通过<strong>投影</strong>到 2D 视图中，并使用冻结的 <strong>DINOv3</strong> 编码器提取丰富的语义特征。</li> <li><strong>机器人动作点流 (Robot Flows)</strong>：动作不再被表示为特定于机器人的关节坐标，而是表示为机器人几何体（通常是夹持器）在空间中的 3D 轨迹。这些轨迹是通过机器人的 <strong>URDF 文件</strong>和给定的<strong>关节动作序列</strong>，利用正运动学（Forward Kinematics）预先计算出来的。 <h4 id="2-预测内容what-it-predicts">2. 预测内容（What it Predicts）</h4> </li> <li><strong>全场景 3D 点流 (Full-Scene 3D Point Flows)</strong>：模型预测场景中每个点在未来一段时间（即预测时界 \(H\)）内的 <strong>3D 位移向量</strong>。</li> <li>这种预测不仅包含受机器人直接接触影响的点，还隐式地包含了物体的物理属性（如刚性、形变、关节连接）以及受重力影响的动力学演化。 <h4 id="3-具体实现架构">3. 具体实现架构</h4> </li> <li>特征化 (Featurization) ： <ul> <li><strong>场景点</strong>：使用 DINOv3 提取 2D 特征。</li> <li><strong>机器人点</strong>：使用<strong>时间嵌入 (Temporal Embedding)</strong> 来表征动作的先后顺序。</li> </ul> </li> <li><strong>骨干网络 (Backbone)</strong>：将场景点和机器人动作点拼接在一起，输入到高性能的 3D 点云骨干网络 <strong>PointTransformerV3 (PTv3)</strong> 中。</li> <li><strong>分块预测 (Chunked Prediction)</strong>：不同于自回归模型，PointWorld 在一次前向传播中同时预测未来多个时间步（如 10 步，总计 1 秒）的状态，这极大地提高了实时推理速度（单次推理约 0.1 秒）。</li> <li><strong>下游应用</strong>：该模型可直接集成到<strong>模型预测控制 (MPC)</strong> 框架中，通过在“想象”的点流中评估不同的候选动作序列，为真实机器人寻找最优操纵方案。 总结来说，PointWorld <strong>通过“当前的 3D 场景点”和“预想的机器人 3D 动作流”，来预测“整个 3D 场景将如何随时间位移”</strong>。 Scene points are featurized with frozen DINOv3 [153, 154] by projecting them to 2D views, while robot points are featurized with temporal embedding. 在《PointWorld》论文中，这句话描述了模型如何为不同类型的点（场景点和机器人点）提取初始特征。虽然 <strong>DINOv3</strong> 本身确实是一个用于二维图像特征提取的模型，但论文通过<strong>几何投影（Projection）</strong>的方法巧妙地将其应用于三维点云。 以下是具体的理解与实现流程： <h4 id="1-dinov3-如何用于-point-cloud">1. DINOv3 如何用于 Point Cloud？</h4> <p>模型并不是直接把点云输入 DINOv3，而是利用<strong>相机标定参数</strong>建立 2D 图像与 3D 点云之间的桥梁：</p> </li> <li><strong>投影（Projection）</strong>：利用相机的内参（Intrinsics）和外参（Extrinsics），将第一帧中的每个 3D 场景点坐标投影到对应的 2D 图像平面上，得到该点在图像中的像素坐标 (\(u, v\))。</li> <li><strong>特征采样（Sampling）</strong>：将 RGB 图像输入预训练并冻结的 <strong>DINOv3</strong> 编码器，生成 2D 特征图。然后，根据投影得到的像素坐标，通过<strong>双线性插值（Bilinear Interpolation）</strong>从 2D 特征图中提取对应的特征向量（Patch Tokens）。</li> <li><strong>多视图融合</strong>：如果一个 3D 点能被多个相机观测到，模型会将来自不同视角的 DINO 特征进行平均聚合，并将这个融合后的特征附加到该 3D 点上，作为它的初始表征。 <h4 id="2-为什么要这么做">2. 为什么要这么做？</h4> </li> <li><strong>引入语义先验</strong>：虽然点云提供了精确的几何信息，但缺乏语义理解。论文指出，DINOv3 的密集特征能在无需显式分割的情况下提供<strong>“物体性先验”（Objectness Priors）</strong>，帮助模型识别哪些点属于同一个物体，或者物体的物理性质。</li> <li><strong>解决三维预训练稀缺问题</strong>：高质量的 3D 预训练模型相对较少且效果有限，而 2D 视觉模型已经在海量数据上学习到了极其强大的特征表达能力，通过这种投影方式可以“借用” 2D 视觉模型的语义能力。 <h4 id="3-场景点-vs-机器人点的特征提取差异">3. 场景点 vs. 机器人点的特征提取差异</h4> <p>论文对不同来源的点采用了不同的表征策略，以满足动态建模的需求：</p> </li> <li><strong>场景点（Scene Points）</strong>：通过上述方式关联 <strong>DINOv3 2D 特征</strong>。这些特征是时间常数（Time-constant）的，主要用于刻画物体的身份和属性。</li> <li><strong>机器人点（Robot Points）</strong>：使用<strong>时间嵌入（Temporal Embedding）</strong>。机器人点的轨迹是由已知的正运动学（URDF）生成的，因此它们的特征重点在于体现随时间变化的动作信息（位置、速度、加速度等），而不是语义外观。 <strong>总结来说</strong>，DINOv3 在这里充当了一个<strong>“3D 点的语义着色器”</strong>：它通过几何映射，把 2D 图像中蕴含的物体语义信息“贴”到了 3D 点云上，从而增强了模型对物理交互和场景演化的预测能力。 除了你提到的《PointWorld》，参考资料中涉及的另外三篇论文分别是 <strong>PhysGaussian</strong>、<strong>SuGaR</strong> 和 <strong>UniSim</strong>。它们各自的工作内容和实现路径如下： <h3 id="1-physgaussian-物理集成的-3d-高斯流体动力学">1. PhysGaussian: 物理集成的 3D 高斯流体动力学</h3> </li> <li><strong>主要工作</strong>：该论文旨在将物理规律（如牛顿动力学）无缝集成到 <strong>3D Gaussian Splatting (3DGS)</strong> 框架中，以实现高质量的动态效果合成。它遵循<strong>“所见即所模拟”（What You See Is What You Simulate, WS2）</strong>的原则，使 3D 高斯点既是渲染单元，也是物理模拟的粒子。</li> <li>具体实现 ： <ul> <li><strong>输入</strong>：静态场景的 3D 高斯表征和用户指定的物理参数（如密度、杨氏模量等）。</li> <li><strong>预测/模拟内容</strong>：利用自定义的<strong>物质点法（MPM）</strong>和连续介质力学原理，模拟 3D 高斯内核随时间的形变和应变属性。</li> <li><strong>实现效果</strong>：能够模拟弹性体、塑性金属、非牛顿流体和碎石材料的物理交互，无需传统的三角形或四面体网格即可生成 photo-realistic 的动态动画。 <h3 id="2-sugar-表面对齐的高斯喷溅实现高效网格重建">2. SuGaR: 表面对齐的高斯喷溅实现高效网格重建</h3> </li> </ul> </li> <li><strong>主要工作</strong>：SuGaR 解决了从数百万个无序的 3D 高斯点中提取高质量、可编辑的三维<strong>网格（Mesh）</strong>的难题。</li> <li>具体实现： <ul> <li><strong>输入</strong>：多视图图像。</li> <li>实现机制： <ol> <li><strong>正则化项</strong>：在优化过程中引入约束，强制高斯点与场景表面对齐并扁平化。</li> <li><strong>网格提取</strong>：对对齐后的高斯点云进行采样，利用<strong>泊松重建（Poisson Reconstruction）</strong>在几分钟内提取出三角形网格。</li> <li><strong>高斯绑定</strong>：将新的“薄高斯”点绑定到网格表面，并进行联合优化。</li> </ol> </li> <li><strong>实现效果</strong>：允许用户使用传统的网格编辑、动画和重新布光工具来操纵 3D 高斯场景，同时保持极高的渲染质量。 <h3 id="3-unisim-通用真实世界交互模拟器">3. UniSim: 通用真实世界交互模拟器</h3> </li> </ul> </li> <li><strong>主要工作</strong>：UniSim 构建了一个<strong>“动作输入-视频输出”（Action-in-video-out）</strong>的通用模拟器，通过生成式框架来模拟真实世界的物理交互。</li> <li>具体实现： <ul> <li><strong>输入</strong>：有限的历史观测（视频帧）和特定的机器人动作序列。</li> <li><strong>预测内容</strong>：使用<strong>视频扩散模型（Video Diffusion Model）</strong>预测未来的视觉观测结果，并通过自回归方式生成长时程视频。</li> <li>实现效果： <ul> <li>模拟长时程交互（如连续打开多个抽屉、移动物体），并保持时间上的一致性。</li> <li><strong>跨越 Sim-to-Real 鸿沟</strong>：机器人可以纯粹在 UniSim 生成的“视频环境”中训练高级语言策略或低级控制策略，然后直接部署到真实世界中。 <h3 id="总结对比">总结对比</h3> <p>| 论文名称 | 核心媒介 | 核心目标 | 实现手段 | | —————- | ———————– | ———————— | ——————————– | | <strong>PhysGaussian</strong> | 3D 高斯点 | 物理动力学模拟 | 集成连续介质力学与物质点法 (MPM) | | <strong>SuGaR</strong> | 3D 高斯点 + 网格 (Mesh) | 快速提取高质量可编辑网格 | 表面对齐正则化 + 泊松重建 | | <strong>UniSim</strong> | 视频帧 (2D) | 构建端到端通用模拟器 | 视频扩散模型 + 动作条件生成 |</p> <h2 id="jepa">JEPA</h2> <p><a href="https://mp.weixin.qq.com/s/HRMVvZcChlcwPkHdK6sUXw">盘点｜4年JEPA进化之路，12篇核心突破！LeCun(杨立昆)如何用它重构AI表征学习？</a> <a href="https://www.youtube.com/watch?v=Jt-m3gho0_0&amp;list=WL&amp;index=2&amp;t=677s">V-JEPA &amp; V-JEPA 2 Explained: The Self-Supervised Revolution in Video Understanding</a> 这个不错，看一下 <a href="https://www.youtube.com/watch?v=7UkJPwz_N_0&amp;list=WL&amp;index=3&amp;t=191s">V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video (Explained)</a></p> </li> </ul> </li> </ul> </li> </ul> </blockquote> <h2 id="你可以把它们的关系看作是一个底座v-jepa-2-ac--一次深度体检251224497--一个导航插件260100844">你可以把它们的关系看作是：<strong>一个底座（V-JEPA-2-AC）</strong> + <strong>一次深度体检（2512.24497）</strong> + <strong>一个导航插件（2601.00844）</strong>。</h2> <h3 id="1-v-jepa-2-ac物理世界的底座">1. V-JEPA-2-AC：物理世界的“底座”</h3> <p>这是 Meta 发布的<strong>基础模型</strong>。它的核心贡献是证明了通过大规模视频预训练（使用 Masking 机制），模型可以学到一个带动作条件的（Action-Conditioned）世界模型。</p> <ul> <li><strong>它解决了：</strong> “如果我在这段视频的这一帧做一个‘推’的动作，下一帧的<strong>特征</strong>会变成什么样？”</li> <li><strong>它的角色：</strong> 提供了一个能够模拟物理演化的<strong>模拟器（Simulator）</strong>。 <h3 id="2-251224497深度的科学诊断">2. [2512.24497]：深度的“科学诊断”</h3> <p><em>What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?</em> 这篇论文是对 V-JEPA 这一类模型的<strong>深度复盘和理论分析</strong>。</p> </li> <li><strong>核心研究点：</strong> 它在问“为什么 JEPA 在物理规划上比别的模型强？”</li> <li><strong>结论：</strong> 它通过实验证明，成功的关键不在于把像素还原得多么清晰（像生成模型那样），而在于 <strong>Latent Space（潜在空间）的动态一致性</strong>。</li> <li><strong>与 V-JEPA-2-AC 的关系：</strong> 它解释了 V-JEPA 这种架构之所以能做规划，是因为它的 Masking 预训练任务迫使模型抓住了物理守恒律（如动量、重力），而不是表面的视觉细节。 <h3 id="3-260100844动作规划的算法升级">3. [2601.00844]：动作规划的“算法升级”</h3> <p><em>Value-guided action planning with JEPA world models</em> 这篇论文是在前两者的基础上，提出了<strong>如何更好、更快地使用这个世界模型</strong>。</p> </li> <li><strong>核心改进：</strong> 之前的模型在规划时（比如想让机器人抓球），可能需要在潜在空间里随机尝试成千上万种动作序列。这篇论文引入了 <strong>Value Function（价值函数）</strong>。</li> <li><strong>技术突破：</strong> 它通过在 JEPA 的特征上叠加一个价值评估器，让模型在“脑补”未来时，能直接朝着“成功率更高”的方向去预测，而不是盲目乱猜。</li> <li> <h2 id="与前二者的关系-它利用了-v-jepa-2-ac-的特征和-251224497-验证的物理一致性最终实现了一个高效的闭环控制算法"><strong>与前二者的关系：</strong> 它利用了 V-JEPA-2-AC 的特征和 [2512.24497] 验证的物理一致性，最终实现了一个高效的闭环控制算法。</h2> <h3 id="三者关系的直观对比表">三者关系的直观对比表</h3> <p>| <strong>维度</strong> | <strong>V-JEPA-2-AC</strong> | <strong>[2512.24497] (Success Factors)</strong> | <strong>[2601.00844] (Value-guided)</strong> | | ————– | ————————— | ———————————- | ——————————- | | <strong>身份</strong> | 预训练好的<strong>模型本体</strong> | 对模型的<strong>诊断报告/说明书</strong> | 对规划算法的<strong>功能升级包</strong> | | <strong>解决的问题</strong> | 赋予 AI 物理直觉 | 搞清楚物理直觉哪来的 | 让物理直觉变现为高效行动 | | <strong>关键词</strong> | Action-Conditioned, Masking | Latent Dynamics, Robustness | IQL, Value-guided Planning |</p> <h3 id="总结">总结</h3> <p>这三篇工作的逻辑非常清晰：</p> <ol> <li><strong>V-JEPA-2-AC</strong> 说：“我通过看视频和动作，学会了物理世界的变化规律。”</li> <li><strong>2512.24497</strong> 说：“我研究了上面那个家伙，发现它强在‘特征空间预测’，而不是‘画图’。”</li> <li><strong>2601.00844</strong> 说：“既然它物理直觉这么好，我给它加个‘价值目标’，让它在脑子里规划动作时不再跑偏。” <h3 id="lejepa">LeJEPA</h3> <p><strong>V-JEPA-2 侧重于“规模与能力（Scale &amp; Action）”，而 LeJEPA 侧重于“理论与效率（Theory &amp; Heuristic-free）”。</strong> 以下是它们关系的深度解析：</p> <h4 id="1-什么是-lejepa-latent-euclidean-jepa">1. 什么是 LeJEPA (Latent-Euclidean JEPA)?</h4> <p><strong>LeJEPA</strong>（发表于 2025 年 11 月）是 JEPA 架构在<strong>训练算法</strong>上的重大突破。</p> </li> </ol> </li> <li><strong>核心贡献：</strong> 它引入了 <strong>SIGReg (Sketched Isotropic Gaussian Regularization)</strong> 目标函数。</li> <li><strong>解决的痛点：</strong> 传统的 JEPA（包括第一代 V-JEPA）依赖很多“训练技巧（Heuristics）”来防止特征崩溃，比如 <strong>Teacher-Student 网络、EMA（指数移动平均）更新、Stop-gradient</strong> 等。</li> <li><strong>突破点：</strong> LeJEPA 证明了只要让特征分布符合“各向同性高斯分布”，就可以扔掉那些复杂的技巧，直接进行稳定的大规模训练。 <h4 id="2-v-jepa-2-与-lejepa-的区别与联系">2. V-JEPA-2 与 LeJEPA 的区别与联系</h4> <p>你提到的 V-JEPA-2（通常指 Assran 等人在 2025 年推出的版本）和 LeJEPA 的关系如下表： | <strong>特性</strong> | <strong>V-JEPA-2 / V-JEPA-2-AC</strong> | <strong>LeJEPA (arXiv:2511.08544)</strong> | | ———— | ———————————————————— | ———————————————————— | | <strong>主攻方向</strong> | <strong>时空规模与动作感知</strong>。研究如何处理超长视频、加入 Action 条件做规划。 | <strong>训练目标函数优化</strong>。研究如何去掉启发式插件，实现数学上的最优训练。 | | <strong>训练配方</strong> | 通常沿用“Teacher-Student”架构（类似 I-JEPA 的升级版）。 | <strong>无教师架构</strong>。使用 SIGReg 直接约束 Latent Space。 | | <strong>模型规模</strong> | 侧重于 ViT-L, ViT-H 等大规模视频底座。 | 证明了其算法在 1.8B (ViT-g) 规模下依然不需要技巧也能稳定。 | | <strong>相互关系</strong> | <strong>目前的“能力底座”</strong>。 | <strong>未来的“训练引擎”</strong>。 |</p> <h4 id="3-v-jepa-2-用到-lejepa-了吗">3. V-JEPA-2 用到 LeJEPA 了吗？</h4> <p>从目前的论文发布顺序看：</p> </li> <li><strong>技术上：</strong> V-JEPA-2-AC 在发布时（2025 年中）主要还是基于改进的掩码机制和动作条件分支。</li> <li><strong>趋势上：</strong> 随着 LeJEPA 在 2025 年底发布，Meta 的后续工作（包括正在研发的更高版本）正在将 <strong>LeJEPA 的 SIGReg 损失函数</strong> 引入到 V-JEPA 的视频训练中。 <strong>结论是：</strong> V-JEPA-2 是 JEPA 架构在<strong>视频理解领域</strong>的巅峰之作；而 LeJEPA 是对所有 JEPA 模型的一次<strong>底层算法重构</strong>。你可以理解为 V-JEPA-2 证明了 JEPA 能看懂世界，而 LeJEPA 找到了让它学得更稳、更高效的数学方法。 —— <h4 id="为什么-lejepa-对你关注的视频增强或规划很重要">为什么 LeJEPA 对你关注的“视频增强”或“规划”很重要？</h4> <p>如果你关注的是 <strong>arXiv:2512.24497</strong>（物理规划）那篇论文，你会发现一个趋势：<strong>特征空间的质量直接决定了规划的成功率。</strong></p> </li> <li><strong>LeJEPA</strong> 强制让特征变成圆球状的高斯分布（Isotropic Gaussian），这使得在特征空间计算“距离”和“代价（Value Function）”变得非常简单且准确。</li> <li>相比之下，传统的 V-JEPA 特征空间可能不够“圆”，在做复杂物理规划时容易产生偏差。 <h2 id="机会">机会</h2> <h3 id="1-真正讨论-jepa-引导生成的关键论文">1. 真正讨论 JEPA 引导生成的关键论文</h3> <p>如果你对“用 JEPA 引导 Diffusion”感兴趣，这篇是目前最权威的：</p> </li> <li><strong>论文标题：</strong> 《<strong>VideoREPA: Video Representation Learning for Video Generation</strong>》</li> <li><strong>arXiv 编号：</strong> <a href="https://arxiv.org/abs/2412.15214">arXiv:2412.15214</a></li> <li><strong>核心内容：</strong> 这篇论文直接回应了你的疑问。它系统地研究了不同的视频表征（包括类似 JEPA 的预测性表征）如何影响视频生成模型。结论是：<strong>具备物理演化逻辑的特征比纯语义特征更能提升视频生成的质量和稳定性。</strong> <h3 id="2-真正讨论-jepa-物理一致性的论文">2. 真正讨论 JEPA 物理一致性的论文</h3> <p>关于 JEPA 为什么能提供物理一致性（解决抖动和形变）：</p> </li> <li><strong>论文标题：</strong> 《<strong>Revisiting Feature Prediction for Learning Visual Representations from Video</strong>》 (V-JEPA 官方论文)</li> <li><strong>arXiv 编号：</strong> <a href="https://arxiv.org/abs/2404.08471">arXiv:2404.08471</a></li> <li><strong>核心内容：</strong> 虽然这是底座论文，但它详述了 <strong>Masking 策略</strong>如何迫使模型理解物体的“跨帧关联”。这是所有后续“视频增强”任务利用其作为先验的理论基础。 <h3 id="3-关于世界模型与潜在空间预测的深入讨论">3. 关于“世界模型”与潜在空间预测的深入讨论</h3> <p>关于“Scaling Laws”和潜在空间预测对生成任务的增益：</p> </li> <li><strong>论文标题：</strong> 《<strong>Learning and Leveraging World Models in Visual Representation Learning</strong>》</li> <li><strong>arXiv 编号：</strong> <a href="https://arxiv.org/abs/2403.00504">arXiv:2403.00504</a></li> <li> <h2 id="核心内容-讨论了像-jepa-这样的潜在空间预测架构如何规模化scaling以及这种预测能力如何转化为对视频内容的深刻理解"><strong>核心内容：</strong> 讨论了像 JEPA 这样的潜在空间预测架构如何规模化（Scaling），以及这种预测能力如何转化为对视频内容的深刻理解。</h2> <h3 id="为什么会出现标题混淆">为什么会出现“标题混淆”？</h3> <p>在目前的学术前沿，V-JEPA-2 及其 AC（Action-Conditioned）变体主要被用于<strong>具身智能</strong>和<strong>物理规划</strong>（正如你提到的 2512.24497 和 2601.00844）。 而在<strong>视频增强/生成</strong>领域，研究者通常不直接搜索“V-JEPA 视频增强”，而是搜索：</p> <ol> <li><strong>“SSL (Self-Supervised Learning) Priors for Video Diffusion”</strong></li> <li><strong>“Latent Dynamics for Consistent Video Synthesis”</strong> 这是一个非常敏锐的技术直觉。在 2025-2026 年的学术语境下，<strong>JEPA (理解世界规律)</strong> 与 <strong>Gaussian Splatting (重建世界表观)</strong> 的结合，被认为是通往“具身智能”和“高度真实模拟器”的必经之路。 如果说 <strong>Gaussian Splatting (3DGS/4DGS)</strong> 提供了物理世界的<strong>“皮囊”</strong>（极其逼真的视觉重建），那么 *<em>JEPA*</em> 就提供了<strong>“灵魂”</strong>（底层的物理逻辑和语义演化）。 以下是这两者结合的三个核心机会点和最新的研究方向分析： —— <h3 id="1-物理增强的-4d-变形场-physics-aware-4dgs">1. 物理增强的 4D 变形场 (Physics-Aware 4DGS)</h3> <p>目前的 4D Gaussian Splatting（动态场景重建）主要依赖于<strong>像素对齐</strong>。当视频中出现快速运动或剧烈遮挡时，Gaussian 点云的运动轨迹往往会“崩坏”，产生不符合物理的形变。</p> </li> </ol> </li> <li><strong>机会点：</strong> 利用 <strong>V-JEPA-2-AC</strong> 的动作预测能力。</li> <li><strong>实现逻辑：</strong> 将 V-JEPA 学习到的特征轨迹作为 4DGS 变形场（Deformation Field）的 <strong>Regularizer（正则项）</strong>。</li> <li><strong>价值：</strong> 即使摄像头看不清某个物体的背面，V-JEPA 提取的物理特征也能“指导” Gaussian 点如何按照重力、惯性进行位移，从而解决动态重建中的“伪影”问题。 <h3 id="2-语义高斯分布-semantic-splatting-with-jepa-features">2. 语义高斯分布 (Semantic Splatting with JEPA Features)</h3> <p>3DGS 最大的缺陷是它“只懂颜色，不懂物体”。每个 Gaussian 点只是一团带有颜色的椭圆。</p> </li> <li><strong>机会点：</strong> 将 JEPA 的 <strong>Latent Embeddings（潜在嵌入）</strong> 蒸馏（Distill）到每个 Gaussian 点中。</li> <li><strong>研究现状：</strong> 类似于 <em>Language Embedded Gaussians</em>，但使用 JEPA 特征。</li> <li><strong>应用：</strong> <ul> <li><strong>交互式编辑：</strong> 因为 Gaussian 点带有了 JEPA 的语义特征，你可以直接下达指令“把那个杯子移走”，模型知道哪些点属于“杯子”。</li> <li><strong>机器人操作：</strong> 机器人可以通过 3DGS 看到高质量的画面，同时通过附着的 JEPA 特征知道哪个部位是可以抓取的，哪个部位是脆弱的。 <h3 id="3-基于-jepa-的脑内交互模拟器-action-conditioned-simulator">3. 基于 JEPA 的“脑内”交互模拟器 (Action-Conditioned Simulator)</h3> <p>这是目前最符合 <strong>V-JEPA-2-AC</strong> 与 <strong>LeJEPA</strong> 发展趋势的方向。</p> </li> </ul> </li> <li><strong>痛点：</strong> 传统的机器人训练依赖于庞大的物理引擎（如 Isaac Sim），但这些引擎的视觉真实感往往不够。</li> <li><strong>解决方案：</strong> <ol> <li><strong>静态/动态重建：</strong> 用 Gaussian Splatting 快速扫描现实场景，建立高保真环境。</li> <li><strong>动态预测：</strong> 接入 <strong>V-JEPA-2-AC</strong>。当你输入一个“推”的动作，JEPA 在特征空间预测物体的后续状态。</li> <li><strong>渲染反馈：</strong> 预测出的特征驱动 3DGS 进行实时渲染更新。</li> </ol> </li> <li> <h2 id="机会-建立一个所见即所得的自监督模拟器机器人直接在-3dgs-重建的现实场景中试错而不需要人工建模"><strong>机会：</strong> 建立一个<strong>“所见即所得”的自监督模拟器</strong>。机器人直接在 3DGS 重建的现实场景中“试错”，而不需要人工建模。</h2> <h3 id="2025-2026-最新论文趋势分析">2025-2026 最新论文趋势分析</h3> <p>结合你之前提到的几篇论文（2512.24497 和 2601.00844），我们可以看到以下融合趋势： | <strong>融合维度</strong> | <strong>关键技术</strong> | <strong>代表性机会</strong> | | ———— | —————————– | —————————————————- | | <strong>空间维度</strong> | 3D-JEPA + 3DGS | 实现<strong>自监督</strong>的 3D 场景解析，无需人工标注标签。 | | <strong>时间维度</strong> | V-JEPA-2 + 4DGS | 解决长序列动态视频重建中的<strong>时空不一致</strong>问题。 | | <strong>动作维度</strong> | V-JEPA-AC + Dynamic Splatting | 创造出能根据不同动作指令<strong>实时形变</strong>的数字孪生场景。 |</p> <h3 id="总结你的研究机会在哪里">总结：你的研究机会在哪里？</h3> <p>如果你正在寻找论文题目或项目方向，我建议关注：<strong>“如何利用 LeJEPA (2511.08544) 的各向同性特征空间来加速 Gaussian Splatting 的收敛速度”。</strong></p> </li> <li><strong>核心假设：</strong> 如果特征空间更平滑、更符合高斯分布（LeJEPA 的贡献），那么在进行特征蒸馏时，Gaussian 点能更快地学习到物体的边界和物理属性，从而减少重建所需的视角数量（Few-shot Reconstruction）。 <strong>这个方向目前非常新，还没有出现统治级的 SOTA 模型，你是否有兴趣尝试用 Python 实现一个简单的 JEPA 特征到 Gaussian 点的映射实验？</strong></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[简要介绍]]></summary></entry><entry><title type="html">Tracking</title><link href="https://cekxm.github.io/blog/2026/tracking/" rel="alternate" type="text/html" title="Tracking"/><published>2026-02-04T09:49:40+00:00</published><updated>2026-02-04T09:49:40+00:00</updated><id>https://cekxm.github.io/blog/2026/tracking</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/tracking/"><![CDATA[<h2 id="samv2">SAMv2</h2> <h2 id="dam4samv2">DAM4SAMv2</h2> <p>[jovanavidenovic/DAM4SAM: <a href="https://github.com/jovanavidenovic/DAM4SAM">CVPR 2025] “A Distractor-Aware Memory for Visual Object Tracking with SAM2”</a></p> <p><a href="https://github.com/SebastianJanampa/DAM4SAMv2#">SebastianJanampa/DAM4SAMv2: This repository is based on the DAM4SAM repo. It includes more features such as creating mask using bounding boxes and points.</a></p> <p>速度慢，不适合实时跟踪，但对变形的目标和遮挡效果很不错。</p> <h2 id="mixformerv2">MixFormerV2</h2> <p>[MCG-NJU/MixFormerV2: <a href="https://github.com/MCG-NJU/MixFormerV2/tree/main">NeurIPS 2023] MixFormerV2: Efficient Fully Transformer Tracking</a></p> <h2 id="siamabc">SiamABC</h2> <p><a href="https://github.com/wvuvl/SiamABC">wvuvl/SiamABC: Improving Accuracy and Generalization for Efficient Visual Tracking</a></p> <p>WACV 2025，安装需要 pytorch 1.7.1</p>]]></content><author><name></name></author><summary type="html"><![CDATA[单目标跟踪]]></summary></entry><entry><title type="html">Diffusion Transformer</title><link href="https://cekxm.github.io/blog/2026/DiTF/" rel="alternate" type="text/html" title="Diffusion Transformer"/><published>2026-01-10T10:45:11+00:00</published><updated>2026-01-10T10:45:11+00:00</updated><id>https://cekxm.github.io/blog/2026/DiTF</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/DiTF/"><![CDATA[<h2 id="dit">DiT</h2> <p><strong>Diffusion Transformer (DiT)</strong> 是一种将 <strong>Transformer 架构</strong>与<strong>扩散模型（Diffusion Models）</strong>相结合的新型生成模型架构。它在图像生成领域逐渐取代了传统的以 UNet 为核心的架构（如早期的 Stable Diffusion），并成为当前最先进模型（如 SD3、Flux、Sora）的核心底座 。 以下是根据您提供的论文资料对 DiT 的详细介绍：</p> <h3 id="1-核心定义与架构">1. 核心定义与架构</h3> <p>DiT 摒弃了扩散模型中常用的带有下采样和上采样操作的 UNet 架构，转而使用完全基于 <strong>Transformer</strong> 的块（Blocks）来处理图像数据 。</p> <ul> <li><strong>图像 Patch 化</strong>：输入图像或潜空间表示（Latent）首先被切分为一个个小方块（Patches），并展平为一串序列（Tokens），这与 Vision Transformer (ViT) 的处理方式一致 。</li> <li><strong>AdaLN-zero 调制</strong>：这是 DiT 的关键技术之一。它根据扩散的时间步（Timestep）和文本条件，回归出缩放（Scale）和偏移（Shift）参数，通过 <strong>自适应层归一化（AdaLN）</strong> 来调制 Transformer 块内的特征 。</li> <li><strong>多模态交互（MM-DiT）</strong>：在 SD3 等模型中，DiT 演进为多模态架构，拥有独立的流（Streams）来分别处理视觉特征和文本 Token，并通过注意力机制实现两者的双向信息流转 。 <h3 id="2-dit-的主要优势">2. DiT 的主要优势</h3> <p>相比传统的 UNet 架构，DiT 展现出了以下优越性：</p> </li> <li><strong>可扩展性（Scalability）</strong>：DiT 非常适合通过增加参数量和训练数据来提升性能（Scaling law） 。</li> <li><strong>生成质量与鲁棒性</strong>：在细节生成和图像质量方面表现更好，具有更强的空间感知和语义理解能力 。</li> <li><strong>模型效率</strong>：Transformer 架构在现代硬件上具有极高的计算效率 。 <h2 id="dit-代表">DiT 代表</h2> <p>在当前的人工智能领域，<strong>DiT (Diffusion Transformer)</strong> 架构已经成为生成式模型的主流选择。以下是 DiT 架构最具有代表性的几个模型及其应用场景：</p> <h3 id="1-基础鼻祖dit-peebles--xie-2023">1. 基础鼻祖：DiT (Peebles &amp; Xie, 2023)</h3> </li> <li><strong>地位</strong>：这是该架构的开山之作。</li> <li><strong>特点</strong>：论文《Scalable Diffusion Models with Transformers》首次证明了使用 Transformer 替代传统的 UNet 作为扩散模型的主干网络是可行的，并且具有极强的<strong>可扩展性（Scaling Law）</strong>。</li> <li><strong>代表性设计</strong>：引入了 <strong>AdaLN-Zero</strong>（自适应层归一化）来将时间步和类别信息注入模型。 <h3 id="2-图像生成领域的标杆">2. 图像生成领域的标杆</h3> </li> <li><strong>Stable Diffusion 3 (SD3)</strong>： <ul> <li><strong>地位</strong>：Stability AI 推出的最强开源图像生成模型之一。</li> <li><strong>创新</strong>：使用了 <strong>MM-DiT（Multimodal DiT）</strong>，即文本和图像拥有各自独立的权重流，但在注意力层中进行交互。</li> </ul> </li> <li><strong>Flux.1</strong>： <ul> <li><strong>地位</strong>：目前开源界公认的画质与文字生成最强的模型（由原 SD 核心团队开发的 Black Forest Labs 推出）。</li> <li><strong>特点</strong>：基于大规模参数的 DiT 架构，解决了之前模型在复杂指令遵循和人类手部生成上的难题。</li> </ul> </li> <li><strong>PixArt-α / PixArt-Σ</strong>： <ul> <li><strong>特点</strong>：主打高效训练的 DiT 模型，通过解耦训练策略，在较小的计算资源下达到了接近 Midjourney 的生成质量。 <h3 id="3-视频生成领域的统治者">3. 视频生成领域的统治者</h3> </li> </ul> </li> <li><strong>Sora (OpenAI)</strong>： <ul> <li><strong>地位</strong>：视频生成领域的里程碑。</li> <li><strong>技术核心</strong>：OpenAI 明确指出 Sora 是一种 <strong>Diffusion Transformer</strong>。它将视频切分为时空补丁（Spacetime Patches），在 Transformer 架构中处理，实现了极长且一致的视频生成。</li> </ul> </li> <li><strong>可灵 (Kling) / Vidu</strong>： <ul> <li><strong>地位</strong>：国产大模型中性能顶尖的视频生成模型，其底层架构同样深度参考了 DiT 的时空建模方案。 <h2 id="pixart-α">PixArt-α</h2> <p><img src="/images/2026-01-10-DiTF/image-20260110184036225.png" alt="image-20260110184036225" class="img-fluid" data-zoomable=""/></p> <h2 id="ditf">DiTF</h2> <p>DiFT（Diffusion Transformer Feature）是一项发表于 <strong>NeurIPS 2025</strong> 的研究，其核心结论围绕着如何“释放”预训练 Diffusion Transformer (DiT) 模型在视觉感知（如视觉对应任务）中的潜力。 以下是该论文的主要结论：</p> <h3 id="1-发现-dit-特征中的巨量激活现象-massive-activations">1. 发现 DiT 特征中的“巨量激活”现象 (Massive Activations)</h3> </li> </ul> </li> <li><strong>现象描述</strong>：与传统的 Stable Diffusion (SD) 模型不同，DiT 模型（如 SD3、Flux）在提取特征时，极少数维度的激活值会比其他维度高出 <strong>100 倍以上</strong> 。</li> <li><strong>空间特性</strong>：这些巨量激活并不是随机分布的，而是<strong>固定集中在极少数特定的通道维度</strong>上（例如 SD3-5 在第 676 维，Flux 在第 154 和 1446 维），且出现在图像的所有 patch token 中 。</li> <li><strong>负面影响</strong>：由于这些极值维度不包含局部语义信息，会导致特征向量在余弦相似度计算时表现出极高的相似性，使得特征变得<strong>无区分度</strong>，从而导致在视觉任务中表现糟糕 。 <h3 id="2-揭示巨量激活与-adaln-层的关联">2. 揭示巨量激活与 AdaLN 层的关联</h3> </li> <li><strong>核心成因</strong>：研究发现这些巨量激活的维度与 DiT 内部的 <strong>自适应层归一化 (AdaLN)</strong> 产生的残差缩放因子 \(\alpha_k\) 高度一致 。</li> <li><strong>AdaLN 的双重作用</strong>：虽然 AdaLN 参与了巨量激活的形成，但它同时也具备<strong>定位和抑制</strong>这些激活的能力 。研究证明，通过 AdaLN 进行通道调制，可以有效归一化这些异常值 。 <h3 id="3-提出了免训练的-ditf-提取框架">3. 提出了免训练的 DiTF 提取框架</h3> </li> <li><strong>AdaLN 调制</strong>：DiTF 并不是直接提取原始特征，而是利用模型内置的 AdaLN 对特征进行<strong>通道维度的缩放和偏移调制</strong>。这种操作显著增强了特征的空间语义一致性和边界清晰度 。</li> <li><strong>通道舍弃策略 (Channel Discard)</strong>：对于调制后仍残留的微弱巨量激活维度，DiTF 采用直接<strong>置零</strong>的策略，进一步消除噪声 。</li> <li><strong>无需训练</strong>：该框架是一个 <strong>Training-free</strong>（免训练）的方案，可以直接应用于现有的预训练 DiT 模型 。 <h3 id="4-性能达到-sota-并与-dinov2-互补">4. 性能达到 SOTA 并与 DINOv2 互补</h3> </li> <li><strong>性能优越</strong>：DiTF 在多个视觉对应任务（如 SPair-71k, AP-10K）上超越了基于 DINOv2 和基于 SD 的模型 。例如，DiTF(Flux) 在 SPair-71k 上的表现比之前的 DIFT 高出 <strong>9.4%</strong> 。</li> <li><strong>特征互补性</strong>：实验显示，DiTF 与 DINOv2 的特征具有互补性。将两者结合使用时，性能会进一步大幅提升（例如在 SPair-71k 上从 64.6% 提升至 <strong>72.2%</strong>） 。 <h2 id="ditf-vs-dino2">DiTF vs DINO2</h2> <p>DiT 特征（DiTF）并非简单的直接提取，而是一种<strong>免训练的、基于 AdaLN（自适应层归一化）调制的增强特征提取框架</strong>。 以下是其与 DINOv2 的区别以及优于后者的原因：</p> <h3 id="1-dit-特征与-dinov2-的本质区别">1. DiT 特征与 DINOv2 的本质区别</h3> </li> <li><strong>提取机制不同</strong>：DINOv2 是通过自监督学习直接训练的视觉骨干网络，其特征可直接提取使用 。而 DiT（如 SD3、Flux）在直接提取原始特征时，会由于“<strong>巨量激活</strong>”（Massive Activations）现象导致表现极差 。DiTF 框架通过 DiT 内部自带的 <strong>AdaLN 层</strong>对这些异常激活进行通道维度的调制（缩放和偏移），从而提取出具有语义区分度的特征 。</li> <li><strong>解决的核心问题不同</strong>：DINOv2 侧重于通用视觉表征；而 DiTF 侧重于消除 DiT 特征中极少数固定维度上的极端激活值（这些值比中值大 100 倍以上，且不包含局部信息），使模型能够像 Stable Diffusion (SD) 一样有效地用于视觉感知任务 。 <h3 id="2-为什么-dit-特征优于-dinov2">2. 为什么 DiT 特征优于 DINOv2？</h3> <p>研究表明，DiTF 在视觉对应（Visual Correspondence）任务中建立了一系列新的性能标杆，超越了 DINOv2 和基于 SD 的模型 ：</p> </li> <li><strong>更强的语义边界和一致性</strong>：经过 AdaLN 调制后的 DiT 特征（Post-AdaLN）在空间语义相干性上显著优于 DINOv2 。可视化显示，它能更清晰地界定物体部位的语义边界，而原始特征或某些自监督特征在区分物体与背景时相对较弱 。</li> <li><strong>强大的生成先验</strong>：DiT 模型（如 SD3、Flux）在大规模文本到图像生成任务中预训练，具备极强的空间感知和语义理解能力 。通过 DiTF 框架“释放”这些潜能后，其特征在处理复杂语义对应（如跨物种对应）时表现出更强的鲁棒性 。</li> <li><strong>特征互补性</strong>：虽然 DiTF 本身性能强劲，但它与 DINOv2 特征具有互补性 。实验显示，将 DiTF 与 DINOv2 特征集成后，性能会进一步大幅提升（例如在 Spair-71k 数据集上从 64.6% 提升至 72.2%） 。 <h3 id="3-ditf-的核心改进手段">3. DiTF 的核心改进手段</h3> <ol> <li><strong>AdaLN 通道调制</strong>：利用 DiT 内部的 AdaLN 层，自适应地定位并归一化巨量激活，增强特征的区分度 。</li> <li><strong>通道舍弃策略（Channel Discard）</strong>：在调制后，主动丢弃（置零）那些仍然存在的、包含局部信息极少的异常维度，以进一步消除对表征学习的负面影响 。</li> </ol> </li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[DiT, SD3, Flux, PixArt, DiTF]]></summary></entry><entry><title type="html">Under water enhancement</title><link href="https://cekxm.github.io/blog/2026/uie/" rel="alternate" type="text/html" title="Under water enhancement"/><published>2026-01-10T03:49:29+00:00</published><updated>2026-01-10T03:49:29+00:00</updated><id>https://cekxm.github.io/blog/2026/uie</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/uie/"><![CDATA[<p><img src="/images/2026-01-10-uie/image-20260110114242376.png" alt="image-20260110114242376" class="img-fluid" data-zoomable=""/></p> <h2 id="five-a-network">Five A+ Network</h2> <p>JIANG J, YE T, CHEN S, et al. Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement [C]// Proceedings of the 34th British Machine Vision Conference (BMVC). 2023.</p> <p><img src="/images/2026-01-10-uie/image-20260110114546113.png" alt="image-20260110114546113" class="img-fluid" data-zoomable=""/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[UIE, U-trans, DNnet, Five A+]]></summary></entry><entry><title type="html">Optical Flow</title><link href="https://cekxm.github.io/blog/2026/optical_flow/" rel="alternate" type="text/html" title="Optical Flow"/><published>2026-01-09T07:08:29+00:00</published><updated>2026-01-09T07:08:29+00:00</updated><id>https://cekxm.github.io/blog/2026/optical_flow</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/optical_flow/"><![CDATA[<h2 id="raft">RAFT</h2> <p><img src="/images/2026-01-09-optical_flow/image-20260108232610993.png" alt="image-20260108232610993" class="img-fluid" data-zoomable=""/></p> <h3 id="核心架构与组成部分">核心架构与组成部分</h3> <p>RAFT 主要由以下三个核心阶段组成：</p> <ul> <li><strong>特征提取（Feature Extraction）：</strong> <ul> <li>使用一个卷积特征编码器对两帧输入图像进行处理，提取每个像素的特征向量 。</li> <li>此外，还包含一个上下文编码器（Context Encoder），仅从第一帧图像中提取特征，用于后续迭代更新时的补充信息。</li> </ul> </li> <li><strong>计算视觉相似度（Computing Visual Similarity）：</strong> <ul> <li><strong>4D 相关体（4D Correlation Volumes）：</strong> 通过计算两帧图像特征向量之间的内积，构建一个包含所有像素对相关性的 4D 体。</li> <li><strong>相关性金字塔（Correlation Pyramid）：</strong> 对 4D 相关体进行多尺度池化，生成一组不同分辨率的相关性卷。这种设计使模型能同时获取大位移和小位移的信息，且由于保留了高分辨率维度，能更好地捕捉小而快移动物体的运动。</li> </ul> </li> <li><strong>迭代更新（Iterative Updates）：</strong> <ul> <li><strong>GRU 单元：</strong> 采用一个基于门控循环单元（GRU）的轻量级更新模块，它通过在相关性金字塔中执行“查表”操作来检索特征，并迭代地更新光流场。</li> <li>与之前常见的“由粗到精”（coarse-to-fine）设计不同，RAFT 始终在固定高分辨率下维护和更新单一的光流场。</li> </ul> </li> </ul> <p>在 RAFT（Recurrent All-Pairs Field Transforms）光流估计方法中，<strong>不需要</strong>像传统的“由粗到精”（coarse-to-fine）方法那样对图像或特征图进行显式的 Warp（重采样/扭曲）操作 。</p> <h2 id="sea-raft">SEA-RAFT</h2> <p><img src="/images/2026-01-09-optical_flow/image-20260108233119443.png" alt="image-20260108233119443" class="img-fluid" data-zoomable=""/></p> <p>SEA-RAFT 旨在通过对原始 RAFT 架构的优化，实现更简单（Simple）、更高效（Efficient）和更准确（Accurate）的光流估计 2。其核心改进包括引入了新的损失函数、直接回归初始光流、刚体运动预训练以及架构简化。</p> <h3 id="核心改进与技术特点">核心改进与技术特点</h3> <ul> <li><strong>混合拉普拉斯损失（Mixture of Laplace Loss）：</strong> <ul> <li>SEA-RAFT 摒弃了标准 RAFT 使用的 \(L_1\) 损失，转而训练网络预测混合拉普拉斯分布的参数。</li> <li>该设计允许模型量化预测的不确定性，特别是在重度遮挡导致的歧义情况下，通过不同的混合分量来应对，从而显著减少过拟合并提升泛化能力。</li> </ul> </li> <li><strong>直接回归初始光流（Directly Regressed Initial Flow）：</strong> <ul> <li>原始 RAFT 将光流初始化为零，这往往偏离真实值较远，需要大量迭代才能收敛。</li> <li>SEA-RAFT 通过复用现有的上下文编码器（Context Encoder），直接根据输入的双帧图像预测一个初始光流估计。</li> <li>这一简单的改变以极低的额外开销大幅减少了所需的迭代次数，提升了效率。</li> </ul> </li> <li><strong>大尺度刚体运动预训练（Rigid-Flow Pre-Training）：</strong> <ul> <li>模型首先在 TartanAir 数据集上进行预训练。虽然该数据集仅包含由相机运动产生的静态场景位移（刚体运动），但其高度的真实感和场景多样性显著增强了模型的跨数据集泛化能力。</li> </ul> </li> <li><strong>架构简化（Architectural Simplifications）：</strong> <ul> <li><strong>标准化主干：</strong> 将 RAFT 原有的定制化编码器替换为标准的 ResNet 模型，使得训练更加稳定。</li> <li>高效 RNN：** 将原始的卷积 GRU 替换为由 ConvNext 模块组成的简单 RNN，这使得模型更容易集成新的神经构建块，且更易于扩展到大规模数据集。</li> </ul> </li> </ul> <h2 id="flowseek">FlowSeek</h2> <p>M. Poggi and F. Tosi, “FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2025, pp. 5667-5679.</p> <p><img src="/images/2026-01-09-optical_flow/image-20260109130959464.png" alt="image-20260109130959464" class="img-fluid" data-zoomable=""/></p> <p>FlowSeek 是一种在 2025 年 ICCV 上提出的新型光流估计方法，其核心理念是通过引入<strong>深度基础模型（Depth Foundation Models）</strong>和<strong>经典运动基底（Motion Bases）</strong>来简化光流任务的训练并提升其泛化能力。</p> <p>以下是 FlowSeek 方法的详细介绍：</p> <h3 id="1-核心设计理念融合三大领域">1. 核心设计理念：融合三大领域</h3> <p>FlowSeek 的架构设计处于以下三个领域的交汇点：</p> <ul> <li><strong>先进光流架构：</strong> 以 <strong>SEA-RAFT</strong> 作为骨干网络（Backbone），继承了其混合拉普拉斯损失和高效的迭代更新机制。</li> <li><strong>深度基础模型 (VFMs)：</strong> 集成了如 <strong>Depth Anything v2</strong> 等在大规模数据集上预训练的模型，利用其丰富的几何和语义先验信息。</li> <li><strong>经典运动参数化：</strong> 引入了 30 年前经典的低维运动基底理论，将相机运动诱导的光流简化为 6 个自由度的线性组合。</li> </ul> <h3 id="2-详细架构组成">2. 详细架构组成</h3> <p>FlowSeek 在 SEA-RAFT 的基础上增加了以下关键模块：</p> <ul> <li><strong>特征增强：</strong> 利用深度基础模型的解码器特征 \(\Phi\) 增强原始的卷积特征 \(F\)，从而构建更具几何感知力的 4D 相关体。</li> <li><strong>BasesNet（基底网络）：</strong> 核心创新模块。它根据预测的逆深度图 \(D_0\) 计算出 8 个运动基底矢量（包括平移和旋转分量），并提取运动特征 \(H_B\)。</li> <li><strong>上下文增强：</strong> 将预测的深度图与原始图像一同输入 ContextNet，以提取更强的上下文特征 \(C\) 和初始隐藏状态 \(H^0\) 。</li> <li><strong>迭代更新：</strong> 改进后的 UpdNet 在迭代过程中同时参考相关性查找结果、隐藏状态以及来自运动基底的特征。</li> </ul> <h3 id="3-主要优势与改进点">3. 主要优势与改进点</h3> <ul> <li><strong>极低的硬件成本：</strong> FlowSeek 仅需 <strong>单张消费级 GPU（如 RTX 3090）</strong> 即可完成训练，相比于 SEA-RAFT 等方法所需的 8 张 GPU，硬件预算降低了约 8 倍。</li> <li><strong>卓越的泛化能力：</strong> 通过利用深度先验，FlowSeek 在未见过的场景（如 Sintel 和 KITTI）中表现出极强的 Zero-shot 泛化性能，比 SEA-RAFT 提升了 10% 到 15%。</li> <li><strong>更细腻的细节表现：</strong> 深度基础模型的引入使得光流预测在物体边缘和精细结构上更加准确，解决了传统模型在跨域测试时细节缺失的问题。</li> </ul> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>SEA-RAFT</strong></th> <th><strong>FlowSeek</strong></th> </tr> </thead> <tbody> <tr> <td><strong>先验知识</strong></td> <td>仅依赖图像特征</td> <td>图像特征 + 深度图 + 运动基底</td> </tr> <tr> <td><strong>硬件需求</strong></td> <td>通常需要 8x 3090 GPU</td> <td><strong>1x 3090 GPU</strong></td> </tr> <tr> <td><strong>性能 (Spring EPE)</strong></td> <td>~4.79 (S版)</td> <td><strong>~3.31 (L版)</strong></td> </tr> <tr> <td><strong>关键改进</strong></td> <td>混合损失、直接回归初始流</td> <td><strong>基础模型特征融合、BasesNet</strong></td> </tr> <tr> <td><strong>复杂度</strong></td> <td>较低</td> <td>较高（需运行额外的深度模型）</td> </tr> </tbody> </table> <h2 id="neuflow-v2">NeuFlow-v2</h2> <p>Z. Zhang, A. Gupta, H. Jiang, and H. Singh, “NeuFlow v2: Push High-Efficiency Optical Flow To the Limit,” in 2024 IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), 2024, doi: 10.1109/IROS58592.2024.10802353.</p> <p><img src="/images/2026-01-09-optical_flow/image-20260109130930368.png" alt="image-20260109130930368" class="img-fluid" data-zoomable=""/></p> <p><strong>NeuFlow-V2</strong>（全称：NeuFlow v2: Push High-Efficiency Optical Flow To the Limit）是 2024 年提出的一款专为<strong>边缘设备和实时机器人应用</strong>设计的高效光流估计算法。</p> <p>它在 NeuFlow-V1 的基础上进行了深度优化，核心目标是：在保持与 SOTA（当前最佳）方法相当精度的同时，极大地降低计算开销。</p> <h3 id="1-核心技术创新点">1. 核心技术创新点</h3> <ul> <li><strong>极简骨干网络 (Simple Backbone)：</strong> <ul> <li>与传统方法（如 RAFT 使用较重的卷积网络）不同，NeuFlow-V2 认为光流任务更依赖<strong>低级特征</strong>而非高级语义。</li> <li>它直接从不同尺度的图像金字塔（1/2, 1/4, 1/8）中提取特征，每个卷积块仅包含两层卷积。这种设计大幅减少了计算量，同时保留了更多的纹理细节。</li> </ul> </li> <li><strong>全局匹配与交叉注意力 (Cross-Attention &amp; Global Matching)：</strong> <ul> <li>为了处理大位移运动，它在 1/16 低分辨率尺度上引入了 <strong>Cross-Attention</strong>。</li> <li>这增强了图像间特征的判别性，通过全局匹配计算出一个初始光流，为后续迭代提供一个良好的起点。</li> </ul> </li> <li><strong>轻量级 RNN 迭代模块：</strong> <ul> <li><strong>抛弃 GRU/LSTM：</strong> 原始 RAFT 使用的 GRU 单元计算相对复杂。NeuFlow-V2 使用基于简单卷积层的 RNN 结构来整合隐藏状态、当前光流和 Warp 后的相关性特征。</li> <li><strong>数值稳定性：</strong> 引入了 <code class="language-plaintext highlighter-rouge">HardTanh</code> 函数来约束隐藏状态的范围，解决了简单 RNN 容易出现的梯度消失或爆炸问题。</li> </ul> </li> <li><strong>多尺度特征融合：</strong> <ul> <li>通过将 1/16 尺度的全局上下文信息与 1/8 尺度的局部细节特征进行融合，弥补了轻量级骨干网络感受野较小的缺点。</li> </ul> </li> </ul> <h3 id="2-性能表现-效率王者">2. 性能表现 (效率王者)</h3> <p>NeuFlow-V2 的最大亮点在于其极致的运行效率，这使它成为目前在嵌入式硬件上表现最好的模型之一：</p> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>表现说明</strong></th> </tr> </thead> <tbody> <tr> <td><strong>推理速度</strong></td> <td>在 <strong>NVIDIA Jetson Orin Nano</strong> 上可达到 <strong>20+ FPS</strong> (512x384 分辨率)。</td> </tr> <tr> <td><strong>加速比</strong></td> <td>相比于 RAFT 或 GMFlow 等主流方法，实现了 <strong>10x - 70x</strong> 的加速。</td> </tr> <tr> <td><strong>精度水平</strong></td> <td>在 Sintel 和 KITTI 基准测试中，精度接近 SEA-RAFT 等 SOTA 模型，但计算成本极低。</td> </tr> <tr> <td><strong>跨域泛化</strong></td> <td>尽管模型轻量，但其设计确保了在真实世界场景（非训练集）中依然保持稳健的预测效果。</td> </tr> </tbody> </table> <h3 id="3-与-raft--sea-raft-的区别">3. 与 RAFT / SEA-RAFT 的区别</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>RAFT / SEA-RAFT</strong></th> <th><strong>NeuFlow-V2</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要目标</strong></td> <td>追求最高精度（科研/离线处理）</td> <td><strong>追求极致实时性（边缘端/机器人）</strong></td> </tr> <tr> <td><strong>迭代核心</strong></td> <td>复杂的 ConvGRU 单元</td> <td>简单的 CNN-based RNN</td> </tr> <tr> <td><strong>匹配方式</strong></td> <td>4D 相关体 + 局部查找</td> <td>全局匹配 (1/16) + 局部细化</td> </tr> <tr> <td><strong>应用场景</strong></td> <td>服务器端视频处理</td> <td><strong>无人机导航、移动机器人 SLAM</strong></td> </tr> </tbody> </table> <h2 id="对比">对比</h2> <p>FlowSeek 和 NeuFlow-V2 代表了光流估计领域的两个极端：<strong>FlowSeek 追求极致的泛化精度与细节还原</strong>（利用基础模型先验），而 <strong>NeuFlow-V2 追求极致的计算效率与嵌入式部署性能</strong>。</p> <p>以下是两者的详细对比分析：</p> <hr/> <h3 id="1-epe-误差精度与泛化能力">1. EPE 误差（精度与泛化能力）</h3> <p><strong>结论：FlowSeek 在绝对精度和零样本（Zero-shot）泛化能力上显著领先。</strong></p> <ul> <li><strong>FlowSeek (高精度标杆)：</strong> <ul> <li><strong>核心优势：</strong> 它引入了“深度基础模型”（如 Depth Anything v2），使模型具备了强大的空间几何感知能力。</li> <li><strong>性能表现：</strong> 在 Sintel Final 和 KITTI 数据集上，其精度比之前的 SOTA 模型 SEA-RAFT 提升了约 <strong>10%-15%</strong>。在 Spring 这种复杂数据集上，其 EPE 表现极其出色（L 版本可达到约 <strong>3.31</strong>）。</li> <li><strong>细节还原：</strong> 能够精确捕捉物体边缘，解决了很多模型在处理细小物体或快速运动时的模糊问题。</li> </ul> </li> <li><strong>NeuFlow-V2 (高效率平衡)：</strong> <ul> <li><strong>核心优势：</strong> 在极低功耗下维持了“可接受”的高精度。</li> <li><strong>性能表现：</strong> 其精度接近但略逊于 SEA-RAFT 和 RAFT。在 Sintel 数据集上的 EPE 通常在 <strong>2.0-3.0</strong> 左右（取决于具体测试子集和训练阶段）。</li> <li><strong>局限性：</strong> 为了换取速度，它使用了极简的骨干网络，在处理极复杂场景（如重度遮挡或光照剧烈变化）时，精度上限不如 FlowSeek。</li> </ul> </li> </ul> <hr/> <h3 id="2-计算量与效率计算开销">2. 计算量与效率（计算开销）</h3> <p><strong>结论：NeuFlow-V2 在推理速度和轻量化方面具有压倒性优势；FlowSeek 在训练成本上表现优异。</strong></p> <ul> <li><strong>NeuFlow-V2 (端侧王者)：</strong> <ul> <li><strong>推理速度：</strong> 专为边缘计算设计。在 <strong>Jetson Orin Nano</strong> 上能跑到 <strong>20+ FPS</strong>，而同类高精度模型通常只有 1-2 FPS 甚至更低。</li> <li><strong>架构极简：</strong> 使用简单的 CNN-RNN 替代了复杂的 GRU，去除了冗余的特征提取层，计算量（FLOPs）远低于其他模型。</li> <li><strong>部署友好：</strong> 它是目前机器人和无人机领域实时光流估计的首选。</li> </ul> </li> <li><strong>FlowSeek (训练高效，推理较重)：</strong> <ul> <li><strong>训练成本：</strong> 最大的卖点之一是<strong>单卡可训</strong>。传统 SOTA 模型往往需要 8 张 A100/H100，而 FlowSeek 仅需一张 RTX 3090 即可完成训练。</li> <li><strong>推理开销：</strong> 尽管在光流分支上进行了优化，但由于其依赖于外部的“深度基础模型”提供先验特征，推理时需要同时运行深度模型和光流模型，总体的计算延迟明显高于 NeuFlow-V2。</li> </ul> </li> </ul> <hr/> <h3 id="3-综合对比总结">3. 综合对比总结</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>FlowSeek</strong></th> <th><strong>NeuFlow-V2</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要定位</strong></td> <td>追求最高泛化精度（科研、离线分析）</td> <td>追求极致实时性（机器人、端侧部署）</td> </tr> <tr> <td><strong>EPE 表现</strong></td> <td><strong>极佳 (SOTA 级别)</strong></td> <td>优秀 (接近 SOTA，但略有妥协)</td> </tr> <tr> <td><strong>硬件需求</strong></td> <td>单张 3090 可训，推理需较强 GPU</td> <td><strong>Jetson 等嵌入式设备即可流畅运行</strong></td> </tr> <tr> <td><strong>核心创新点</strong></td> <td>深度基础模型 + 运动基底 (Motion Bases)</td> <td>极简骨干网络 + 轻量 RNN + 全局匹配</td> </tr> <tr> <td><strong>适用场景</strong></td> <td>高精度视频合成、运动捕捉、电影特效</td> <td><strong>无人机避障、SLAM、增强现实 (AR)</strong></td> </tr> </tbody> </table>]]></content><author><name></name></author><summary type="html"><![CDATA[RAFT, SEA-RAFT, Neuflow]]></summary></entry><entry><title type="html">Teaching Tailored To Talent</title><link href="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/" rel="alternate" type="text/html" title="Teaching Tailored To Talent"/><published>2026-01-01T01:20:30+00:00</published><updated>2026-01-01T01:20:30+00:00</updated><id>https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent</id><content type="html" xml:base="https://cekxm.github.io/blog/2026/Teaching-Tailored-to-Talent/"><![CDATA[<p><img src="/images/2026-01-01-Teaching-Tailored-to-Talent/image-20260101091803120.png" alt="image-20260101091803120" class="img-fluid" data-zoomable=""/></p> <h2 id="介绍">介绍</h2> <p>这篇论文介绍了一种名为 <strong>\(T^3\)-DiffWeather</strong> 的新型图像恢复框架，专门用于解决复杂和多变的恶劣天气条件（如雨、雪、雾等）下的图像修复问题。</p> <p>其核心理念是<strong>“因材施教”（Teaching Tailored to Talent）</strong>，通过结合 <strong>Prompt Learning（提示学习）</strong> 和 <strong>Diffusion Model（扩散模型）</strong>，使网络能够根据不同的天气退化情况动态地调整修复策略。</p> <p>以下是该方法的主要组成部分：</p> <h3 id="1-核心管线预测退化残差-degradation-residual">1. 核心管线：预测退化残差 (Degradation Residual)</h3> <p>不同于传统的扩散模型直接从噪声中恢复清晰图像，该论文将扩散模型的目标转向<strong>预测退化残差 \(r_d\)</strong>（即退化图像与清晰图像之间的差值） 3333。这种设计能够更清晰地表示退化特征，从而引导扩散过程更精准地重建背景。</p> <h3 id="2-提示词池-prompt-pool--针对天气退化">2. 提示词池 (Prompt Pool) —— 针对天气退化</h3> <p>为了应对现实世界中不可预测的天气组合，作者设计了一个<strong>提示词池 (Prompt Pool)</strong> ：</p> <ul> <li><strong>自主构建：</strong> 网络能根据输入图像的退化残差，从提示词池中自动选择并组合最相关的“子提示词”（Sub-prompts），构建出针对特定样本的“天气提示词”（Weather-prompts）。</li> <li><strong>灵活性：</strong> 这种方式通过共享子提示词来捕捉天气的相似性（如雾气和低对比度），同时利用独立的子提示词来区分不同天气的独特性。</li> </ul> <h3 id="3-depth-anything-约束的通用提示词--针对场景建模">3. Depth-Anything 约束的通用提示词 —— 针对场景建模</h3> <p>作者观察到，尽管天气千变万化，但被遮挡的图像背景场景往往具有共同特征。</p> <ul> <li><strong>通用提示词 (General Prompts)：</strong> 专门用于捕捉背景场景的共同属性，为扩散过程提供场景级约束。</li> <li><strong>Depth-Anything 约束：</strong> 论文首次提出利用预训练的 <strong>Depth-Anything</strong> 模型提取的鲁棒特征来引导这些通用提示词 10101010。由于 Depth-Anything 模型在极端天气下仍能保持极高的背景鲁棒性，它能提供准确的场景先验，使修复过程不受天气干扰。</li> </ul> <h3 id="4-对比提示词损失-contrastive-prompt-loss">4. 对比提示词损失 (Contrastive Prompt Loss)</h3> <p>为了确保上述两类提示词（天气提示词和场景通用提示词）能够各司其职，作者引入了<strong>对比提示词损失</strong>：</p> <ul> <li><strong>相互推开：</strong> 将针对天气的提示词和针对场景的提示词视为负样本对，通过对比学习增强各自的表征能力。</li> <li><strong>特征拉近：</strong> 引导提示词与其对应的特征嵌入（如 Depth-Anything 特征）在潜层空间中更接近。</li> </ul> <h3 id="5-高效的推理性能">5. 高效的推理性能</h3> <p>由于采用了精准的提示词条件引导和残差预测策略，\(T^3\)-DiffWeather 的效率极高：</p> <ul> <li><strong>采样步数：</strong> 仅需 <strong>2 步</strong> 采样即可达到优秀性能（相比之下，之前的 Weather Diffusion 模型通常需要更多步骤）。</li> <li><strong>计算开销：</strong> 推理时的计算复杂度仅为目前最先进（SOTA）方法的几十分之一。</li> </ul> <p><strong>总结：</strong> 该论文通过“天气提示词池”和“深度约束的背景提示词”实现了对复杂天气退化的解耦处理，在显著提升修复质量的同时，大幅降低了扩散模型的推理成本。</p> <h2 id="promt-的嵌入">Promt 的嵌入</h2> <p>根据论文中的描述，\(\hat{\mathcal{F}}_e\) 的生成和使用方式如下：</p> <h3 id="1-hatmathcalf_e-的生成过程">1. \(\hat{\mathcal{F}}_e\) 的生成过程</h3> <p>\(\hat{\mathcal{F}}_e\) 是通过<strong>交叉注意力（Cross-Attention）</strong>机制将两种不同类型的 Prompt 逐步嵌入到扩散网络（Diffusion Network）的潜层（Latent Layer）中得到的 111111：</p> <ul> <li><strong>第一步：</strong> 将潜层的特征嵌入 \(\mathcal{F}_e\) 作为 Query，与由<strong>天气提示词（Weather-Prompts）</strong> \(\mathcal{P}_w\) 生成的 Key 和 Value 进行注意力计算，得到中间特征 \(\mathcal{F}_e^{\prime}\) 。</li> <li><strong>第二步：</strong> 将 \(\mathcal{F}_e^{\prime}\) 作为 Query，与由<strong>通用提示词（General Prompts）</strong> \(\mathcal{P}_{gd}\) 生成的 Key 和 Value 再次进行注意力计算，最终输出 \(\hat{\mathcal{F}}_e\) 。</li> </ul> <h3 id="2-hatmathcalf_e-的最终用途">2. \(\hat{\mathcal{F}}_e\) 的最终用途</h3> <p>\(\hat{\mathcal{F}}_e\) 最终被用作<strong>扩散模型去噪过程中的核心条件（Condition）</strong>，具体体现在以下几个方面：</p> <ul> <li><strong>作为信息丰富的引导条件：</strong> 论文指出，\(\hat{\mathcal{F}}_e\) 承载了来自提示词池（Prompt Pool）的退化特征信息以及来自 Depth-Anything 约束的场景背景信息。这些信息共同构成了公式（1）中的条件 \(c\)，用于引导扩散模型从噪声中准确恢复出图像。</li> <li><strong>指导退化残差（Degradation Residual）的重建：</strong> 该论文将扩散模型的目标从直接生成清晰图像转变为生成“退化残差” \(r_d\)（即退化图像与清晰图像之差）。\(\hat{\mathcal{F}}_e\) 作为潜层特征，直接参与到这个残差的去噪重建过程中。</li> <li><strong>实现“因材施教”的恢复：</strong> 由于 \(\hat{\mathcal{F}}_e\) 包含了针对具体样本自适应选择的天气属性和稳健的场景先验，它使得扩散模型能够根据输入图像的具体退化类型（如雨、雪、雾的组合）进行针对性的修复。</li> </ul> <p>在公式（2）中，<strong>\(F_e\) 的原始信息确实通过残差连接（或类似的加和操作）被保留了下来</strong>，\(\hat{\mathcal{F}}_e\) 并不是简单地“扔掉”了 \(F_e\)，而是在 \(F_e\) 的基础上进行了<strong>信息增强</strong> 。</p> <p>以下是基于论文内容及引用 [67]（Stable Diffusion / LDM 架构）的详细确认：</p> <h3 id="f_e-的信息依然存在">\(F_e\) 的信息依然存在</h3> <p>在该论文中，作者明确指出其交叉注意力机制<strong>类似于 Stable Diffusion [67] 中的文本嵌入方式</strong> 22。在 SD 的标准实现中，潜层特征 \(F_e\) 的使用遵循以下逻辑：</p> <ul> <li><strong>计算逻辑：</strong> \(F_e\) 作为 <strong>Query (Q)</strong> 输入到 Attention 模块中。</li> <li><strong>残差更新：</strong> 最终传给下一层的特征通常遵循公式：\(\text{Output} = F_e + \text{Attention}(F_e, \text{Prompt})\)。</li> <li><strong>物理意义：</strong> 这意味着 \(\hat{\mathcal{F}}_e\) 实际上是“<strong>原始图像特征 \(F_e\) + 提示词引导的修正/补充信息</strong>”。\(F_e\) 提供了场景的基础结构和纹理，而 Prompt 信息负责告诉网络哪些部分需要针对天气退化进行调整。</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Prompt, Depth-Anything, Diffusion, Image Restoration]]></summary></entry><entry><title type="html">Vins And Esvo2</title><link href="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/" rel="alternate" type="text/html" title="Vins And Esvo2"/><published>2025-12-31T00:19:46+00:00</published><updated>2025-12-31T00:19:46+00:00</updated><id>https://cekxm.github.io/blog/2025/VINS-and-ESVO2</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/VINS-and-ESVO2/"><![CDATA[<h2 id="vins">VINS</h2> <h3 id="1-系统架构通用因子框架">1. 系统架构：通用因子框架</h3> <p>该方法的核心思想是将每种传感器都视为框架中的一个<strong>通用因子（Factor）</strong> 。</p> <ul> <li><strong>多传感器支持</strong>：框架支持多种传感器组合，如单目相机+IMU、双目相机、以及双目相机+IMU。</li> <li><strong>因子图构建</strong>：共享公共状态变量的因子会被累加在一起，构建一个位姿图（Pose Graph）优化问题 。</li> <li><strong>鲁棒性</strong>：由于不依赖特定传感器，系统可以轻松处理传感器失效的情况，通过移除失效因子的方式快速切换传感器组合。</li> </ul> <p>该方法不是学习类的方法。</p> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251226153814897.png" alt="image-20251226153814897" class="img-fluid"/></p> <p>系统通过一个滑动窗口（Sliding Window）来维护待优化的状态变量。以下是该方法中<strong>状态向量（State Vector）</strong>的详细构成：</p> <h4 id="完整的状态向量">完整的状态向量</h4> <p>在滑动窗口优化中，总的状态向量 \(\mathcal{X}\) 被定义为窗口内所有帧的状态、外参以及特征点深度的集合：</p> \[\mathcal{X} = [\mathbf{x}_m, \mathbf{x}_{m+1}, \dots, \mathbf{x}_n, \mathbf{x}_c^b, \lambda_1, \lambda_2, \dots, \lambda_l]\] <p>其中：</p> <ul> <li>\(m, n\) 是滑动窗口的起始帧和结束帧。</li> <li>\(\mathbf{x}_i\) 是第 \(i\) 帧对应的 <strong>IMU 核心状态</strong>。</li> <li>\(\mathbf{x}_c^b\) 是相机与IMU之间的 <strong>外参（Extrinsic Parameters）</strong>。</li> <li>\(\lambda_l\) 是第 \(l\) 个视觉特征点的 <strong>逆深度（Inverse Depth）</strong>。</li> </ul> <p><strong>和论文表示不太一样，但实际是一样的。这边body frame 的位姿状态放在IMU里面了。</strong></p> <hr/> <h5 id="具体的-imu-状态-mathbfx_i">具体的 IMU 状态 (\(\mathbf{x}_i\))</h5> <p>对于窗口中的每一帧 \(i\)，其核心状态变量包含 15 个维度：</p> \[\mathbf{x}_i = [\mathbf{p}_{b_i}^w, \mathbf{v}_{b_i}^w, \mathbf{q}_{b_i}^w, \mathbf{b}_a, \mathbf{b}_g]\] <ul> <li><strong>\(\mathbf{p}_{b_i}^w\) (3D Position)</strong>：IMU 坐标系在世界坐标系下的位置。</li> <li><strong>\(\mathbf{v}_{b_i}^w\) (3D Velocity)</strong>：IMU 在世界坐标系下的速度。</li> <li><strong>\(\mathbf{q}_{b_i}^w\) (Quaternion/Rotation)</strong>：从 IMU 坐标系到世界坐标系的旋转（通常用四元数表示 Hamilton 形式，或旋转矩阵）。</li> <li><strong>\(\mathbf{b}_a\) (Accelerometer Bias)</strong>：加速度计的零偏。</li> <li><strong>\(\mathbf{b}_g\) (Gyroscope Bias)</strong>：陀螺仪的零偏。</li> </ul> <hr/> <h5 id="辅助状态变量">辅助状态变量</h5> <p>为了保证系统的通用性和精度，论文还包括了以下状态：</p> <ul> <li> <p>传感器外参 (\(\mathbf{x}_c^b\))：</p> <p>包含从相机到 IMU 坐标系的变换 \([\mathbf{p}_c^b, \mathbf{q}_c^b]\)。在系统在线运行时，这些外参也可以被放入优化器进行实时修正。</p> </li> <li> <p>视觉特征点状态 (\(\lambda_l\))：</p> <p>VINS 采用<strong>逆深度（Inverse Depth）</strong>作为特征点的参数化方式。相比于直接使用 3D 坐标 \((x, y, z)\)，逆深度可以更好地处理距离相机非常远的特征点，且其分布更接近高斯分布，有助于数值优化的收敛。</p> </li> </ul> <h4 id="因子">因子</h4> <p>每一个方框是一个 factor，也就是一次测量。更细致的话</p> <ul> <li>相机：一个测量对应一个特征点在该时刻的位置。特征点使用KLT进行跟踪，从之前 \(l\) 时刻到当前 \(t\) 时刻有对应关系，这两个时刻在图像上的位置有约束。</li> <li>如果双目相机，则特征点在左边和右边的位置上，也有约束。</li> <li>IMU: 基于预积分结果的相邻关键帧间的运动约束残差。</li> </ul> <h3 id="2-代价函数-cost-function">2. 代价函数 (Cost Function)</h3> <p>该系统将状态估计建模为一个<strong>最大似然估计（MLE）</strong>问题。在假设测量噪声符合高斯分布的条件下，MLE被转化为一个非线性最小二乘问题，即最小化所有传感器测量残差的加权平方和。</p> <p>在滑动窗口优化中，总代价函数（MAP 形式）定义如下：</p> \[\mathcal{X}_{m:n}^* = \arg \min_{\mathcal{X}_{m:n}} \left\{ \sum_{t=m}^{n} \sum_{k \in S} \\mid z_t^k - h_t^k(\mathcal{X}_{m:n}) \\mid _{\Omega_t^k}^2 + \\mid H_p \delta \mathcal{X}_{m:n} - b_p \\mid ^2 \right\}\] <p>该代价函数主要由以下三部分组成：</p> <ul> <li><strong>视觉因子残差（Camera Factor）</strong>：特征点在不同帧之间的重投影误差 。</li> <li><strong>IMU 因子残差（IMU Factor）</strong>：基于预积分结果的相邻关键帧间的运动约束残差。</li> <li><strong>边缘化先验项（Prior Term）</strong>：由滑动窗口中被移除（边缘化）的旧状态所转换而来的约束信息，用于保留历史观测对当前状态的影响。</li> </ul> <h3 id="3-光束法平差-bundle-adjustment-ba">3. 光束法平差 (Bundle Adjustment, BA)</h3> <p>论文将状态估计过程称为<strong>基于滑动窗口的光束法平差</strong>。</p> <ul> <li><strong>基本原理</strong>：BA通过同时优化相机位姿、IMU状态（速度、偏置）以及特征点的逆深度，使得所有观测残差最小化。</li> <li><strong>求解方式</strong>： <ul> <li><strong>线性化</strong>：在当前估计值附近对代价函数进行一阶泰勒展开，将其转化为线性最小二乘问题。</li> <li><strong>迭代优化</strong>：采用 <strong>Gauss-Newton</strong> 或 <strong>Levenberg-Marquardt</strong> 方法进行多次迭代直至收敛。系统具体使用了 <strong>Ceres Solver</strong> 进行高效求解。</li> </ul> </li> <li><strong>计算效率控制</strong>： <ul> <li><strong>滑动窗口（Sliding Window）</strong>：为了维持实时性，优化只在固定大小的窗口（通常是10帧左右）内进行，而不是处理整个轨迹。</li> <li><strong>边缘化（Marginalization）</strong>：当新帧加入导致窗口满时，利用<strong>舒尔补（Schur Complement）</strong>将最老的帧移除，并将其携带的信息转化为当前窗口内状态的先验分布。</li> </ul> </li> </ul> <p>这种基于优化的方法相比于传统的滤波器（如EKF）具有更高的精度，因为它可以在非线性空间内进行多次迭代更新，并能更好地处理传感器之间的时钟同步偏差。</p> <h2 id="esvo2">ESVO2</h2> <p><img src="/images/2025-12-31-VINS-and-ESVO2/image-20251227073033444.png" alt="image-20251227073033444" class="img-fluid"/></p> <p><strong>ESVO2</strong> 是由湖南大学（Yi Zhou 教授团队）与香港科技大学（Shaojie Shen 教授团队）等合作开发的一个<strong>实时、紧耦合的双目事件相机视觉惯性里程计系统</strong> 。</p> <h3 id="概述">概述</h3> <p>相比前代系统，ESVO2 引入了以下四大创新 ：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：提出了一种新型的类图像事件表示方法，能够根据事件的局部动态自动调整累积时间。这使得系统能高效提取边缘轮廓点，而不受运动速度变化的影响。</li> <li><strong>引入 IMU 预积分</strong>：通过将 IMU 测量值作为运动先验，解决了纯视觉追踪在特定旋转维度（如 Pitch 和 Yaw）上的简并（Degeneracy）问题 。</li> <li><strong>紧耦合后端优化</strong>：设计了一个精简的后端，专门优化线性速度和 IMU 偏置（Bias），从而抑制轨迹漂移，确保全局一致性。</li> <li><strong>增强的建图模块</strong>：结合了“时间同步双目（Temporal Stereo）”和“静态双目（Static Stereo）”配置，并引入快速块匹配方案，显著提升了深度图的完整性和局部平滑度。</li> </ul> <h4 id="系统架构">系统架构</h4> <p>ESVO2 采用并行设计，由四个独立线程组成：</p> <ol> <li><strong>预处理线程</strong>：负责事件的自适应累积及时间表面（Time Surface）的更新。</li> <li><strong>追踪线程（Localization）</strong>：执行 3D-2D 时空配准，利用无偏移平滑时间表面（OS-TS）进行位姿估计。</li> <li><strong>建图线程（Mapping）</strong>：实时恢复半稠密深度图并维护局部 3D 地图。</li> <li><strong>后端线程（Back-end）</strong>：进行滑动窗口优化，不断更新速度和 IMU 参数。</li> </ol> <h3 id="mapping">Mapping</h3> <h4 id="temporal-stereo时间双目"><strong>Temporal Stereo（时间双目）</strong></h4> <p>是一种核心的深度估计技术。它通过结合<strong>时间维度</strong>（单相机的运动）和<strong>空间维度</strong>（双相机的基线）来提高建图的鲁棒性。</p> <h5 id="1-定义与核心思想">1. 定义与核心思想</h5> <p><strong>Temporal Stereo</strong> 指的是利用<strong>单只相机在不同时刻（即随时间位移）</strong>观测到的视觉信息来估计物体深度的技术 。</p> <ul> <li> <p><strong>对比 Static Stereo（静态双目）</strong>：静态双目是利用左右两个相机在<strong>同一时刻</strong>的视图差异（视差）来计算深度。</p> </li> <li> <p><strong>结合的意义</strong>：在 ESVO2 中，系统并不只依赖左右相机的瞬时匹配，而是将相机的<strong>运动位移</strong>也作为一种“虚拟基线”。</p> </li> </ul> <h5 id="2-工作原理">2. 工作原理</h5> <p>在 ESVO2 的建图模块中，Temporal Stereo 的具体运作方式如下：</p> <ul> <li> <p><strong>时空观测一致性</strong>：当机器人移动时，同一个空间点会在不同时间点被相机捕获。系统会寻找当前事件流与该相机在不久前的历史观测之间的相关性 。</p> </li> <li> <p><strong>代价合并（Cost Fusion）</strong>：ESVO2 将 Temporal Stereo 和 Static Stereo 的匹配代价（Matching Cost）进行融合 。</p> </li> <li>如果运动方向有利于时间双目（例如向前运动，提供了纵向观测变化），Temporal Stereo 能提供很好的约束。</li> <li>如果左右相机之间的视差更明显，Static Stereo 则占据主导。</li> </ul> <h5 id="3-在-esvo2-中的作用">3. 在 ESVO2 中的作用</h5> <p>引入 Temporal Stereo 对事件相机 SLAM 具有至关重要的作用：</p> <ul> <li> <p><strong>解决观测退化</strong>：事件相机对垂直于其边缘运动的方向敏感。如果机器人仅进行某种特定方向的运动，静态双目可能会失效。Temporal Stereo 增加了额外的观测维度，保证了在 6-DoF（六自由度）复杂运动下的建图完整性 。</p> </li> <li> <p><strong>提高深度图质量</strong>：通过在时间轴上累积观测，系统能够过滤掉瞬时的噪声事件，生成的深度图更加平滑且空洞更少 。</p> </li> <li> <p><strong>快速块匹配</strong>：论文提到在执行 Temporal Stereo 匹配时采用了快速块匹配方案，这使得系统能够在普通 CPU 上实时处理 VGA 分辨率的高频数据 。</p> </li> </ul> <p>在 ESVO2 中，<strong>Temporal Stereo 实际上是将 VIO 系统变成了一个“多视图立体视觉（MVS）”系统</strong>。它不再局限于双目相机那段固定的物理基线，而是将相机的整个运动轨迹都变成了获取几何信息的源泉，从而在各种运动条件下都能输出高精度的半稠密深度图 。</p> <h4 id="fast-static-stereo快速静态双目"><strong>Fast Static Stereo（快速静态双目）</strong></h4> <p><strong>Fast Static Stereo（快速静态双目）</strong> 的目的确实是为了获得三维点的坐标，但它的实现方式和传统的“特征点提取 + 三角化”有所不同。</p> <p>以下是针对您问题的详细解答：</p> <h5 id="1-是为了获得稀疏三维点吗">1. 是为了获得稀疏三维点吗？</h5> <p><strong>是的，但更准确地说是“半稠密（Semi-dense）”的三维点。</strong></p> <ul> <li><strong>非稀疏</strong>：传统的 VO（如 ORB-SLAM）只提取少量的特征点（如角点），产生的点云非常稀疏。</li> <li><strong>非全稠密</strong>：由于事件相机只在亮度变化的地方产生数据（通常是物体的边缘），所以它无法像结构光或 LiDAR 那样获得物体的完整表面。</li> <li><strong>半稠密</strong>：ESVO2 的 Fast Static Stereo 针对事件累积产生的<strong>边缘轮廓</strong>进行匹配，因此得到的是物体轮廓处的 3D 点云，这比传统的稀疏点云要密得多，足以辅助避障和局部建图。</li> </ul> <h5 id="2-只需要做三角化吗">2. 只需要做三角化吗？</h5> <p><strong>不完全是。</strong> 虽然双目外参（变换矩阵）已经离线标定好了，但“三角化”只是最后一步。最难、最核心的步骤是<strong>数据关联（Data Association / Correspondence）</strong>，即：<strong>左目里的这个事件点，对应右目里的哪个像素？</strong></p> <p>在 ESVO2 的 Fast Static Stereo 中，这个过程包含以下关键步骤：</p> <ul> <li><strong>极线搜索 (Epipolar Search)</strong>：利用已标定的外参，在右目的极线上寻找匹配点。</li> <li><strong>块匹配 (Block Matching)</strong>：由于事件数据不是灰度图，ESVO2 使用了 <strong>Time Surface（时间表面）</strong> 或像素块来进行相似度比较。</li> <li><strong>时空一致性检查</strong>：系统会检查左右目事件在时间上的同步性（是否在极短的时间差内发生）以及空间上的几何一致性。</li> <li><strong>三角化 (Triangulation)</strong>：只有当匹配的代价（Cost）足够低且置信度高时，系统才会根据已知的双目基线进行几何三角化，计算出深度 \(Z\)。</li> </ul> <h5 id="3-esvo2-fast-的体现">3. ESVO2 “Fast” 的体现</h5> <p>之所以称为 <strong>Fast</strong>，是因为它做了一些专门针对 CPU 实时性的优化：</p> <ol> <li><strong>查找表 (Look-up Table)</strong>：预先计算极线方向，减少运行时的几何计算。</li> <li><strong>跳过非边缘区域</strong>：只对有事件发生的像素点进行匹配，避开大量空白区域。</li> <li><strong>并行化</strong>：利用多线程并行处理左右目的事件流块。</li> </ol> <p>你理解的“通过三角化获得坐标”是最终目的，但 <strong>Fast Static Stereo 的核心在于如何利用标定好的几何关系，在极高频率、高分辨率的事件流中快速找到左右目的匹配对。</strong></p> <p>值得注意的是，ESVO2 还会将 <strong>Static Stereo</strong>（左右目匹配）的结果与 <strong>Temporal Stereo</strong>（单目随时间运动产生的匹配）进行融合，这样即使在双目基线方向运动导致视差较小时，依然能获得准确的深度。</p> <h4 id="轮廓">轮廓</h4> <p>在 ESVO2 论文中，<strong>Fast Static Stereo（快速静态双目）不使用传统的稀疏特征点（如角点），而是直接使用物体的轮廓（Contour Points）进行匹配</strong> 。</p> <p>这种设计是由事件相机的物理特性决定的。以下是具体的实现逻辑：</p> <h5 id="1-为什么选择轮廓而不是特征点">1. 为什么选择轮廓而不是特征点？</h5> <ul> <li><strong>物理机制</strong>：事件相机只在亮度变化时触发信号，因此其天然的输出就是物体的<strong>边缘和轮廓</strong>。</li> <li><strong>鲁棒性</strong>：在高速运动中，传统的特征点（如 Harris 角点）可能因为观察不全而难以追踪 3。直接使用轮廓点能保留更完整的环境几何结构。</li> </ul> <h5 id="2-esvo2-的具体处理方式">2. ESVO2 的具体处理方式</h5> <p>ESVO2 并没有处理所有的事件点，而是通过以下步骤提取<strong>精准的轮廓点</strong>进行匹配：</p> <ul> <li><strong>自适应累积 (Adaptive Accumulation, AA)</strong>：系统会根据事件的动态变化（速度快慢），自动决定累积多长时间的事件来生成一张类似图像的“AA图”。</li> <li><strong>轮廓点采样 (Contour-point Sampling)</strong>：在 AA 图上，系统会采样那些代表瞬时边缘的像素点（即轮廓点） 。</li> <li><strong>排除冗余</strong>：相比于直接使用原始事件流，这种方法筛选掉了大量噪声和冗余点，使得输入点更“精简且准确”。</li> </ul> <h5 id="3-fast-static-stereo-的快体现在哪">3. Fast Static Stereo 的“快”体现在哪？</h5> <p>由于处理的是轮廓点，ESVO2 采用了以下策略实现高效匹配：</p> <ul> <li><strong>块匹配 (Block Matching)</strong>：在左右目相机的 AA 图或时间表面（Time Surface）上，沿着极线对这些<strong>采样后的轮廓像素块</strong>进行相似度搜索。</li> <li><strong>取消非线性精化</strong>：论文提到，由于采样后的轮廓点已经非常精确，系统在 ESVO2 中<strong>取消了耗时的子像素级非线性精化步骤</strong>，从而显著提升了速度（在 VGA 分辨率下达到 20Hz 实时性），且精度几乎没有下降。</li> </ul> <p>在 ESVO2 的 <strong>Fast Static Stereo</strong> 模块中，获得 3D 坐标的过程是一个从“事件像素”到“空间点”的几何推导过程。虽然不使用复杂的特征描述子，但它利用了极其严格的<strong>时空约束</strong>。</p> <p>以下是具体的实现步骤：</p> <h5 id="1-极线约束搜索-epipolar-search">1. 极线约束搜索 (Epipolar Search)</h5> <p>假设左目相机在 \(u_L\) 像素处采样了一个轮廓点，由于双目相机的外参（基线 \(b\)、焦距 \(f\)）已预先标定且图像已做过极线校正（Rectification），那么这个点在右目图像中对应的匹配点一定位于<strong>水平的极线</strong>上。</p> <ul> <li><strong>搜索范围</strong>：系统会在右目的极线上，根据设定的最小和最大深度范围，确定一个搜索区间。</li> </ul> <h5 id="2-基于时间表面的匹配-time-surface-matching">2. 基于“时间表面”的匹配 (Time Surface Matching)</h5> <p>这是最关键的一步。因为事件相机没有灰度值，它使用 <strong>Time Surface (TS)</strong> 或 <strong>AA 图</strong>来计算匹配相似度：</p> <ul> <li><strong>提取 Patch</strong>：在左目 \(u_L\) 周围取一个小的像素块（例如 \(5 \times 5\)）。</li> <li><strong>计算相似度</strong>：在右目极线的搜索区间内滑动，通过 <strong>零均值归一化互相关 (ZNCC)</strong> 或类似的度量函数，寻找与左目 Patch 最相似的区域。</li> <li><strong>原理</strong>：虽然没有颜色，但物体轮廓在左右目形成的时间表面形状是非常相似的。</li> </ul> <h5 id="3-概率深度估计-probabilistic-depth-estimation">3. 概率深度估计 (Probabilistic Depth Estimation)</h5> <p>ESVO2 并不只是简单地计算一个视差，而是为每个点维护一个概率分布：</p> <ul> <li> <p><strong>逆深度建模</strong>：系统将深度表示为逆深度（Inverse Depth），并假设其服从高斯分布。</p> </li> <li> <p>代价聚合：当匹配代价最低的点被找到后，通过视差公式计算初始深度：</p> \[Z = \frac{f \cdot b}{d}\] <p>（其中 \(f\) 是焦距，\(b\) 是基线，\(d\) 是左右目像素的位移差/视差）。</p> </li> </ul> <h5 id="4-空间三角化-triangulation">4. 空间三角化 (Triangulation)</h5> <p>一旦确定了左右目的匹配对 \((u_L, v_L)\) 和 \((u_R, v_R)\)，就可以通过标准的<strong>三角化公式</strong>求出该轮廓点在左目相机坐标系下的 3D 坐标 \((X, Y, Z)\)：</p> <ul> <li> \[Z = \frac{f \cdot b}{u_L - u_R}\] </li> <li> \[X = \frac{(u_L - c_x) \cdot Z}{f}\] </li> <li> \[Y = \frac{(v_L - c_y) \cdot Z}{f}\] </li> </ul> <h5 id="5-esvo2-的特殊优化取消非线性精化">5. ESVO2 的特殊优化：取消非线性精化</h5> <p>在早期的 ESVO 版本中，三角化后通常还需要一个耗时的“非线性最小二乘精化”过程来微调 3D 位置。</p> <p>ESVO2 的改进：由于采用了自适应累积 (AA) 提取的轮廓点非常精准，论文提到他们直接跳过了非线性精化步骤。只要匹配代价足够低，就直接输出三角化的结果。这种“直接法”极大地提升了处理 VGA 分辨率数据的速度。</p> <h5 id="总结">总结</h5> <p>简单来说，获得 3D 坐标的流程是：</p> <p>左目采样轮廓点 \(\rightarrow\) 右目极线搜索匹配块 \(\rightarrow\) 计算视差 \(\rightarrow\) 几何三角化 \(\rightarrow\) 得到 3D 坐标。</p> <p>这一过程生成的点云是<strong>半稠密</strong>的，因为它只存在于有光强变化的边缘位置，但对于机器人感知环境（如避障、路径规划）已经足够丰富。</p> <h3 id="localization">Localization</h3> <h4 id="1-空间上的-3d-2d-配准当前时刻-vs-历史时刻">1. 空间上的 3D-2D 配准（当前时刻 vs 历史时刻）</h4> <p>追踪线程的核心逻辑是：<strong>“用过去建立的地图，来对齐现在的观测。”</strong></p> <ul> <li><strong>3D 信息（来自过去/不同时刻）：</strong> 追踪线程使用的 3D 点云是由“建图线程（Mapping）”提供的。这些 3D 点是根据<strong>之前的一系列时刻</strong>（滑动窗口内的历史帧）计算并累积出来的局部地图。</li> <li><strong>2D 信息（来自当前时刻）：</strong> 追踪线程使用的是<strong>当前最新时刻</strong>产生的事件流（并转化成了 OS-TS 时间表面）。</li> </ul> <p><strong>结论：</strong> 它是将<strong>历史时刻积累的 3D 结构</strong>投影到<strong>当前时刻的 2D 平面</strong>上。如果投影的位置和当前看到的位置重合，就说明位姿估计是准确的。</p> <hr/> <h4 id="2-时间上的时空一致性时空配准">2. 时间上的“时空一致性”（时空配准）</h4> <p>ESVO2 之所以强调“时空（Spatio-temporal）”，是因为它不仅仅看空间位置，还考虑了<strong>事件发生的时间戳</strong>。</p> <ul> <li><strong>Time Surface (TS) 的本质：</strong> 时间表面本身就存储了时间信息（像素值代表该位置最近一次事件发生的时间戳）。</li> <li>配准逻辑： 1. 假设当前时间是 \(t_{now}\)。 <ol> <li>如果 3D 点投影到 \(u\) 位置，而 OS-TS 在 \(u\) 位置记录的事件时间非常接近 \(t_{now}\)，说明这个 3D 点在当前时刻是“活跃”的，配准残差就小。</li> <li>反之，如果该位置没有新事件，或者事件发生的时间很久远，残差就会很大。</li> </ol> </li> </ul> <hr/> <h4 id="3-imu-的作用连接不同时刻的纽带">3. IMU 的作用：连接不同时刻的纽带</h4> <p>IMU 预积分在这里扮演了极其重要的角色：</p> <ul> <li>它利用<strong>两个时刻之间</strong>的高频惯性数据，预测出从上一时刻到当前时刻的相对位姿变化。</li> <li>这个预测值作为初值，告诉追踪线程：“根据 IMU 估计，那些 3D 点现在应该出现在这个位置。” 然后视觉配准再在这个基础上进行微调。</li> </ul> <h3 id="跨时刻的配准">“跨时刻”的配准</h3> <p>“跨时刻”的配准（通常指 SLAM 或视觉里程计中的位姿估计）确实是通过<strong>雅可比矩阵（Jacobian）</strong>来建立误差与位姿增量之间的线性关系，进而通过迭代优化的方式更新位姿的。</p> <p>由于位姿（旋转 + 平移）所在的空间（如 \(SE(3)\)）并不是一个欧几里得空间（你不能简单地把两个旋转矩阵相加），数学上通常会引入<strong>李群（Lie Group）</strong>和<strong>李代数（Lie Algebra）</strong>来处理求导问题。</p> <p>以下是这一过程的数学推导核心步骤：</p> <hr/> <h4 id="1-定义误差函数residual">1. 定义误差函数（Residual）</h4> <p>假设在 \(t\) 时刻，我们观察到一个空间点 \(P\)。在 \(t+1\) 时刻，相机的位姿变为 \(T\)（包含旋转 \(R\) 和平移 \(t\)）。该点在当前相机坐标系下的投影预测值为：</p> \[\hat{z} = h(T, P)\] <p>其中 \(h\) 是投影函数。如果实际观测到的特征点坐标是 \(z\)，那么<strong>误差（残差）</strong>定义为：</p> \[e(T) = z - h(T, P)\] <h4 id="2-引入扰动模型perturbation-model">2. 引入扰动模型（Perturbation Model）</h4> <p>因为直接对旋转矩阵 \(R\) 求导非常复杂且必须保持正交性约束，我们通常给当前的位姿 \(T\) 左乘一个微小的扰动 \(\Delta T\)。</p> <p>在李代数上，这个扰动可以用一个 6 维向量 \(\xi = [\rho, \phi]^T\) 表示（前三维为平移扰动，后三维为旋转扰动）：</p> \[T_{new} = \exp(\xi^{\wedge}) \cdot T_{old}\] <h4 id="3-利用雅可比矩阵进行线性化">3. 利用雅可比矩阵进行线性化</h4> <p>我们要计算的是：当位姿发生微小变化 \(\xi\) 时，误差 \(e\) 发生了多少变化？</p> <p>利用泰勒展开：</p> \[e(T_{new}) = e(\exp(\xi^{\wedge})T_{old}) \approx e(T_{old}) + \frac{\partial e}{\partial \xi} \xi\] <p>这里的 \(J = \frac{\partial e}{\partial \xi}\) 就是你所说的雅可比矩阵。</p> <h4 id="4-雅可比矩阵的具体分解">4. 雅可比矩阵的具体分解</h4> <p>根据链式法则，这个雅可比矩阵通常可以拆解为两部分：</p> \[J = \frac{\partial e}{\partial P'} \cdot \frac{\partial P'}{\partial \xi}\] <ul> <li> <p><strong>第一部分 \(\frac{\partial e}{\partial P'}\)</strong>：像素误差对空间点坐标（在相机坐标系下）的导数。这取决于相机的内参模型（如针孔模型）。</p> </li> <li> <p>第二部分 \(\frac{\partial P'}{\partial \xi}\)：变换后的空间点坐标对位姿扰动的导数。在 \(SE(3)\) 下，对于点 \(P' = [X, Y, Z]^T\)，其推导结果通常是一个 \(3 \times 6\) 的矩阵：</p> \[\frac{\partial P'}{\partial \xi} = \begin{bmatrix} I &amp; -P'^{\wedge} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; Z &amp; -Y \\ 0 &amp; 1 &amp; 0 &amp; -Z &amp; 0 &amp; X \\ 0 &amp; 0 &amp; 1 &amp; Y &amp; -X &amp; 0 \end{bmatrix}\] </li> </ul> <h4 id="5-求解与位姿更新">5. 求解与位姿更新</h4> <p>为了使误差最小化（最小二乘问题），我们构建高斯-牛顿（Gauss-Newton）方程：</p> \[(J^T J) \Delta \xi = -J^T e\] <ul> <li><strong>\(H = J^T J\)</strong>：近似海森矩阵（Hessian）。</li> <li><strong>\(\Delta \xi\)</strong>：计算出的最优位姿增量。</li> </ul> <p>更新步：</p> <p>得到 \(\Delta \xi\) 后，我们将其映射回李群，更新当前的位姿估计：</p> \[T \leftarrow \exp(\Delta \xi^{\wedge}) \cdot T\] <p>通过不断重复上述过程（线性化 -&gt; 求解 -&gt; 更新），位姿会逐渐收敛到能够使跨时刻特征点对齐的最优值。</p> <h3 id="localization-与-mapping并行且循环的工作流">Localization 与 Mapping并行且循环的工作流</h3> <p>在 ESVO2 的追踪线程（Localization）中，<strong>它同时利用了“跨时刻”的 3D 信息和“当前时刻”的 2D 信息。</strong></p> <h4 id="1-追踪线程localization利用过去引导现在">1. 追踪线程（Localization）：利用“过去”引导“现在”</h4> <ul> <li><strong>正确性确认</strong>：是的。追踪线程是第一步。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 2D 信息（OS-TS 时间表面）+ 之前的 Local 3D Map。</li> <li><strong>过程</strong>：它执行 3D-2D 的时空配准。简单说，就是把已经建好的 3D 地图点投影到当前的 2D 画面上，看对不对得上。</li> <li><strong>结果</strong>：计算出当前最准确的 <strong>Camera Pose</strong>（相机位姿）。IMU 在这里提供了一个非常关键的初始预测位姿，使得追踪在剧烈运动时不会丢。</li> </ul> </li> </ul> <h4 id="2-建图线程mapping利用现在更新未来">2. 建图线程（Mapping）：利用“现在”更新“未来”</h4> <ul> <li><strong>正确性确认</strong>：是的。获得当前位姿后，紧接着（或并行）执行 Mapping。</li> <li><strong>逻辑</strong>： <ul> <li><strong>输入</strong>：当前的 Camera Pose + 当前的 2D 信息（左右目事件流）。</li> <li><strong>Temporal Stereo 的作用</strong>：正如你所说，Temporal Stereo 需要知道相机的运动轨迹。它利用刚算出来的 <strong>Current Pose</strong> 结合历史位姿，计算出相机在移动过程中产生的“时间视差”。</li> <li><strong>结果</strong>：结合 <strong>Static Stereo</strong>（左右目即时视差）和 <strong>Temporal Stereo</strong>，生成新的 3D 点，并更新 <strong>Local 3D Map</strong>。</li> </ul> </li> </ul> <h4 id="3-为什么这个顺序很重要">3. 为什么这个顺序很重要？</h4> <p>这个循环构成了一个典型的自洽系统：</p> <ol> <li><strong>没有 Pose，无法建图</strong>：尤其是 Temporal Stereo，必须精确知道相机从 A 点挪到了 B 点，才能根据像素的移动反推深度。</li> <li><strong>没有 Map，无法追踪</strong>：追踪线程必须有一个“参照物”（3D Map），才能知道当前看到的 2D 图像代表自己在空间中的什么位置。</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[VINS, ESVO, SFM]]></summary></entry><entry><title type="html">DINO 如何用于密集预测</title><link href="https://cekxm.github.io/blog/2025/dino/" rel="alternate" type="text/html" title="DINO 如何用于密集预测"/><published>2025-12-26T05:54:16+00:00</published><updated>2025-12-26T05:54:16+00:00</updated><id>https://cekxm.github.io/blog/2025/dino</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/dino/"><![CDATA[<h2 id="资料">资料</h2> <ul> <li><a href="https://zhuanlan.zhihu.com/p/1933583851923439816">(3 封私信 / 80 条消息) 万字长文超详解读之DINO全系列—视觉表征对比学习的高峰 - 知乎</a></li> <li><a href="https://zhuanlan.zhihu.com/p/1940400858836742367">(3 封私信 / 80 条消息) 万字长文超详解之DINO-V3（DINO全系列之补充篇） - 知乎</a></li> <li><a href="https://www.youtube.com/watch?v=j2_42Yx_1_w">Inside DINOv2: Architecture Analysis + CIFAR-10 Experiment - YouTube</a></li> <li><a href="https://mashaan14.github.io/YouTube-channel/self_supervised_learning/2025_05_19_SSL">Self-Supervised Learning Review: from SimCLR to DINOv2 | Mashaan blog</a></li> </ul> <blockquote> <p>DINOv3 的发布，标志着计算机视觉进入了类似 NLP 的“GPT-3 时刻”<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p> <p><strong>对于学术界：</strong></p> <ul> <li><strong>Bad News</strong>：传统的“设计一个新 Backbone”、“魔改 Transformer 模块”刷点数的路子越来越窄了。在 7B 模型面前，微小的架构创新几乎没有意义。</li> <li><strong>Good News</strong>：新的研究方向被打开了。 <ul> <li><strong>Post-training</strong>：如何更高效地利用这些冻结特征？</li> <li><strong>多模态对齐</strong>：DINOv3 展示了初步的文本对齐能力，但这方面远未饱和。</li> <li><strong>视频理解</strong>：利用 DINOv3 强大的时序一致性做原生的视频大模型。</li> </ul> </li> </ul> <p><strong>对于工业界/工程师：</strong></p> <ul> <li><strong>这是巨大的利好</strong>。你不再需要收集几十万张标注数据去训练一个分割模型。直接下载 DINOv3 的权重，冻结它，用几百张图训练一个轻量级 Head，你就能得到工业级可用的效果。</li> <li><strong>Deployment</strong>：Meta 提供的蒸馏版小模型（特别是 ViT-Small 和 Base）将是边缘端部署的神器。</li> </ul> <p><strong>DINOv3 并没有让天塌下来，它只是铺平了地基。</strong> 它把提取“好特征”这件最脏最累最费算力的事做完了。现在的我们，可以站在 70 亿参数的肩膀上，去探索视觉智能更上层的逻辑——这何尝不是一种幸运？</p> </blockquote> <h2 id="如何使用-dino">如何使用 DINO</h2> <p>对于 dense prediction，一般要使用多个层的特征，比如 VGGT，以及 dinov3 中的应用部分有提及。</p> <ol> <li><strong>多层特征提取：</strong> 在 VGGT 的实现细节中，为了生成高分辨率的密集输出（如深度图和点图），模型将来自 DINOv2 骨干网络不同阶段的特征提供给 <strong>DPT（密集预测 Transformer）头</strong>进行处理。具体而言，VGGT 会提取 DINOv2（ViT-L/14）中<strong>第 4、11、17 和 23 块（blocks，实际上就是layer）</strong>的令牌（tokens），并将这些中间层的特征输入 DPT 进行上采样。</li> <li><strong>与 DINOv3 结合时的用法：</strong> 在 DINOv3 的后续实验中，研究人员将 VGGT 的图像特征提取器更换为 DINOv3 ViT-L。在这种配置下，他们同样使用了 <strong>4 个中间层特征的拼接（concatenation）</strong> 作为下游模块的输入，而不是仅使用最后一层。实验发现，这种使用多个中间层的方法对 DINOv3 带来了性能提升。</li> </ol> <h2 id="dpt">DPT</h2> <p>R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense prediction,” in <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 12173-12183.</p> <p><img src="/images/2025-12-26-dino/image-20251226103424453.png" alt="image-20251226103424453" class="img-fluid"/></p> <h3 id="encoder">Encoder</h3> <p>DPT共需要4层的特征，如果用ViT/dino，那这四层的尺寸是一样的。</p> <h3 id="reassemble">Reassemble</h3> <p>Read-&gt;Concatenate-&gt;Resample</p> <p>为了更清楚地理解，可以看它在完整流程中的作用：</p> <ol> <li><strong>Read：</strong> 处理 CLS 令牌（这里可能涉及拼接投影等线性运算）。</li> <li><strong>Concatenate：</strong> 将令牌<strong>摆放</strong>回网格位置。</li> <li><strong>Resamples：</strong> 使用 \(1 \times 1\) 和 \(3 \times 3\) 卷积进行通道投影及空间缩放（这里才进行真正的<strong>卷积运算</strong>）。</li> </ol> <p><strong>总结：</strong> Concatenate 阶段就像是<strong>拼图</strong>。Read 操作决定了每一块拼图（token）上的内容，而 Concatenate 只是<strong>按照位置把拼图摆好</strong>。真正的“修图”和“放大”工作是由接下来的 Resamples 卷积层完成的。</p> <h4 id="read">Read</h4> <p>在 DPT（Dense Prediction Transformer）架构中，<strong>Read 操作</strong>是其核心组件“重组操作”（Reassemble）的<strong>第一阶段</strong>。它的主要任务是处理 Vision Transformer (ViT) 输出的特殊令牌，并将令牌序列转换为可以进行空间排列的形式。</p> <p>以下是 Read 操作的详细介绍：</p> <h5 id="1-核心定义与目的">1. 核心定义与目的</h5> <p>在 ViT 中，输出包含 \(N_p\) 个图像块令牌（patch tokens）和 <strong>1 个特殊的“读取令牌”（readout token，通常指 CLS token）</strong>。</p> <ul> <li><strong>输入：</strong> \(N_p + 1\) 个令牌。</li> <li><strong>目的：</strong> 将这 \(N_p + 1\) 个令牌映射回 \(N_p\) 个令牌，以便后续的“拼接操作”（Concatenate）能将它们按照原始图像位置还原成特征图。</li> <li><strong>数学表达式：</strong> \(Read: \mathbb{R}^{(N_p+1) \times D} \to \mathbb{R}^{N_p \times D}\)。</li> </ul> <h5 id="2-三种实现方案">2. 三种实现方案</h5> <p>DPT 论文评估了处理读取令牌（\(t_0\)）与图像块令牌（\(t_1, \dots, t_{N_p}\)）之间关系的三种不同方式：</p> <ul> <li><strong>Readignore（忽略）：</strong> 直接<strong>丢弃读取令牌</strong>，只保留 \(N_p\) 个图像块令牌。这是最简单的方法，即 \(Read_{ignore}(t) = {t_1, \dots, t_{N_p}}\)。</li> <li><strong>Readadd（相加）：</strong> 将读取令牌的信息<strong>加到所有其他令牌上</strong>。即 \(Read_{add}(t) = {t_1 + t_0, \dots, t_{N_p} + t_0}\)。</li> <li><strong>Readproj（投影/默认方案）：</strong> 通过<strong>拼接后投影</strong>的方式融合信息。将读取令牌与每个图像块令牌拼接，然后通过一个线性层（MLP）将维度投影回原始大小 \(D\)。其公式为：\(Read_{proj}(t) = {mlp(cat(t_1, t_0)), \dots, mlp(cat(t_{N_p}, t_0))}\)。</li> </ul> <h5 id="3-性能表现与结论">3. 性能表现与结论</h5> <p>根据来源中的消融实验结果：</p> <ul> <li><strong>Readproj 是默认的最优方案</strong>，在单目深度估计等任务中表现略优于其他方案，因为它能更有效地捕获并分配全局信息。</li> <li>相比之下，<code class="language-plaintext highlighter-rouge">Readadd</code> 的效果甚至差于完全忽略令牌的 <code class="language-plaintext highlighter-rouge">Readignore</code>。</li> </ul> <p><strong>总结：</strong> <strong>Read 操作</strong>就像是一个“令牌筛选与融合器”，它决定了如何将 Transformer 学习到的<strong>全局图像表示（读取令牌）*<em>回馈给各个*</em>局部特征（图像块令牌）</strong>，从而确保在进入后续的卷积解码阶段前，每个像素级别的特征都已融入了全局上下文信息。</p> <h4 id="concatenate">Concatenate</h4> <p>在 DPT（Dense Prediction Transformer）的 <strong>Reassemble</strong> 操作中，<strong>Concatenate（拼接）</strong> 阶段本身<strong>不涉及任何复杂的数学运算或学习参数</strong>，它的本质是一个<strong>形状变换（Reshape/Rearrange）</strong>过程。</p> <h4 id="resample">Resample</h4> <p>这一步才真正涉及空间缩放。</p> <ul> <li><strong>输入：</strong> 空间排列好的特征图，尺寸为 \(\frac{H}{p} \times \frac{W}{p}\)，通道数为 \(D\)。</li> <li><strong>输出：</strong> 缩放后的特征图，尺寸为 \(\frac{H}{s} \times \frac{W}{s}\)，通道数为 \(\hat{D}\)（DPT 默认 \(\hat{D} = 256\)）。</li> </ul> <p>Resample 通过两步卷积运算来完成变换：</p> <ol> <li> <p><strong>通道投影：</strong> 首先使用 <strong>1x1 卷积</strong>。这一步负责将来自不同 Transformer 层的令牌维度（如 ViT-Large 的 1024 维）投影到解码器所需的统一维度 \(\hat{D}\)。</p> </li> <li> <p>空间缩放：</p> <p>随后根据目标缩放比例 \(s\) 与初始图像块大小 \(p\) 的关系，使用</p> <p>3x3 卷积</p> <p>进行调整：</p> <ul> <li><strong>下采样（\(s \ge p\)）：</strong> 使用<strong>步长（strided）为 3x3 的卷积</strong>来降低分辨率。</li> <li><strong>上采样（\(s &lt; p\)）：</strong> 使用<strong>步长为 3x3 的转置卷积（transpose convolution）</strong>来提升分辨率。</li> </ul> </li> </ol> <h3 id="fusion">Fusion</h3> <p>每级上采样2倍。</p> <h2 id="multi-task-image-restoration-guided-by-robust-dino-features">Multi-task image restoration guided by robust DINO features</h2> <p>X. Lin, C. Ren, K. C. Chan, L. Qi, J. Pan, and M. H. Yang, “Multi-task image restoration guided by robust DINO features,” <em>arXiv preprint arXiv:2312.01677</em>, 2023 (v3 revised 2024).</p> <p><img src="/images/2025-12-26-dino/image-20251226133529550.png" alt="image-20251226133529550" class="img-fluid"/></p> <p>其核心思路是：传统的图像恢复模型在任务数量增加时性能会下降，而 DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节，因此可以作为一种<strong>退化无关的表示</strong>来引导恢复过程。</p> <p>以下是 DINO-IR 的具体实现方法和核心组件：</p> <h3 id="1-核心架构与模块">1. 核心架构与模块</h3> <p>DINO-IR 基于 <strong>Restormer</strong> 架构，并集成了以下三个关键组件：</p> <ul> <li>像素-语义融合模块 (PSF, Pixel-Semantic Fusion)： <ul> <li><strong>目的：</strong> 动态融合 DINOv2 不同层级的特征。由于浅层包含像素信息，深层包含语义信息，该模块负责提取并加权这些特征。</li> <li><strong>实现：</strong> 采用<strong>门控网络（Gating Network）*<em>和多个*</em>专家网络（Expert Networks）</strong>。门控网络会根据输入图像自适应地学习浅层、中层和深层特征的权重，将对恢复任务最有益的特征赋予更高的权重进行融合。</li> </ul> </li> <li>DINO-Restore (D-R) 适配与融合模块： <ul> <li><strong>目的：</strong> 将 DINOv2 的特征集成到图像恢复主模型中。</li> <li><strong>实现：</strong> 首先通过适配层调整 PSF 融合特征的通道数和尺度，使其与恢复模型对齐。然后采用<strong>基于自注意力的融合方式</strong>：将适配后的 DINO 特征作为 <strong>Query (Q)</strong>，而将恢复模型的中间特征作为 <strong>Key (K)</strong> 和 <strong>Value (V)</strong>，通过交叉注意力机制实现特征融合。</li> </ul> </li> </ul> <h3 id="2-dino-感知对比损失-dpc-loss">2. DINO 感知对比损失 (DPC Loss)</h3> <p>为了约束模型训练，DINO-IR 提出了一种基于 DINOv2 特征空间的<strong>对比学习损失</strong>：</p> <ul> <li><strong>原理：</strong> 提取恢复后的输出图像、原始清晰图像（正样本）和退化输入图像（负样本）在 DINOv2 隐藏层中的特征。</li> <li><strong>目标：</strong> 强制要求输出图像的 DINO 特征在空间中尽可能<strong>靠近清晰目标图像</strong>，并尽可能<strong>远离低质量输入图像</strong>。这种损失利用了 DINOv2 特征区分图像质量的能力来提升视觉效果。</li> </ul> <h3 id="3-方法优势">3. 方法优势</h3> <ul> <li><strong>退化鲁棒性：</strong> DINOv2 特征在不同噪声水平和退化类型下表现出极高的稳定性（方差远低于图像像素特征），这使得模型在处理冲突任务（如去噪需要滤除高频，而去模糊需要增强高频）时更加稳定。</li> <li><strong>泛化能力：</strong> 实验证明 DINO-IR 在<strong>未见过的退化级别</strong>（如更高强度的噪声）和<strong>未见过的测试数据集</strong>上具有更好的泛化效果。</li> <li><strong>性能提升：</strong> 在 deraining, denoising, deblurring, dehazing 四项任务的平均 PSNR 表现上，DINO-IR 优于 AirNet 和 PromptIR 等现有先进的多任务恢复方法。</li> </ul> <h3 id="note">Note</h3> <p>作者有做额外实验，DINOv2 的深层特征对退化因素不敏感且保留了语义信息，浅层特征则捕获了像素级细节。</p> <p>It is known that the features extracted from shallow layersof DINOv2 (M, T, and T 2023) can discern low- and high-quality images。</p> <p>根据 DINO-IR（基于鲁棒 DINO 特征引导的多任务图像恢复）的研究资料，其提出的 <strong>DINO 感知对比损失（DINO Perception Contrastive Loss，简称 \(L_{DINO}\) 或 DPC Loss）</strong> 的公式及相关总损失公式如下：</p> <p>该损失函数旨在通过对比学习，使恢复后的图像在 DINOv2 的特征空间中靠近清晰图像，并远离退化的输入图像。公式表达为：</p> \[L_{DINO} = L(v, v^+, v^-) = \sum_{i=1}^n w_i \frac{D(\Psi_i(v), \Psi_i(v^+))}{D(\Psi_i(v), \Psi_i(v^-))}\] <p><strong>参数含义：</strong></p> <ul> <li><strong>\(v\)</strong>：恢复模型生成的输出图像。</li> <li><strong>\(v^+\)</strong>：正样本，即对应的<strong>清晰目标图像（Ground Truth）</strong>。</li> <li><strong>\(v^-\)</strong>：负样本，即<strong>低质量的退化输入图像</strong>。</li> <li><strong>\(\Psi_i\)</strong>：表示从固定的预训练 DINOv2 模型中提取的第 \(i\) 个隐藏层特征。</li> <li><strong>\(D(x, y)\)</strong>：表示 \(x\) 与 \(y\) 之间的 <strong>\(L1\) 距离</strong>。</li> <li><strong>\(w_i\)</strong>：对应层级的权重系数。</li> </ul> <p>作者并没有给出是第几层。但太深的层应该没用。</p> <h2 id="处理任意输入大小图片">处理任意输入大小图片</h2> <p>DINO（包括 DINOv2 和 DINOv3）处理任意大小图片的核心机制在于其 <strong>Transformer 架构的灵活性</strong>、<strong>分块（Patchification）策略</strong>以及<strong>位置编码的动态插值或旋转机制</strong>。</p> <p>以下是具体的实现方式：</p> <h3 id="1-灵活的序列长度处理set-to-set-架构">1. 灵活的序列长度处理（Set-to-set 架构）</h3> <p>DINO 系列模型基于 Vision Transformer (ViT)。与传统的卷积神经网络不同，Transformer 是一种<strong>“集合到集合”（set-to-set）的架构</strong>，它将图像视为一系列令牌（tokens）。</p> <ul> <li><strong>分块机制：</strong> 图像被切分为固定大小的 patch（例如 DINOv2 使用 \(14 \times 14\)，DINOv3 使用 \(16 \times 16\)）。</li> <li><strong>令牌数量随分辨率变化：</strong> 当输入图像变大时，模型只会产生更多的令牌，而 Transformer 的自注意力机制（Self-attention）天然可以处理任意长度的输入序列。</li> </ul> <h3 id="2-位置编码的适配核心技术">2. 位置编码的适配（核心技术）</h3> <p>由于 Transformer 本身无法感知令牌的空间顺序，必须加入位置编码。处理不同分辨率图像的关键在于如何让固定长度的位置编码适应变动的令牌数量：</p> <ul> <li><strong>线性插值（DINOv2/DPT 方案）：</strong> 在 DINOv2 和 DPT 中，如果输入图像的分辨率与训练时的分辨率不同，模型会对预训练的<strong>位置嵌入（Position Embeddings）进行线性插值</strong>。这使得模型能够动态适配到新的令牌网格尺寸，确保每个令牌都能获得其在图像中相对位置的信息。</li> <li><strong>旋转位置编码（DINOv3 的 RoPE 机制）：</strong> <strong>DINOv3</strong> 引入了更先进的 <strong>RoPE（Rotary Positional Embeddings）</strong> 机制。它将每个 patch 的坐标分配在一个归一化的 \([-1, 1]\) 框内，并在多头注意力操作中根据 patch 间的相对位置应用偏差。</li> <li><strong>无缝缩放：</strong> 依靠 RoPE 和坐标框抖动（box jittering）技术，DINOv3 能够<strong>在不进行任何适配的情况下无缝处理不同分辨率的图像</strong>。实验显示，即使在远超训练分辨率（如 4k 分辨率）的情况下，DINOv3 仍能保持稳定的特征表现。</li> </ul> <h3 id="3-全局感受野的维持">3. 全局感受野的维持</h3> <p>在卷积网络中，感受野随层数增加而受限，但在 DINO 中，由于使用了全局自注意力机制，<strong>每一个阶段（stage）都拥有全局感受野</strong>。</p> <ul> <li>这意味着无论图像多大，每个令牌都能与图像中的所有其他令牌进行交互。</li> <li>这种特性确保了模型在处理任意大小图片时，都能产生<strong>全局连贯（globally coherent）</strong>且精细的预测结果。</li> </ul> <h3 id="总结">总结</h3> <p>DINO 处理任意大小图片的逻辑可以类比为<strong>“拼图”</strong>：</p> <ul> <li><strong>分块</strong>是把图片切成小拼块，图越大拼块越多。</li> <li><strong>Transformer</strong> 是拼图者，他能处理任意数量的拼块。</li> <li><strong>RoPE 或插值编码</strong> 就像是在每一块拼图背面标注坐标的记号笔，通过动态缩放记号的刻度（坐标），拼图者总能知道每一块在大图中的精确位置。</li> </ul> <h3 id="dinov2-处理任意大小图像的代码">DINOv2 处理任意大小图像的代码</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">skimage.color</span> <span class="kn">import</span> <span class="n">hsv2rgb</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="c1"># --- 1. 配置参数 ---
# 图像文件路径 (已设置为您的文件)
</span><span class="n">IMAGE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fruits.jpg</span><span class="sh">'</span> 
<span class="c1"># DINOv2 模型 ID (Base 版本，公开且无需权限)
</span><span class="n">MODEL_ID</span> <span class="o">=</span> <span class="sh">"</span><span class="s">facebook/dinov2-base</span><span class="sh">"</span> 
<span class="n">DEVICE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="c1"># DINOv2-base 默认 Patch size 为 14x14
</span><span class="n">PATCH_SIZE</span> <span class="o">=</span> <span class="mi">14</span> 
<span class="c1"># 设置一个最小的安全尺寸，防止原图太小
</span><span class="n">MIN_SIZE</span> <span class="o">=</span> <span class="mi">224</span> 

<span class="k">def</span> <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    加载 DINOv2 模型，提取 Patch 特征，使用 PCA 降维并可视化。
    图像尺寸会调整到最接近原始尺寸且是 Patch Size 的整数倍。
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">错误: 图像文件未找到于 </span><span class="sh">'</span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="sh">'</span><span class="s">。请检查路径并重试。</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 2. 初始化模型和处理器 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">正在加载 DINOv2 模型: </span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s"> 到 </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># 评估模式
</span>    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型加载失败。请检查模型 ID 或网络连接。</span><span class="se">\n</span><span class="s">错误信息: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># --- 3. 图像处理与特征提取 (重点修改部分) ---
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="sh">"</span><span class="s">RGB</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">size</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">原始图像尺寸 (W x H): </span><span class="si">{</span><span class="n">W_orig</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_orig</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># a. 计算目标输入尺寸 (必须是 PATCH_SIZE 的整数倍)
</span>    <span class="c1"># 取最接近原始尺寸且小于等于原始尺寸的 PATCH_SIZE 倍数
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">H_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">W_orig</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">patch_size</span>
    
    <span class="c1"># 确保尺寸不小于最小安全尺寸
</span>    <span class="n">H_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="n">W_target</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">W_target</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">模型目标输入尺寸 (W x H): </span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s"> x </span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># b. 预处理
</span>    <span class="c1"># 显式传递 size 和 crop_size 参数，控制预处理器的缩放行为
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">processor</span><span class="p">(</span>
        <span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> 
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 缩放或调整到目标尺寸
</span>        <span class="n">crop_size</span><span class="o">=</span><span class="p">(</span><span class="n">H_target</span><span class="p">,</span> <span class="n">W_target</span><span class="p">),</span> <span class="c1"># 确保不进行中心裁剪
</span>        <span class="n">do_center_crop</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 明确禁用中心裁剪
</span>    <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 实际输入模型张量的尺寸
</span>    <span class="n">h_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">w_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="sh">'</span><span class="s">pixel_values</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    
    <span class="c1"># 重新计算 Patch 网格尺寸 (H, W)
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">h_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w_input</span> <span class="o">//</span> <span class="n">patch_size</span>
    
    <span class="c1"># c. 提取特征
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># **inputs 解包字典作为命名参数
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> 
        <span class="n">features</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="c1"># d. 移除 CLS Token
</span>    <span class="k">if</span> <span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">已移除 CLS Token。剩余 Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Patch 特征形状: </span><span class="si">{</span><span class="n">features</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># --- 4. PCA 降维 ---
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">正在进行 PCA 降维...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">pca_features</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

    <span class="c1"># 归一化到 [0, 1] 范围
</span>    <span class="n">pca_min</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pca_max</span> <span class="o">=</span> <span class="n">pca_features</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">pca_max</span> <span class="o">-</span> <span class="n">pca_min</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">denominator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-8</span> 
    <span class="n">pca_features_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">pca_features</span> <span class="o">-</span> <span class="n">pca_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># --- 5. 可视化映射 ---
</span>
    <span class="c1"># a. 重塑为网格形状 (H, W, 3)
</span>    <span class="n">pca_grid</span> <span class="o">=</span> <span class="n">pca_features_norm</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="c1"># b. 映射到 HSV 颜色空间 
</span>    <span class="n">hsv_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">pca_grid</span><span class="p">)</span>
    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Hue (色调)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>              <span class="c1"># Saturation (饱和度)
</span>    <span class="n">hsv_image</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca_grid</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># Value (亮度)
</span>
    <span class="c1"># 转换为 RGB 颜色
</span>    <span class="n">rgb_vis</span> <span class="o">=</span> <span class="nf">hsv2rgb</span><span class="p">(</span><span class="n">hsv_image</span><span class="p">)</span>
    
    <span class="c1"># c. 缩放可视化结果到原始图像大小
</span>    <span class="n">rgb_vis_upscaled</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span>
        <span class="n">rgb_vis</span><span class="p">,</span> 
        <span class="p">(</span><span class="n">W_orig</span><span class="p">,</span> <span class="n">H_orig</span><span class="p">),</span> <span class="c1"># 使用原图尺寸进行缩放
</span>        <span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">INTER_NEAREST</span> <span class="c1"># 最近邻插值保持 Patch 块状效果
</span>    <span class="p">)</span>

    <span class="c1"># --- 6. 显示和保存结果 ---
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">原始图像 (Original Image: fruits.jpg)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">rgb_vis_upscaled</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">DINOv2 特征 PCA 可视化 (</span><span class="si">{</span><span class="n">W_target</span><span class="si">}</span><span class="s">x</span><span class="si">{</span><span class="n">H_target</span><span class="si">}</span><span class="s"> 输入)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">output_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">dinov2_fruits_pca_visualization.png</span><span class="sh">"</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">output_filename</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">可视化结果已保存为 </span><span class="si">{</span><span class="n">output_filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">visualize_dinov2_features_variable_res</span><span class="p">(</span><span class="n">IMAGE_PATH</span><span class="p">,</span> <span class="n">MODEL_ID</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">PATCH_SIZE</span><span class="p">,</span> <span class="n">MIN_SIZE</span><span class="p">)</span>
</code></pre></div></div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="https://zhuanlan.zhihu.com/p/1986387271688139358">(3 封私信 / 80 条消息) DINOv3 is All You Need? 为什么 DINOv3 发布后，CV 圈感觉“天塌了”？ - 知乎</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[如何使用DINO, DPT, RoPE]]></summary></entry><entry><title type="html">3D 任务：SFM, MVS, NVS, VO, VIO, SLAM</title><link href="https://cekxm.github.io/blog/2025/3dtasks/" rel="alternate" type="text/html" title="3D 任务：SFM, MVS, NVS, VO, VIO, SLAM"/><published>2025-12-26T00:00:00+00:00</published><updated>2025-12-26T00:00:00+00:00</updated><id>https://cekxm.github.io/blog/2025/3dtasks</id><content type="html" xml:base="https://cekxm.github.io/blog/2025/3dtasks/"><![CDATA[<h2 id="多视角几何mvs新视角生成nvs对比">多视角几何（MVS），新视角生成（NVS）对比</h2> <p><img src="/images/2025-12-26-3dtasks/d3dcd8d3-35cb-403b-998d-64256b21ba06.png" alt="SfM_NeRF" class="img-fluid"/></p> <h3 id="mvssfm-vs-vggt-vs-nvs-nerfgs-综合对比表">(MVS)SfM vs. VGGT vs. NVS (NeRF/GS) 综合对比表</h3> <table> <thead> <tr> <th><strong>维度</strong></th> <th><strong>(MVS) SfM (如 COLMAP)</strong></th> <th><strong>VGGT (Visual Geometry Grounded Transformer)</strong></th> <th><strong>NVS (NeRF / 3D GS)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>核心任务目标</strong></td> <td><strong>三维重建</strong>：求解精确的相机姿态和场景几何（点云/深度）。</td> <td><strong>统一几何推断</strong>：一站式、秒级预测相机、点图、深度图和追踪。</td> <td><strong>新视角合成</strong>：在未拍摄过的角度生成照片级逼真的图像。</td> </tr> <tr> <td><strong>底层表示</strong></td> <td><strong>离散几何</strong>：稀疏或稠密的 3D 点云、深度图。</td> <td><strong>稠密几何图</strong>：Point Maps (\(H \times W \times 3\)) 和 Depth Maps。</td> <td><strong>光场表示</strong>：NeRF 使用 MLP（隐式）；GS 使用高斯球（显式）。</td> </tr> <tr> <td><strong>运行机制</strong></td> <td><strong>优化驱动</strong>：基于特征匹配 + 束调整 (Bundle Adjustment) 迭代求解。</td> <td><strong>前馈推理 (Feed-forward)</strong>：一次性通过 Transformer 模型直接“看”出几何。</td> <td><strong>视图对齐优化</strong>：通过渲染结果与原图的颜色误差来反向训练。</td> </tr> <tr> <td><strong>对相机的需求</strong></td> <td><strong>未知或已知</strong>：通常用于解算未知相机参数。</td> <td><strong>无需预设</strong>：直接预测相机的 9 维参数（内外参）。</td> <td><strong>必须已知</strong>：通常依赖 SfM (COLMAP) 提供位姿初始化。</td> </tr> <tr> <td><strong>几何精确度</strong></td> <td><strong>高（度量级）</strong>：数学推导严谨，但易受弱纹理、模糊影响。</td> <td><strong>高且鲁棒</strong>：利用 DINOv2 先验，在挑战性场景下比传统方法更稳。</td> <td><strong>中/低</strong>：主要优化视觉效果，几何结构往往存在“漂浮物”或误差。</td> </tr> <tr> <td><strong>渲染视觉效果</strong></td> <td><strong>差</strong>：只有离散点，无法生成连续、真实的图像。</td> <td><strong>中</strong>：提供稠密几何，但主要用于几何任务而非美学渲染。</td> <td><strong>极高</strong>：支持照片级渲染、阴影、反射和透明效果。</td> </tr> <tr> <td><strong>处理速度</strong></td> <td><strong>极慢</strong>：通常需要数分钟到数小时。</td> <td><strong>极快</strong>：全流程通常在 <strong>1 秒以内</strong>。</td> <td><strong>训练慢/渲染快</strong>：NeRF 训练慢，GS 渲染极快，但都需要初始化。</td> </tr> </tbody> </table> <p>简而言之：<strong>(MVS)SfM</strong> 是传统的“几何尺子”；<strong>VGGT</strong> 是现代的“几何大模型”；而 <strong>NVS</strong> 是“虚拟照相机”。</p> <p>根据您上传的论文内容及相关技术背景，<strong>ESVO2 是一个 VIO（视觉惯性里程计）系统</strong>，它是基于纯视觉算法 ESVO 的增强版，引入了惯性测量单元（IMU）来提高鲁棒性。</p> <p>以下是关于 ESVO2 的定位及其与 SLAM 区别的详细介绍：</p> <h2 id="vio-与-slam-的区别">VIO 与 SLAM 的区别</h2> <p>虽然两者都旨在解决“我在哪”的问题，但它们在功能覆盖和目标重点上有所不同：</p> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>视觉惯性里程计 (VIO)</strong></th> <th><strong>同步定位与建图 (SLAM)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>主要组成</strong></td> <td>视觉处理 + IMU 预积分 + 后端优化</td> <td>VIO (前端) + <strong>回环检测</strong> + 全局优化</td> </tr> <tr> <td><strong>误差累计</strong></td> <td>会随着位移增加而产生<strong>累计漂移（Drift）</strong></td> <td>通过回环检测修正累计误差，具有<strong>全局一致性</strong></td> </tr> <tr> <td><strong>地图规模</strong></td> <td>通常只维护一个局部地图（滑动窗口内）</td> <td>维护全局地图，允许机器人回到已知点时重定位</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>相对较低，适合实时性要求极高的场景</td> <td>较高，需要存储和检索大量历史关键帧数据</td> </tr> <tr> <td><strong>典型代表</strong></td> <td>VINS-Mono (VIO模式), ESVO2, OKVIS</td> <td>ORB-SLAM3, VINS-Fusion (含回环)</td> </tr> </tbody> </table> <h2 id="slam">SLAM</h2> <p>在 SLAM（同步定位与建图）框架中，<strong>回环检测（Loop Closure Detection）</strong>和<strong>全局优化（Global Optimization）</strong>是消除累计误差、保证地图全局一致性的核心机制。</p> <p>如果没有这两个部分，系统仅仅是一个<strong>里程计（Odometry）</strong>，位姿误差会随着时间的推移不断增加，轨迹最终会“漂散”。</p> <hr/> <h3 id="回环检测-loop-closure-detection">回环检测 (Loop Closure Detection)</h3> <p>回环检测的任务是：<strong>识别机器人是否回到了曾经到过的地方。</strong></p> <ul> <li> <p>为什么要检测？</p> <p>里程计每一步都会引入微小的误差，经过长距离运行后，系统估计的当前位置与真实位置可能相差巨大。如果能识别出“旧地重游”，就能通过这个历史“锚点”来纠正累积误差。</p> </li> <li> <p><strong>如何实现？（主流方法：词袋模型 BoW）</strong></p> <ol> <li><strong>特征提取</strong>：提取当前图像的特征点（如 ORB）。</li> <li><strong>词袋描述</strong>：将特征点转化为一个数值向量（类似一篇文章的关键词提取）。</li> <li><strong>相似度检索</strong>：在历史图像库中寻找与当前图像最相似的帧。</li> <li><strong>几何验证</strong>：通过对极几何等方法，确认两张图不仅长得像，而且在空间逻辑上也是匹配的，从而防止“感知歧义”（例如两间长得一模一样的办公室）。</li> </ol> </li> </ul> <hr/> <h3 id="全局优化-global-optimization">全局优化 (Global Optimization)</h3> <p>一旦回环检测发现当前帧 $i$ 与历史帧 $j$ 匹配成功，系统就获得了一个闭环约束。接下来需要通过全局优化来调整整个轨迹。</p> <h4 id="位姿图优化-pose-graph-optimization"><strong>位姿图优化 (Pose Graph Optimization)</strong></h4> <p>在回环发生后，为了保持实时性，通常不再优化复杂的 3D 空间点，而仅仅优化<strong>相机的位姿节点</strong>。</p> <ul> <li><strong>节点（Nodes）</strong>：机器人每一时刻的位姿。</li> <li><strong>边（Edges）</strong>： <ul> <li><strong>里程计边</strong>：相邻帧之间的约束（由 VIO 提供）。</li> <li><strong>回环边</strong>：当前帧与历史帧之间的约束（由回环检测提供）。</li> </ul> </li> <li><strong>优化目标</strong>：调整所有节点，使得所有“边”的残差总和最小。这就像是一个弹簧网，回环边强行把漂移的轨迹拉回到历史位置，而其他轨迹点则像弹簧一样跟随调整。</li> </ul> <hr/> <h3 id="区别总结vio-vs-slam">区别总结：VIO vs SLAM</h3> <table> <thead> <tr> <th><strong>模块</strong></th> <th><strong>VIO (如 ESVO2, VINS 前端)</strong></th> <th><strong>SLAM (如 VINS 后端, ORB-SLAM3)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>漂移</strong></td> <td>随距离/时间线性增长</td> <td>遇到回环时清零</td> </tr> <tr> <td><strong>一致性</strong></td> <td>局部一致</td> <td>全局一致</td> </tr> <tr> <td><strong>地图</strong></td> <td>瞬时、局部</td> <td>持久、可重用</td> </tr> </tbody> </table> <p><strong>COLMAP</strong> 通常不被认为是一个 <strong>VO（视觉里程计）</strong> 方法，它是一个标准的 <strong>SfM（运动恢复结构）</strong> 框架。</p> <p>虽然两者底层都使用了相似的技术（如特征提取、对极几何、Bundle Adjustment），但它们在设计目标、运行方式和应用场景上有显著区别。</p> <hr/> <h2 id="sfm-与-vo-的本质区别">SfM 与 VO 的本质区别</h2> <table> <thead> <tr> <th><strong>特性</strong></th> <th><strong>COLMAP (SfM)</strong></th> <th><strong>Visual Odometry (VO)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>处理时效</strong></td> <td><strong>离线 (Offline)</strong>：处理一组预先采集好的图片或视频序列。</td> <td><strong>实时 (Real-time)</strong>：随着相机的移动即时计算位姿。</td> </tr> <tr> <td><strong>图像顺序</strong></td> <td><strong>无序/有序</strong>：可以处理乱序的照片（如不同人的街拍），通过特征匹配寻找联系。</td> <td><strong>必须有序</strong>：依赖视频流的连续性进行帧间追踪。</td> </tr> <tr> <td><strong>优化范围</strong></td> <td><strong>全局 (Global)</strong>：通常对所有相机位姿和所有点进行全局优化（Global BA）。</td> <td><strong>局部 (Local)</strong>：通常只在滑动窗口内进行优化以维持实时性。</td> </tr> <tr> <td><strong>应用目标</strong></td> <td>追求<strong>极致精度</strong>和高质量的 3D 重建（如制作模型、地图）。</td> <td>追求<strong>低延迟</strong>和机器人的实时定位（如无人机飞行）。</td> </tr> <tr> <td><strong>计算消耗</strong></td> <td>极高：可能需要数小时或数天处理大型场景。</td> <td>较低：需要在嵌入式设备或普通 CPU/GPU 上实时运行。</td> </tr> </tbody> </table> <hr/> <h3 id="它们之间的联系">它们之间的联系</h3> <p>虽然它们定位不同，但有很深的血缘关系：</p> <ul> <li><strong>技术基础一致</strong>：它们都遵循“特征提取 $\rightarrow$ 特征匹配 $\rightarrow$ 位姿估计 $\rightarrow$ 三角化建图 $\rightarrow$ 后端优化”的流程。</li> <li><strong>增量式 SfM</strong>：COLMAP 属于“增量式（Incremental）”SfM，它的工作方式是一张一张地把新照片注册到现有地图中。这种“一张张增加”的逻辑在形式上非常接近 VO，只是 COLMAP 每增加一张都会做大量的全局检查以保证精度，而 VO 只看局部。</li> <li><strong>VO 是 SfM 的子集</strong>：你可以把 VO 看作是一种追求实时性、牺牲全局一致性、且只能处理有序序列的特殊 SfM。</li> </ul> <hr/> <h3 id="为什么不把-colmap-当作-vo-使用">为什么不把 COLMAP 当作 VO 使用？</h3> <p>如果你尝试用 COLMAP 跑实时定位，会遇到以下问题：</p> <ol> <li><strong>速度太慢</strong>：COLMAP 的每一步（尤其是匹配和全局优化）都非常耗时，无法做到每秒处理 30 帧。</li> <li><strong>内存消耗</strong>：COLMAP 试图建立和维护完整的场景模型，随着照片增加，内存开销会爆炸。</li> <li><strong>缺乏状态估计</strong>：VO 通常会融合 <strong>IMU（惯性测量单元）</strong> 来处理快速运动（如 VINS-Fusion），而 COLMAP 主要是纯视觉处理。</li> </ol> <h3 id="总结">总结</h3> <p><strong>COLMAP 是用来“建图”的利器，而 VO 是用来“带路”的工具。</strong></p> <ul> <li>如果你有 1000 张从各个角度拍的景区照片，想做一个 3D 模型，用 <strong>COLMAP</strong>。</li> <li>如果你有一个机器人正在屋里跑，需要知道它现在在哪，用 <strong>VINS-Mono</strong> 或 <strong>ORB-SLAM3</strong>。</li> </ul> <h2 id="研究方向">研究方向</h2> <p>在 2025 年的时间节点上，<strong>Novel View Synthesis (NVS)</strong> 在学术热度和资本市场显然更“火”，但 <strong>Multi-View Stereo (MVS)</strong> 作为底层基石，正在经历从“传统算法”向“几何大模型”的深刻转型。</p> <p>这两者并非孤立竞争，而是呈现出一种<strong>深度融合</strong>的趋势。以下是从热门程度、技术前景和应用价值三个维度的详细对比：</p> <hr/> <h3 id="1-热门程度novel-view-synthesis-nvs-占据-c-位">1. 热门程度：Novel View Synthesis (NVS) 占据 C 位</h3> <p><strong>核心技术：3D Gaussian Splatting (3DGS), NeRF, Generative 3D</strong></p> <ul> <li><strong>学术热度：</strong> 2024-2025 年，视觉顶级会议（CVPR, ICCV）中关于 <strong>3DGS (3D 高斯溅射)</strong> 和 <strong>生成式新视角合成</strong> 的论文数量呈爆炸式增长。</li> <li><strong>AIGC 助力：</strong> 随着视频生成模型（如 Sora, Kling）的爆发，如何从单张图或一段视频生成可交互的 3D 场景（即 <strong>Generative NVS</strong>）成了最热门的方向。</li> <li><strong>用户感知度：</strong> NVS 能生成“照片级”的视觉效果，普通人一眼就能看出好坏，因此在 VR/AR、数字孪生、影视特效领域极具吸引力。</li> </ul> <h3 id="2-发展前景mvs-正在向几何大模型进化">2. 发展前景：MVS 正在向“几何大模型”进化</h3> <p><strong>核心技术：VGGT, MVSNet 系列, Foundation Models for Geometry</strong></p> <ul> <li><strong>从“工具”到“大脑”：</strong> 传统的 MVS（如 COLMAP）依赖复杂的数学优化。2025 年的趋势是像 <strong>VGGT</strong> 这样，利用大规模预训练（如 DINOv2）将 MVS 变成一个<strong>前馈网络（Feed-forward）</strong>。</li> <li><strong>解决“不可能任务”：</strong> 传统的 MVS 在面对弱纹理（白墙）、反光（玻璃）时会失败。2025 年的发展方向是利用先验知识（Priors）来预测这些区域的几何。</li> <li><strong>工业刚需：</strong> 无论 NVS 渲染得多么好看，自动驾驶、无人机导航、工业精密测量、建筑 BIM 仍然需要 MVS 提供的<strong>精确绝对坐标（Metric Geometry）</strong>。</li> </ul> <hr/> <h3 id="3-2025-年的关键趋势两者边界的模糊融合">3. 2025 年的关键趋势：两者边界的模糊（融合）</h3> <p>如果你在考虑职业发展或研究方向，<strong>“几何感知的 NVS” (Geometry-aware NVS)</strong> 是 2025 年最具前景的方向。</p> <ol> <li><strong>MVS 为前，NVS 为后：</strong> 就像您之前提到的，用 VGGT（MVS 思路）快速初始化几何，再用 3DGS（NVS 思路）进行精修和渲染。这是目前 3D 重建最前沿的 Pipeline。</li> <li><strong>可推广性 (Generalizability)：</strong> 以前的 NeRF/GS 需要针对每个场景单独训练。2025 年的突破点在于 <strong>LRM (Large Reconstruction Models)</strong>，即输入几张图，模型直接秒级输出可渲染的 3D 表示，这背后本质上是 MVS 与生成式架构的结合。</li> <li><strong>动态场景：</strong> 静态场景的重建基本解决，2025 年的蓝海是<strong>动态 4D 重建</strong>（例如重建一个正在运动的人或动物），这同时需要 MVS 的点追踪（Point Tracking）能力和 NVS 的实时渲染能力。</li> </ol> <hr/> <h3 id="总结建议">总结建议</h3> <ul> <li><strong>如果你追求“视觉震撼”和“快速产出”：</strong> 选择 <strong>Novel View Synthesis (尤其是 3DGS)</strong>。这是目前 AIGC 落地最快的方向，适合互联网、游戏、广告和元宇宙行业。</li> <li><strong>如果你追求“底层技术”和“稳健性”：</strong> 选择 <strong>MVS/几何大模型</strong>。这是 3D 视觉的根基。虽然它可能没有渲染图那么惊艳，但在自动驾驶、机器人和空间计算领域，它的不可替代性极高。</li> <li><strong>最具潜力的路径：</strong> 研究<strong>如何将 MVS 的几何约束引入 NVS</strong>（例如 VGGT 的思路）。这种既有“精确骨架（MVS）”又有“华丽皮肤（NVS）”的技术架构，是 2025 年 3D 视觉的终极答案。</li> </ul> <p><strong>结论：</strong> <strong>NVS 更“热门”（Hotter）</strong>，但 <strong>MVS 的“底座”地位在 2025 年因大模型的介入而重新变得极具“前景”（More Promising）</strong>。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[多视角几何，新视角生成]]></summary></entry></feed>